{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a BDT to predict $\\delta C_9$ on an event-by-event basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from library.utilities.plotting import setup_mpl_params\n",
    "from library.data.datasets.aggregated_signal_binned import Aggregated_Signal_Binned_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup / Deactivate Fancy Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_mpl_params()\n",
    "# mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_set_likelihood(x, clf):\n",
    "    \"\"\"\n",
    "    x : ndarray of events\n",
    "    clf : sklearn classifier \n",
    "    \"\"\"\n",
    "    pred = clf.predict_proba(x)\n",
    "    sum_log_pred = np.sum(np.log(pred), axis=0)\n",
    "    return sum_log_pred\n",
    "\n",
    "\n",
    "def predict_likelihood_over_bins(x, y, clf):\n",
    "    \"\"\"\n",
    "    x : ndarray of events\n",
    "    y : ndarray of bins\n",
    "    clf : sklearn classifier\n",
    "    \"\"\"\n",
    "    bins = np.unique(y)\n",
    "    preds = []\n",
    "    for bin in bins:\n",
    "        x_bin = x[y==bin]\n",
    "        pred = predict_set_likelihood(x_bin, clf)\n",
    "        preds.append(np.expand_dims(pred, axis=0))\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    assert preds.shape == (len(bins), len(bins))\n",
    "    return preds\n",
    "\n",
    "\n",
    "def bootstrap_over_bins(x, y, n, rng=np.random.default_rng()):\n",
    "    \"\"\"\n",
    "    x : ndarray of events\n",
    "    y : ndarray of bins\n",
    "    n : number of events to sample from each bin    \n",
    "    \"\"\"\n",
    "    bootstrap_x = []\n",
    "    bootstrap_y = []\n",
    "    for bin in np.unique(y):\n",
    "    \n",
    "        pool_x = x[y==bin]\n",
    "        pool_y = y[y==bin]\n",
    "        assert pool_x.shape[0] == pool_y.shape[0]\n",
    "\n",
    "        selection_indices = rng.choice(len(pool_x), n)\n",
    "\n",
    "        bin_bootstrap_x = pool_x[selection_indices]\n",
    "        bin_bootstrap_y = pool_y[selection_indices]\n",
    "\n",
    "        bootstrap_x.append(bin_bootstrap_x)\n",
    "        bootstrap_y.append(bin_bootstrap_y)\n",
    "\n",
    "    bootstrap_x = np.concatenate(bootstrap_x)\n",
    "    bootstrap_y = np.concatenate(bootstrap_y)\n",
    "\n",
    "    return bootstrap_x, bootstrap_y\n",
    "\n",
    "\n",
    "def plot_likelihood_over_bins(predictions_over_bins, bin_values, cmap=plt.cm.viridis):\n",
    "    \"\"\"\n",
    "    predictions_over_bins : ndarray of summed log event probabilities\n",
    "        (rows are input bins, columns are bin predictions)\n",
    "    bin_values : ndarray of the value each bin represents \n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(layout=\"constrained\")\n",
    "\n",
    "    bounds = np.append(bin_values, bin_values[-1] + (bin_values[-1] - bin_values[-2]))\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    for b_v, pred in zip(bin_values, predictions_over_bins):\n",
    "        pred_bin = np.argmax(pred)\n",
    "        ax.plot(bin_values, pred, color=cmap(norm(b_v)))\n",
    "        ax.scatter(bin_values[pred_bin], np.max(pred), color=cmap(norm(b_v)), edgecolors=\"black\", zorder=100)\n",
    "\n",
    "    fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax, label=r\"$\\delta C_9$\")\n",
    "    ax.set_xlabel(r\"$\\delta C_9$\")\n",
    "    ax.set_ylabel(r\"$\\sum_i \\log p(\\delta C_9 \\;|\\; x_i)$\")\n",
    "\n",
    "    plt.show()    \n",
    "\n",
    "\n",
    "def predict_likelihood_over_bootstrapped_trials(x, y, n_trials, n_events, clf):\n",
    "    \"\"\"\n",
    "    x : ndarray of events\n",
    "    y : ndarray of bins\n",
    "    n_trials : number of bootstrapped sample sets\n",
    "    n_events : number of events to bootstrap per bin\n",
    "    \"\"\"\n",
    "    pred_bins_over_trials = []\n",
    "    for _ in range(n_trials):\n",
    "        boot_x, boot_y = bootstrap_over_bins(x, y, n_events)\n",
    "        preds = predict_likelihood_over_bins(boot_x, boot_y, clf)\n",
    "        pred_bins = np.argmax(preds, axis=1)\n",
    "        pred_bins_over_trials.append(np.expand_dims(pred_bins, axis=0))\n",
    "    pred_bins_over_trials = np.concatenate(pred_bins_over_trials)\n",
    "    return pred_bins_over_trials\n",
    "\n",
    "\n",
    "def plot_prediction_linearity(input_values, avg_pred, stdev_pred, ref_line_buffer, xlim=None, ylim=None, xlabel=None, ylabel=None):\n",
    "    \"\"\"\n",
    "    input_values : value corresponding to each bin index\n",
    "    avg_pred : ndarray of average prediction per input bin\n",
    "    stdev_pred : ndarray of standard deviation of prediction per input bin \n",
    "    ref_line_buffer : extra amount to extend reference line\n",
    "    xlim : x limits\n",
    "    ylim : y limits\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots()\n",
    "        \n",
    "    ax.scatter(input_values, avg_pred, label=\"Validation Results\", color=\"firebrick\", s=16, zorder=5)\n",
    "    ax.errorbar(input_values, avg_pred, yerr=stdev_pred, fmt=\"none\", elinewidth=0.5, capsize=0.5, color=\"black\", label=\"Std. Dev.\", zorder=10)\n",
    "\n",
    "    ref_ticks = np.linspace(np.min(input_values)-ref_line_buffer, np.max(input_values)+ref_line_buffer, 2)\n",
    "    ax.plot(\n",
    "        ref_ticks, ref_ticks,\n",
    "        label=\"Ref. Line (Slope = 1)\",\n",
    "        color=\"grey\",\n",
    "        linewidth=0.5,\n",
    "        zorder=0\n",
    "    )\n",
    "\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim)\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(ylim)\n",
    "\n",
    "    ax.legend()\n",
    "    if xlabel is not None:\n",
    "        ax.set_xlabel(xlabel) # )\n",
    "    if ylabel is not None:\n",
    "        ax.set_ylabel(ylabel) # r\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"gen\"\n",
    "\n",
    "train_dataset = Aggregated_Signal_Binned_Dataset()\n",
    "train_dataset.load(level, \"train\", \"../../state/new_physics/data/processed\")\n",
    "eval_dataset = Aggregated_Signal_Binned_Dataset()\n",
    "eval_dataset.load(level, \"eval\", \"../../state/new_physics/data/processed\")\n",
    "\n",
    "np.testing.assert_equal(train_dataset.bins, eval_dataset.bins)\n",
    "bin_values = train_dataset.bins\n",
    "\n",
    "train_x = train_dataset.feat.numpy()\n",
    "train_y = train_dataset.labels.numpy()\n",
    "\n",
    "eval_x = eval_dataset.feat.numpy()\n",
    "eval_y = eval_dataset.labels.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "bins, counts = np.unique(train_y, return_counts=True)\n",
    "plt.plot(counts)\n",
    "plt.show()\n",
    "\n",
    "# Evaluation data\n",
    "bins, counts = np.unique(eval_y, return_counts=True)\n",
    "plt.plot(counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balance Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_events_train = 200_000\n",
    "n_events_eval = 75_000\n",
    "\n",
    "bins_train = np.unique(train_y)\n",
    "bins_eval = np.unique(eval_y)\n",
    "np.testing.assert_equal(bins_train, bins_eval)\n",
    "\n",
    "train_x_trimmed = np.concatenate([train_x[train_y==b][:n_events_train] for b in bins_train])\n",
    "train_y_trimmed = np.concatenate([train_y[train_y==b][:n_events_train] for b in bins_train])\n",
    "\n",
    "eval_x_trimmed = np.concatenate([eval_x[eval_y==b][:n_events_eval] for b in bins_train])\n",
    "eval_y_trimmed = np.concatenate([eval_y[eval_y==b][:n_events_eval] for b in bins_train])\n",
    "\n",
    "# Observe - training data\n",
    "bins, counts = np.unique(train_y_trimmed, return_counts=True)\n",
    "plt.plot(counts)\n",
    "plt.show()\n",
    "\n",
    "# Observe - evaluation data\n",
    "bins, counts = np.unique(eval_y_trimmed, return_counts=True)\n",
    "plt.plot(counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the BDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = HistGradientBoostingClassifier(max_iter=100, verbose=5).fit(train_x_trimmed, train_y_trimmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On all data\n",
    "preds = predict_likelihood_over_bins(eval_x_trimmed, eval_y_trimmed, clf)\n",
    "plot_likelihood_over_bins(preds, bin_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On bootstrapped data\n",
    "boot_x, boot_y = bootstrap_over_bins(eval_x_trimmed, eval_y_trimmed, 24_000)\n",
    "preds = predict_likelihood_over_bins(boot_x, boot_y, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_likelihood_over_bins(preds, bin_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over multiple bootstrapped trials\n",
    "\n",
    "pred_bins_over_trials = predict_likelihood_over_bootstrapped_trials(eval_x_trimmed, eval_y_trimmed, 100, 24_000, clf)\n",
    "\n",
    "pred_values_over_trials = bin_values[pred_bins_over_trials]\n",
    "avg_pred_values_over_trials = np.mean(pred_values_over_trials, axis=0)\n",
    "stdev_pred_values_over_trials = np.std(pred_values_over_trials, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_linearity(\n",
    "    bin_values, \n",
    "    avg_pred_values_over_trials, \n",
    "    stdev_pred_values_over_trials, \n",
    "    ref_line_buffer=0.05, \n",
    "    xlim=(-2.25, 1.35), \n",
    "    ylim=(-2.25, 1.35), \n",
    "    xlabel=r\"Actual $\\delta C_9$\", \n",
    "    ylabel=r\"Predicted $\\delta C_9$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maybe_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
