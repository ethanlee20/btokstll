{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import MSELoss, CrossEntropyLoss\n",
    "\n",
    "from helpers.datasets.make_and_save.aggregated_signal import Aggregated_Signal_Dataframe_Handler\n",
    "from helpers.datasets.constants import Names_of_Levels, Names_of_q_Squared_Vetos, Raw_Signal_Trial_Ranges, Numbers_of_Events_per_Set, Names_of_Splits\n",
    "from helpers.experiment.experiment import CNN_Group, Deep_Sets_Group, Event_by_Event_Group\n",
    "from helpers.experiment.results_table import Results_Table\n",
    "from helpers.experiment.constants import Paths_to_Directories\n",
    "from helpers.models.hardware_util import select_device\n",
    "from helpers.experiment.experiment import evaluate_model\n",
    "from helpers.datasets.settings.settings import Binned_Sets_Dataset_Settings\n",
    "from helpers.datasets.datasets import Unbinned_Sets_Dataset, Binned_Sets_Dataset\n",
    "\n",
    "results_table = Results_Table()\n",
    "device = select_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  level in (Names_of_Levels().generator, Names_of_Levels().detector):\n",
    "    for trial_range in Raw_Signal_Trial_Ranges().tuple_:\n",
    "        \n",
    "        Aggregated_Signal_Dataframe_Handler(\n",
    "            path_to_main_datasets_dir=Paths_to_Directories().path_to_main_datasets_dir,\n",
    "            level=level,\n",
    "            trial_range=trial_range\n",
    "        ).make_and_save(Paths_to_Directories().path_to_raw_signal_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helpers.experiment.experiment import evaluate_model\n",
    "# from helpers.datasets.datasets import Unbinned_Sets_Dataset\n",
    "\n",
    "deep_sets_group = Deep_Sets_Group(\n",
    "    num_sets_per_label=50,\n",
    "    num_sets_per_label_sensitivity=2_000,\n",
    "    q_squared_veto=Names_of_q_Squared_Vetos().loose,\n",
    "    std_scale=True,\n",
    "    shuffle=True,\n",
    "    loss_fn=MSELoss(),\n",
    "    learning_rate=3e-4, # 3e-4\n",
    "    learning_rate_scheduler_reduction_factor=0.9, # 0.9\n",
    "    size_of_training_batch=32,\n",
    "    size_of_evaluation_batch=32,\n",
    "    number_of_epochs=100, # 100\n",
    "    number_of_epochs_between_checkpoints=5,\n",
    "    results_table=results_table,\n",
    "    device=device,\n",
    "    bkg_fraction=0.5,\n",
    "    bkg_charge_fraction=0.5\n",
    ")\n",
    "\n",
    "deep_sets_group.evaluate_subset(levels=(Names_of_Levels().generator, Names_of_Levels().detector), nums_events_per_set=Numbers_of_Events_per_Set().tuple_, remake_datasets=False)\n",
    "\n",
    "\n",
    "# evaluate_model(\n",
    "#     model=deep_sets.model,\n",
    "#     evaluation_dataset=Unbinned_Sets_Dataset(deep_sets.training_dataset_settings, remake=True),\n",
    "#     sensitivity_evaluation_dataset=Unbinned_Sets_Dataset(deep_sets.sensitivity_evaluation_dataset_settings, remake=False),\n",
    "#     results_table=results_table,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "\n",
    "# deep_sets_group.train_all(remake_datasets=False)\n",
    "# deep_sets_group.evaluate_all(remake_datasets=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_group = CNN_Group(\n",
    "    num_sets_per_label=50,\n",
    "    num_sets_per_label_sensitivity=2_000,\n",
    "    num_bins_per_dimension=10,\n",
    "    q_squared_veto=Names_of_q_Squared_Vetos().loose,\n",
    "    std_scale=True,\n",
    "    shuffle=True,\n",
    "    loss_fn=MSELoss(),\n",
    "    learning_rate=3e-4,\n",
    "    learning_rate_scheduler_reduction_factor=0.9,\n",
    "    size_of_training_batch=32,\n",
    "    size_of_evaluation_batch=32,\n",
    "    number_of_epochs=100,\n",
    "    number_of_epochs_between_checkpoints=5,\n",
    "    results_table=results_table,\n",
    "    device=device,\n",
    "    bkg_fraction=0.5,\n",
    "    bkg_charge_fraction=0.5\n",
    ")\n",
    "\n",
    "# cnn_group.train_all(remake_datasets=True)\n",
    "# cnn_group.evaluate_all(remake_datasets=True)\n",
    "cnn_group.plot_image_examples_all(remake_datasets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making binned events dataset.\n",
      "Number of NA values: \n",
      " q_squared          0\n",
      "costheta_mu      170\n",
      "costheta_K       760\n",
      "chi              760\n",
      "dc9                0\n",
      "dc9_bin_index      0\n",
      "dtype: int64\n",
      "Removed rows that have a NaN.\n",
      "Applying standand scale.\n",
      "Applied standard scale.\n",
      "Shuffled dataframe.\n",
      "Saved tensor of shape: torch.Size([7732945, 4]) to ..\\..\\state\\new_physics\\data\\processed\\events_binned_det_q2v_loose\\train_features.pt\n",
      "Saved tensor of shape: torch.Size([7732945]) to ..\\..\\state\\new_physics\\data\\processed\\events_binned_det_q2v_loose\\train_labels.pt\n",
      "Saved tensor of shape: torch.Size([44]) to ..\\..\\state\\new_physics\\data\\processed\\events_binned_det_q2v_loose\\train_bin_map.pt\n",
      "Made binned events dataset.\n",
      "Loaded tensor of shape: torch.Size([7732945, 4]) from ..\\..\\state\\new_physics\\data\\processed\\events_binned_det_q2v_loose\\train_features.pt\n",
      "Loaded tensor of shape: torch.Size([7732945]) from ..\\..\\state\\new_physics\\data\\processed\\events_binned_det_q2v_loose\\train_labels.pt\n",
      "Loaded tensor of shape: torch.Size([44]) from ..\\..\\state\\new_physics\\data\\processed\\events_binned_det_q2v_loose\\train_bin_map.pt\n",
      "Making binned events dataset.\n",
      "Number of NA values: \n",
      " q_squared          0\n",
      "costheta_mu      168\n",
      "costheta_K       706\n",
      "chi              706\n",
      "dc9                0\n",
      "dc9_bin_index      0\n",
      "dtype: int64\n",
      "Removed rows that have a NaN.\n",
      "Applying standand scale.\n",
      "Applied standard scale.\n",
      "Shuffled dataframe.\n",
      "Saved tensor of shape: torch.Size([7759244, 4]) to ..\\..\\state\\new_physics\\data\\processed\\events_binned_det_q2v_loose\\eval_features.pt\n",
      "Saved tensor of shape: torch.Size([7759244]) to ..\\..\\state\\new_physics\\data\\processed\\events_binned_det_q2v_loose\\eval_labels.pt\n",
      "Saved tensor of shape: torch.Size([44]) to ..\\..\\state\\new_physics\\data\\processed\\events_binned_det_q2v_loose\\eval_bin_map.pt\n",
      "Made binned events dataset.\n",
      "Loaded tensor of shape: torch.Size([7759244, 4]) from ..\\..\\state\\new_physics\\data\\processed\\events_binned_det_q2v_loose\\eval_features.pt\n",
      "Loaded tensor of shape: torch.Size([7759244]) from ..\\..\\state\\new_physics\\data\\processed\\events_binned_det_q2v_loose\\eval_labels.pt\n",
      "Loaded tensor of shape: torch.Size([44]) from ..\\..\\state\\new_physics\\data\\processed\\events_binned_det_q2v_loose\\eval_bin_map.pt\n",
      "\n",
      "Epoch 0 complete:\n",
      "    Train loss: 3.782192512632124\n",
      "    Eval loss: 3.781803460880322\n",
      "\n",
      "Learning rate: [0.003]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 1 complete:\n",
      "    Train loss: 3.7813035749490314\n",
      "    Eval loss: 3.781572386856715\n",
      "\n",
      "Learning rate: [0.003]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 2 complete:\n",
      "    Train loss: 3.781210625575442\n",
      "    Eval loss: 3.781504512565521\n",
      "\n",
      "Learning rate: [0.003]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 3 complete:\n",
      "    Train loss: 3.7811426439272386\n",
      "    Eval loss: 3.781519757307698\n",
      "\n",
      "Learning rate: [0.00285]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 4 complete:\n",
      "    Train loss: 3.781088678940132\n",
      "    Eval loss: 3.7814894767390657\n",
      "\n",
      "Learning rate: [0.00285]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 5 complete:\n",
      "    Train loss: 3.781056890690446\n",
      "    Eval loss: 3.781367490489714\n",
      "\n",
      "Learning rate: [0.00285]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 6 complete:\n",
      "    Train loss: 3.7810287155006614\n",
      "    Eval loss: 3.781329822946889\n",
      "\n",
      "Learning rate: [0.00285]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 7 complete:\n",
      "    Train loss: 3.78100507706515\n",
      "    Eval loss: 3.7813413002184513\n",
      "\n",
      "Learning rate: [0.0027075]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 8 complete:\n",
      "    Train loss: 3.780991310801832\n",
      "    Eval loss: 3.7812677232991465\n",
      "\n",
      "Learning rate: [0.0027075]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 9 complete:\n",
      "    Train loss: 3.7809548490482108\n",
      "    Eval loss: 3.781408194511848\n",
      "\n",
      "Learning rate: [0.0025721249999999998]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 10 complete:\n",
      "    Train loss: 3.780942828578969\n",
      "    Eval loss: 3.781272542421314\n",
      "\n",
      "Learning rate: [0.0024435187499999996]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 11 complete:\n",
      "    Train loss: 3.780918990194981\n",
      "    Eval loss: 3.781292422927795\n",
      "\n",
      "Learning rate: [0.0023213428124999997]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 12 complete:\n",
      "    Train loss: 3.780901071798336\n",
      "    Eval loss: 3.781244966058812\n",
      "\n",
      "Learning rate: [0.0023213428124999997]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 13 complete:\n",
      "    Train loss: 3.7809021524451327\n",
      "    Eval loss: 3.7812459956089426\n",
      "\n",
      "Learning rate: [0.0022052756718749997]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 14 complete:\n",
      "    Train loss: 3.7808783030653554\n",
      "    Eval loss: 3.781314192752323\n",
      "\n",
      "Learning rate: [0.0020950118882812497]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 15 complete:\n",
      "    Train loss: 3.780862173518136\n",
      "    Eval loss: 3.781245678661773\n",
      "\n",
      "Learning rate: [0.001990261293867187]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 16 complete:\n",
      "    Train loss: 3.7808511628530903\n",
      "    Eval loss: 3.7812409626818395\n",
      "\n",
      "Learning rate: [0.001990261293867187]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 17 complete:\n",
      "    Train loss: 3.780845216398612\n",
      "    Eval loss: 3.781274890009799\n",
      "\n",
      "Learning rate: [0.0018907482291738277]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 18 complete:\n",
      "    Train loss: 3.780826735569717\n",
      "    Eval loss: 3.7812638995339904\n",
      "\n",
      "Learning rate: [0.0017962108177151362]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 19 complete:\n",
      "    Train loss: 3.7808284559264873\n",
      "    Eval loss: 3.7812288235525737\n",
      "\n",
      "Learning rate: [0.0017962108177151362]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 20 complete:\n",
      "    Train loss: 3.7808176284231183\n",
      "    Eval loss: 3.781221146626066\n",
      "\n",
      "Learning rate: [0.0017962108177151362]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 21 complete:\n",
      "    Train loss: 3.780819918649522\n",
      "    Eval loss: 3.781261578672041\n",
      "\n",
      "Learning rate: [0.0017064002768293794]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 22 complete:\n",
      "    Train loss: 3.780803908660291\n",
      "    Eval loss: 3.78124600763732\n",
      "\n",
      "Learning rate: [0.0016210802629879103]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 23 complete:\n",
      "    Train loss: 3.780800786806713\n",
      "    Eval loss: 3.7812539676815233\n",
      "\n",
      "Learning rate: [0.0015400262498385148]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 24 complete:\n",
      "    Train loss: 3.780796633365635\n",
      "    Eval loss: 3.781198620301669\n",
      "\n",
      "Learning rate: [0.0015400262498385148]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 25 complete:\n",
      "    Train loss: 3.780786079268746\n",
      "    Eval loss: 3.7812656010856376\n",
      "\n",
      "Learning rate: [0.001463024937346589]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 26 complete:\n",
      "    Train loss: 3.780778261028125\n",
      "    Eval loss: 3.781163079372393\n",
      "\n",
      "Learning rate: [0.001463024937346589]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 27 complete:\n",
      "    Train loss: 3.7807767593793846\n",
      "    Eval loss: 3.7811753845869163\n",
      "\n",
      "Learning rate: [0.0013898736904792595]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 28 complete:\n",
      "    Train loss: 3.7807748524163416\n",
      "    Eval loss: 3.7812204330076757\n",
      "\n",
      "Learning rate: [0.0013203800059552965]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 29 complete:\n",
      "    Train loss: 3.780764034922351\n",
      "    Eval loss: 3.7812435597998246\n",
      "\n",
      "Learning rate: [0.0012543610056575316]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 30 complete:\n",
      "    Train loss: 3.7807599642870238\n",
      "    Eval loss: 3.7812064684295716\n",
      "\n",
      "Learning rate: [0.001191642955374655]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 31 complete:\n",
      "    Train loss: 3.7807500704723025\n",
      "    Eval loss: 3.781163872394746\n",
      "\n",
      "Learning rate: [0.001132060807605922]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 32 complete:\n",
      "    Train loss: 3.780747732063779\n",
      "    Eval loss: 3.7811995210296123\n",
      "\n",
      "Learning rate: [0.0010754577672256258]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 33 complete:\n",
      "    Train loss: 3.7807395571966027\n",
      "    Eval loss: 3.781204382653362\n",
      "\n",
      "Learning rate: [0.0010216848788643445]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 34 complete:\n",
      "    Train loss: 3.7807311527775265\n",
      "    Eval loss: 3.7811975405493516\n",
      "\n",
      "Learning rate: [0.0009706006349211272]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 35 complete:\n",
      "    Train loss: 3.780726627152512\n",
      "    Eval loss: 3.781176085399952\n",
      "\n",
      "Learning rate: [0.0009220706031750709]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 36 complete:\n",
      "    Train loss: 3.780723215552772\n",
      "    Eval loss: 3.781176096023965\n",
      "\n",
      "Learning rate: [0.0008759670730163172]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 37 complete:\n",
      "    Train loss: 3.780717692856001\n",
      "    Eval loss: 3.7812041352513988\n",
      "\n",
      "Learning rate: [0.0008321687193655013]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 38 complete:\n",
      "    Train loss: 3.7807161564519527\n",
      "    Eval loss: 3.7811860133645676\n",
      "\n",
      "Learning rate: [0.0007905602833972262]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 39 complete:\n",
      "    Train loss: 3.780708419708284\n",
      "    Eval loss: 3.7811898976625287\n",
      "\n",
      "Learning rate: [0.0007510322692273649]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 40 complete:\n",
      "    Train loss: 3.780707817894228\n",
      "    Eval loss: 3.78116855223654\n",
      "\n",
      "Learning rate: [0.0007134806557659966]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 41 complete:\n",
      "    Train loss: 3.780699695851857\n",
      "    Eval loss: 3.781157245443495\n",
      "\n",
      "Learning rate: [0.0007134806557659966]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 42 complete:\n",
      "    Train loss: 3.7807035881029143\n",
      "    Eval loss: 3.7811559704165236\n",
      "\n",
      "Learning rate: [0.0007134806557659966]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 43 complete:\n",
      "    Train loss: 3.7807009796262685\n",
      "    Eval loss: 3.7811577432249988\n",
      "\n",
      "Learning rate: [0.0006778066229776968]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 44 complete:\n",
      "    Train loss: 3.7806986724469667\n",
      "    Eval loss: 3.781153826214116\n",
      "\n",
      "Learning rate: [0.0006778066229776968]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 45 complete:\n",
      "    Train loss: 3.7806955478107382\n",
      "    Eval loss: 3.7811609797298926\n",
      "\n",
      "Learning rate: [0.0006439162918288119]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 46 complete:\n",
      "    Train loss: 3.7806920299795337\n",
      "    Eval loss: 3.781163887659445\n",
      "\n",
      "Learning rate: [0.0006117204772373713]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 47 complete:\n",
      "    Train loss: 3.780691374186632\n",
      "    Eval loss: 3.7811570476869467\n",
      "\n",
      "Learning rate: [0.0005811344533755027]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 48 complete:\n",
      "    Train loss: 3.7806886585671085\n",
      "    Eval loss: 3.781146056023051\n",
      "\n",
      "Learning rate: [0.0005811344533755027]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 49 complete:\n",
      "    Train loss: 3.7806892755109236\n",
      "    Eval loss: 3.781158786934789\n",
      "\n",
      "Learning rate: [0.0005520777307067275]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 50 complete:\n",
      "    Train loss: 3.7806824961068997\n",
      "    Eval loss: 3.781171591967573\n",
      "\n",
      "Learning rate: [0.0005244738441713911]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 51 complete:\n",
      "    Train loss: 3.780684291954055\n",
      "    Eval loss: 3.781143530998984\n",
      "\n",
      "Learning rate: [0.0005244738441713911]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 52 complete:\n",
      "    Train loss: 3.7806797614966423\n",
      "    Eval loss: 3.7811755662867306\n",
      "\n",
      "Learning rate: [0.0004982501519628216]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 53 complete:\n",
      "    Train loss: 3.780679172169713\n",
      "    Eval loss: 3.781145941235564\n",
      "\n",
      "Learning rate: [0.0004733376443646804]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 54 complete:\n",
      "    Train loss: 3.7806783006549187\n",
      "    Eval loss: 3.7811614867671284\n",
      "\n",
      "Learning rate: [0.00044967076214644637]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 55 complete:\n",
      "    Train loss: 3.7806756311823646\n",
      "    Eval loss: 3.781163503496877\n",
      "\n",
      "Learning rate: [0.00042718722403912405]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 56 complete:\n",
      "    Train loss: 3.780672469835213\n",
      "    Eval loss: 3.7811674184761235\n",
      "\n",
      "Learning rate: [0.0004058278628371678]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 57 complete:\n",
      "    Train loss: 3.780670004941993\n",
      "    Eval loss: 3.781147478738014\n",
      "\n",
      "Learning rate: [0.0003855364696953094]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 58 complete:\n",
      "    Train loss: 3.7806669863633324\n",
      "    Eval loss: 3.781143841218569\n",
      "\n",
      "Learning rate: [0.0003662596462105439]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 59 complete:\n",
      "    Train loss: 3.7806644169602266\n",
      "    Eval loss: 3.7811472719797123\n",
      "\n",
      "Learning rate: [0.0003479466639000167]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 60 complete:\n",
      "    Train loss: 3.7806649339954337\n",
      "    Eval loss: 3.7811451971590877\n",
      "\n",
      "Learning rate: [0.00033054933070501586]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 61 complete:\n",
      "    Train loss: 3.780660335730186\n",
      "    Eval loss: 3.7811639841003872\n",
      "\n",
      "Learning rate: [0.00031402186416976504]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 62 complete:\n",
      "    Train loss: 3.7806591987322715\n",
      "    Eval loss: 3.7811556050078403\n",
      "\n",
      "Learning rate: [0.0002983207709612768]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 63 complete:\n",
      "    Train loss: 3.780657120239204\n",
      "    Eval loss: 3.7811545002540106\n",
      "\n",
      "Learning rate: [0.00028340473241321294]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 64 complete:\n",
      "    Train loss: 3.7806575763557873\n",
      "    Eval loss: 3.7811531019932336\n",
      "\n",
      "Learning rate: [0.0002692344957925523]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 65 complete:\n",
      "    Train loss: 3.7806554560432084\n",
      "    Eval loss: 3.7811393074443287\n",
      "\n",
      "Learning rate: [0.0002692344957925523]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 66 complete:\n",
      "    Train loss: 3.7806518222272225\n",
      "    Eval loss: 3.781158030502005\n",
      "\n",
      "Learning rate: [0.0002557727710029247]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 67 complete:\n",
      "    Train loss: 3.780652001203852\n",
      "    Eval loss: 3.781145631635802\n",
      "\n",
      "Learning rate: [0.00024298413245277842]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 68 complete:\n",
      "    Train loss: 3.780651255763296\n",
      "    Eval loss: 3.7811384360165885\n",
      "\n",
      "Learning rate: [0.00024298413245277842]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 69 complete:\n",
      "    Train loss: 3.780650358814122\n",
      "    Eval loss: 3.7811363408149044\n",
      "\n",
      "Learning rate: [0.00024298413245277842]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 70 complete:\n",
      "    Train loss: 3.7806496812650785\n",
      "    Eval loss: 3.781149570003921\n",
      "\n",
      "Learning rate: [0.0002308349258301395]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 71 complete:\n",
      "    Train loss: 3.7806489452572687\n",
      "    Eval loss: 3.781136765967787\n",
      "\n",
      "Learning rate: [0.0002192931795386325]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 72 complete:\n",
      "    Train loss: 3.780649247656111\n",
      "    Eval loss: 3.7811427103691635\n",
      "\n",
      "Learning rate: [0.00020832852056170087]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 73 complete:\n",
      "    Train loss: 3.7806478613825947\n",
      "    Eval loss: 3.781143009294436\n",
      "\n",
      "Learning rate: [0.0001979120945336158]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 74 complete:\n",
      "    Train loss: 3.7806466625509834\n",
      "    Eval loss: 3.781140078305619\n",
      "\n",
      "Learning rate: [0.000188016489806935]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 75 complete:\n",
      "    Train loss: 3.7806452764636638\n",
      "    Eval loss: 3.7811411131560106\n",
      "\n",
      "Learning rate: [0.00017861566531658825]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 76 complete:\n",
      "    Train loss: 3.7806442608505924\n",
      "    Eval loss: 3.781141803484266\n",
      "\n",
      "Learning rate: [0.00016968488205075882]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 77 complete:\n",
      "    Train loss: 3.7806431431676186\n",
      "    Eval loss: 3.781135185500215\n",
      "\n",
      "Learning rate: [0.00016968488205075882]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 78 complete:\n",
      "    Train loss: 3.7806429830781934\n",
      "    Eval loss: 3.781138872495212\n",
      "\n",
      "Learning rate: [0.00016120063794822088]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 79 complete:\n",
      "    Train loss: 3.7806432517161626\n",
      "    Eval loss: 3.781136320992085\n",
      "\n",
      "Learning rate: [0.0001531406060508098]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 80 complete:\n",
      "    Train loss: 3.780640887537895\n",
      "    Eval loss: 3.7811384841671005\n",
      "\n",
      "Learning rate: [0.0001454835757482693]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 81 complete:\n",
      "    Train loss: 3.7806406646323145\n",
      "    Eval loss: 3.7811497086938535\n",
      "\n",
      "Learning rate: [0.00013820939696085585]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 82 complete:\n",
      "    Train loss: 3.780639579926493\n",
      "    Eval loss: 3.7811357769668317\n",
      "\n",
      "Learning rate: [0.00013129892711281305]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 83 complete:\n",
      "    Train loss: 3.7806389946989953\n",
      "    Eval loss: 3.7811379237597467\n",
      "\n",
      "Learning rate: [0.0001247339807571724]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 84 complete:\n",
      "    Train loss: 3.7806378395957285\n",
      "    Eval loss: 3.7811388907747796\n",
      "\n",
      "Learning rate: [0.00011849728171931376]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 85 complete:\n",
      "    Train loss: 3.780636865040846\n",
      "    Eval loss: 3.7811358835308178\n",
      "\n",
      "Learning rate: [0.00011257241763334806]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 86 complete:\n",
      "    Train loss: 3.780637764288355\n",
      "    Eval loss: 3.781141206582047\n",
      "\n",
      "Learning rate: [0.00010694379675168066]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 87 complete:\n",
      "    Train loss: 3.7806357334813714\n",
      "    Eval loss: 3.7811365716474774\n",
      "\n",
      "Learning rate: [0.00010159660691409662]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 88 complete:\n",
      "    Train loss: 3.780636413595859\n",
      "    Eval loss: 3.781140525704195\n",
      "\n",
      "Learning rate: [9.65167765683918e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 89 complete:\n",
      "    Train loss: 3.780635114662977\n",
      "    Eval loss: 3.781133211793609\n",
      "\n",
      "Learning rate: [9.65167765683918e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 90 complete:\n",
      "    Train loss: 3.780634262615202\n",
      "    Eval loss: 3.781133730872609\n",
      "\n",
      "Learning rate: [9.16909377399722e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 91 complete:\n",
      "    Train loss: 3.7806347025381246\n",
      "    Eval loss: 3.7811413781950094\n",
      "\n",
      "Learning rate: [8.710639085297359e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 92 complete:\n",
      "    Train loss: 3.7806339298983094\n",
      "    Eval loss: 3.781136846341574\n",
      "\n",
      "Learning rate: [8.275107131032491e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 93 complete:\n",
      "    Train loss: 3.780633421140141\n",
      "    Eval loss: 3.7811389193676375\n",
      "\n",
      "Learning rate: [7.861351774480866e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 94 complete:\n",
      "    Train loss: 3.7806333138006964\n",
      "    Eval loss: 3.781136459164835\n",
      "\n",
      "Learning rate: [7.468284185756822e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 95 complete:\n",
      "    Train loss: 3.7806314134437096\n",
      "    Eval loss: 3.7811415348268493\n",
      "\n",
      "Learning rate: [7.094869976468981e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 96 complete:\n",
      "    Train loss: 3.7806319743093706\n",
      "    Eval loss: 3.7811416247081833\n",
      "\n",
      "Learning rate: [6.740126477645532e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 97 complete:\n",
      "    Train loss: 3.780631666937669\n",
      "    Eval loss: 3.7811338713242497\n",
      "\n",
      "Learning rate: [6.403120153763255e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 98 complete:\n",
      "    Train loss: 3.7806308955267953\n",
      "    Eval loss: 3.781139121207852\n",
      "\n",
      "Learning rate: [6.082964146075092e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 99 complete:\n",
      "    Train loss: 3.7806298711904898\n",
      "    Eval loss: 3.7811380708971787\n",
      "\n",
      "Learning rate: [5.778815938771337e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 100 complete:\n",
      "    Train loss: 3.780630429644486\n",
      "    Eval loss: 3.7811367444089\n",
      "\n",
      "Learning rate: [5.48987514183277e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 101 complete:\n",
      "    Train loss: 3.7806296024808934\n",
      "    Eval loss: 3.781138998795289\n",
      "\n",
      "Learning rate: [5.215381384741131e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 102 complete:\n",
      "    Train loss: 3.78062866984616\n",
      "    Eval loss: 3.7811350050425014\n",
      "\n",
      "Learning rate: [4.954612315504074e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 103 complete:\n",
      "    Train loss: 3.7806284022345196\n",
      "    Eval loss: 3.781135738345902\n",
      "\n",
      "Learning rate: [4.70688169972887e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 104 complete:\n",
      "    Train loss: 3.780627279870572\n",
      "    Eval loss: 3.7811395400908774\n",
      "\n",
      "Learning rate: [4.4715376147424265e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 105 complete:\n",
      "    Train loss: 3.7806275836430427\n",
      "    Eval loss: 3.781140503332618\n",
      "\n",
      "Learning rate: [4.247960734005305e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 106 complete:\n",
      "    Train loss: 3.7806273758365734\n",
      "    Eval loss: 3.7811384343443817\n",
      "\n",
      "Learning rate: [4.03556269730504e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 107 complete:\n",
      "    Train loss: 3.780627416835559\n",
      "    Eval loss: 3.781140286299478\n",
      "\n",
      "Learning rate: [3.833784562439787e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 108 complete:\n",
      "    Train loss: 3.780627222427585\n",
      "    Eval loss: 3.7811399542936996\n",
      "\n",
      "Learning rate: [3.6420953343177974e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 109 complete:\n",
      "    Train loss: 3.7806279953811037\n",
      "    Eval loss: 3.7811349873241524\n",
      "\n",
      "Learning rate: [3.4599905676019076e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 110 complete:\n",
      "    Train loss: 3.7806275978478103\n",
      "    Eval loss: 3.7811373345267136\n",
      "\n",
      "Learning rate: [3.286991039221812e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 111 complete:\n",
      "    Train loss: 3.780627021613118\n",
      "    Eval loss: 3.7811361092324236\n",
      "\n",
      "Learning rate: [3.122641487260721e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 112 complete:\n",
      "    Train loss: 3.7806268137682992\n",
      "    Eval loss: 3.781135545864223\n",
      "\n",
      "Learning rate: [2.9665094128976848e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 113 complete:\n",
      "    Train loss: 3.7806267416803103\n",
      "    Eval loss: 3.7811364110139145\n",
      "\n",
      "Learning rate: [2.8181839422528005e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 114 complete:\n",
      "    Train loss: 3.7806252990653184\n",
      "    Eval loss: 3.781135275688629\n",
      "\n",
      "Learning rate: [2.6772747451401602e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 115 complete:\n",
      "    Train loss: 3.780626177678251\n",
      "    Eval loss: 3.7811351587978943\n",
      "\n",
      "Learning rate: [2.5434110078831522e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 116 complete:\n",
      "    Train loss: 3.7806255174207615\n",
      "    Eval loss: 3.7811375847367996\n",
      "\n",
      "Learning rate: [2.4162404574889944e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 117 complete:\n",
      "    Train loss: 3.7806257895312116\n",
      "    Eval loss: 3.7811401939424645\n",
      "\n",
      "Learning rate: [2.2954284346145446e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 118 complete:\n",
      "    Train loss: 3.7806260210449505\n",
      "    Eval loss: 3.7811379538814442\n",
      "\n",
      "Learning rate: [2.1806570128838174e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 119 complete:\n",
      "    Train loss: 3.780624441866409\n",
      "    Eval loss: 3.781136486976216\n",
      "\n",
      "Learning rate: [2.0716241622396264e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 120 complete:\n",
      "    Train loss: 3.7806246706227675\n",
      "    Eval loss: 3.7811375343582725\n",
      "\n",
      "Learning rate: [1.968042954127645e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 121 complete:\n",
      "    Train loss: 3.780624651252395\n",
      "    Eval loss: 3.7811382398191515\n",
      "\n",
      "Learning rate: [1.8696408064212627e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 122 complete:\n",
      "    Train loss: 3.780625577528602\n",
      "    Eval loss: 3.7811378777624767\n",
      "\n",
      "Learning rate: [1.7761587661001995e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 123 complete:\n",
      "    Train loss: 3.780623659675308\n",
      "    Eval loss: 3.781139319296693\n",
      "\n",
      "Learning rate: [1.6873508277951895e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 124 complete:\n",
      "    Train loss: 3.7806254865903806\n",
      "    Eval loss: 3.781136407121887\n",
      "\n",
      "Learning rate: [1.60298328640543e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 125 complete:\n",
      "    Train loss: 3.780624556529164\n",
      "    Eval loss: 3.781136590067826\n",
      "\n",
      "Learning rate: [1.5228341220851583e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 126 complete:\n",
      "    Train loss: 3.7806243997504185\n",
      "    Eval loss: 3.7811388134411947\n",
      "\n",
      "Learning rate: [1.4466924159809002e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 127 complete:\n",
      "    Train loss: 3.7806231522478684\n",
      "    Eval loss: 3.781135782741924\n",
      "\n",
      "Learning rate: [1.3743577951818552e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 128 complete:\n",
      "    Train loss: 3.7806247165582794\n",
      "    Eval loss: 3.781136092499931\n",
      "\n",
      "Learning rate: [1.3056399054227623e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 129 complete:\n",
      "    Train loss: 3.7806237688324327\n",
      "    Eval loss: 3.7811374886614764\n",
      "\n",
      "Learning rate: [1.2403579101516242e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 130 complete:\n",
      "    Train loss: 3.780623311426597\n",
      "    Eval loss: 3.7811385386410596\n",
      "\n",
      "Learning rate: [1.178340014644043e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 131 complete:\n",
      "    Train loss: 3.780623611989854\n",
      "    Eval loss: 3.781137654848247\n",
      "\n",
      "Learning rate: [1.1194230139118406e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 132 complete:\n",
      "    Train loss: 3.7806240369187805\n",
      "    Eval loss: 3.7811378011574437\n",
      "\n",
      "Learning rate: [1.0634518632162485e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 133 complete:\n",
      "    Train loss: 3.780623027274222\n",
      "    Eval loss: 3.781137167972958\n",
      "\n",
      "Learning rate: [1.010279270055436e-05]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 134 complete:\n",
      "    Train loss: 3.7806224634200882\n",
      "    Eval loss: 3.7811374119709984\n",
      "\n",
      "Learning rate: [9.597653065526641e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 135 complete:\n",
      "    Train loss: 3.7806242650507094\n",
      "    Eval loss: 3.7811369749221164\n",
      "\n",
      "Learning rate: [9.11777041225031e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 136 complete:\n",
      "    Train loss: 3.7806224658834697\n",
      "    Eval loss: 3.7811380769545058\n",
      "\n",
      "Learning rate: [8.661881891637793e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 137 complete:\n",
      "    Train loss: 3.7806236245409304\n",
      "    Eval loss: 3.781138063614028\n",
      "\n",
      "Learning rate: [8.228787797055902e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 138 complete:\n",
      "    Train loss: 3.7806227517519106\n",
      "    Eval loss: 3.7811370032977063\n",
      "\n",
      "Learning rate: [7.817348407203107e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 139 complete:\n",
      "    Train loss: 3.7806229098425743\n",
      "    Eval loss: 3.781137017901904\n",
      "\n",
      "Learning rate: [7.426480986842951e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 140 complete:\n",
      "    Train loss: 3.7806237708427863\n",
      "    Eval loss: 3.781136966725253\n",
      "\n",
      "Learning rate: [7.055156937500804e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 141 complete:\n",
      "    Train loss: 3.780623425016455\n",
      "    Eval loss: 3.781139968913546\n",
      "\n",
      "Learning rate: [6.702399090625764e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 142 complete:\n",
      "    Train loss: 3.7806222639813907\n",
      "    Eval loss: 3.7811368514158903\n",
      "\n",
      "Learning rate: [6.367279136094475e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 143 complete:\n",
      "    Train loss: 3.7806234641794547\n",
      "    Eval loss: 3.7811383685917264\n",
      "\n",
      "Learning rate: [6.048915179289751e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 144 complete:\n",
      "    Train loss: 3.780622670565326\n",
      "    Eval loss: 3.781138098528625\n",
      "\n",
      "Learning rate: [5.746469420325264e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 145 complete:\n",
      "    Train loss: 3.7806237651452625\n",
      "    Eval loss: 3.781137249835721\n",
      "\n",
      "Learning rate: [5.459145949309e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 146 complete:\n",
      "    Train loss: 3.7806223031225406\n",
      "    Eval loss: 3.781138412868602\n",
      "\n",
      "Learning rate: [5.18618865184355e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 147 complete:\n",
      "    Train loss: 3.780622297682744\n",
      "    Eval loss: 3.7811393643483604\n",
      "\n",
      "Learning rate: [4.926879219251373e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 148 complete:\n",
      "    Train loss: 3.7806222638243097\n",
      "    Eval loss: 3.7811382938351756\n",
      "\n",
      "Learning rate: [4.680535258288804e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 149 complete:\n",
      "    Train loss: 3.780622290179294\n",
      "    Eval loss: 3.781138589727759\n",
      "\n",
      "Learning rate: [4.446508495374363e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 150 complete:\n",
      "    Train loss: 3.780622238628241\n",
      "    Eval loss: 3.7811370287797406\n",
      "\n",
      "Learning rate: [4.224183070605645e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 151 complete:\n",
      "    Train loss: 3.7806230456973795\n",
      "    Eval loss: 3.7811380523169564\n",
      "\n",
      "Learning rate: [4.012973917075362e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 152 complete:\n",
      "    Train loss: 3.780622635891088\n",
      "    Eval loss: 3.781136745711408\n",
      "\n",
      "Learning rate: [3.812325221221594e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 153 complete:\n",
      "    Train loss: 3.7806221174786385\n",
      "    Eval loss: 3.781138386474533\n",
      "\n",
      "Learning rate: [3.621708960160514e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 154 complete:\n",
      "    Train loss: 3.7806234151575424\n",
      "    Eval loss: 3.7811361321458077\n",
      "\n",
      "Learning rate: [3.440623512152488e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 155 complete:\n",
      "    Train loss: 3.7806224640272386\n",
      "    Eval loss: 3.781136599442269\n",
      "\n",
      "Learning rate: [3.2685923365448633e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 156 complete:\n",
      "    Train loss: 3.7806224043974206\n",
      "    Eval loss: 3.7811360401497662\n",
      "\n",
      "Learning rate: [3.10516271971762e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 157 complete:\n",
      "    Train loss: 3.7806223195916258\n",
      "    Eval loss: 3.7811378857344415\n",
      "\n",
      "Learning rate: [2.949904583731739e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 158 complete:\n",
      "    Train loss: 3.7806227154871217\n",
      "    Eval loss: 3.7811383602778443\n",
      "\n",
      "Learning rate: [2.802409354545152e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 159 complete:\n",
      "    Train loss: 3.7806212782684407\n",
      "    Eval loss: 3.781138881256755\n",
      "\n",
      "Learning rate: [2.662288886817894e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 160 complete:\n",
      "    Train loss: 3.7806221814344476\n",
      "    Eval loss: 3.7811400350641664\n",
      "\n",
      "Learning rate: [2.5291744424769995e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 161 complete:\n",
      "    Train loss: 3.7806216980136655\n",
      "    Eval loss: 3.7811386856213276\n",
      "\n",
      "Learning rate: [2.4027157203531494e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 162 complete:\n",
      "    Train loss: 3.780622076314089\n",
      "    Eval loss: 3.7811369143930444\n",
      "\n",
      "Learning rate: [2.2825799343354918e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 163 complete:\n",
      "    Train loss: 3.780621783724495\n",
      "    Eval loss: 3.7811369171731606\n",
      "\n",
      "Learning rate: [2.168450937618717e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 164 complete:\n",
      "    Train loss: 3.7806224886750757\n",
      "    Eval loss: 3.7811373642353834\n",
      "\n",
      "Learning rate: [2.0600283907377814e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 165 complete:\n",
      "    Train loss: 3.7806216972165414\n",
      "    Eval loss: 3.781138909537497\n",
      "\n",
      "Learning rate: [1.9570269712008923e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 166 complete:\n",
      "    Train loss: 3.7806221280879506\n",
      "    Eval loss: 3.7811382524928887\n",
      "\n",
      "Learning rate: [1.8591756226408476e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 167 complete:\n",
      "    Train loss: 3.7806213510839664\n",
      "    Eval loss: 3.781136541473588\n",
      "\n",
      "Learning rate: [1.7662168415088052e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 168 complete:\n",
      "    Train loss: 3.7806215141647646\n",
      "    Eval loss: 3.7811389283004737\n",
      "\n",
      "Learning rate: [1.6779059994333648e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 169 complete:\n",
      "    Train loss: 3.7806221421667283\n",
      "    Eval loss: 3.781134491027371\n",
      "\n",
      "Learning rate: [1.5940106994616965e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 170 complete:\n",
      "    Train loss: 3.78062271763398\n",
      "    Eval loss: 3.78113950593925\n",
      "\n",
      "Learning rate: [1.5143101644886117e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 171 complete:\n",
      "    Train loss: 3.7806227128786256\n",
      "    Eval loss: 3.781137255547957\n",
      "\n",
      "Learning rate: [1.438594656264181e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 172 complete:\n",
      "    Train loss: 3.7806213943794607\n",
      "    Eval loss: 3.7811371980596307\n",
      "\n",
      "Learning rate: [1.3666649234509719e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 173 complete:\n",
      "    Train loss: 3.7806215456529313\n",
      "    Eval loss: 3.781136598219218\n",
      "\n",
      "Learning rate: [1.2983316772784233e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 174 complete:\n",
      "    Train loss: 3.7806215308410818\n",
      "    Eval loss: 3.781134585924812\n",
      "\n",
      "Learning rate: [1.233415093414502e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 175 complete:\n",
      "    Train loss: 3.780621812338338\n",
      "    Eval loss: 3.7811385052165214\n",
      "\n",
      "Learning rate: [1.171744338743777e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 176 complete:\n",
      "    Train loss: 3.7806216630028295\n",
      "    Eval loss: 3.781138056812704\n",
      "\n",
      "Learning rate: [1.113157121806588e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 177 complete:\n",
      "    Train loss: 3.7806213950096645\n",
      "    Eval loss: 3.7811377680391076\n",
      "\n",
      "Learning rate: [1.0574992657162585e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 178 complete:\n",
      "    Train loss: 3.780622198310732\n",
      "    Eval loss: 3.781137334698337\n",
      "\n",
      "Learning rate: [1.0046243024304456e-06]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 179 complete:\n",
      "    Train loss: 3.780622444535324\n",
      "    Eval loss: 3.7811356315137328\n",
      "\n",
      "Learning rate: [9.543930873089232e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 180 complete:\n",
      "    Train loss: 3.7806223730899915\n",
      "    Eval loss: 3.781136565804816\n",
      "\n",
      "Learning rate: [9.06673432943477e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 181 complete:\n",
      "    Train loss: 3.7806229720978526\n",
      "    Eval loss: 3.781135869214382\n",
      "\n",
      "Learning rate: [8.613397612963031e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 182 complete:\n",
      "    Train loss: 3.7806233292161453\n",
      "    Eval loss: 3.781138525254974\n",
      "\n",
      "Learning rate: [8.182727732314878e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 183 complete:\n",
      "    Train loss: 3.7806238526106015\n",
      "    Eval loss: 3.781136343685737\n",
      "\n",
      "Learning rate: [7.773591345699134e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 184 complete:\n",
      "    Train loss: 3.7806227222647744\n",
      "    Eval loss: 3.781136632971204\n",
      "\n",
      "Learning rate: [7.384911778414176e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 185 complete:\n",
      "    Train loss: 3.7806227177974505\n",
      "    Eval loss: 3.7811382827583135\n",
      "\n",
      "Learning rate: [7.015666189493467e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 186 complete:\n",
      "    Train loss: 3.7806227915823984\n",
      "    Eval loss: 3.7811400949554392\n",
      "\n",
      "Learning rate: [6.664882880018793e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 187 complete:\n",
      "    Train loss: 3.780623558814467\n",
      "    Eval loss: 3.781139684295817\n",
      "\n",
      "Learning rate: [6.331638736017853e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 188 complete:\n",
      "    Train loss: 3.780623247770718\n",
      "    Eval loss: 3.7811378580826194\n",
      "\n",
      "Learning rate: [6.01505679921696e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 189 complete:\n",
      "    Train loss: 3.780621428663665\n",
      "    Eval loss: 3.781137187789826\n",
      "\n",
      "Learning rate: [5.714303959256111e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 190 complete:\n",
      "    Train loss: 3.7806231532418666\n",
      "    Eval loss: 3.781137648938024\n",
      "\n",
      "Learning rate: [5.428588761293305e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 191 complete:\n",
      "    Train loss: 3.7806230319814484\n",
      "    Eval loss: 3.781138997179116\n",
      "\n",
      "Learning rate: [5.15715932322864e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 192 complete:\n",
      "    Train loss: 3.7806219302933934\n",
      "    Eval loss: 3.781138192228614\n",
      "\n",
      "Learning rate: [4.899301357067207e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 193 complete:\n",
      "    Train loss: 3.780622869092582\n",
      "    Eval loss: 3.7811365111666975\n",
      "\n",
      "Learning rate: [4.6543362892138464e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 194 complete:\n",
      "    Train loss: 3.780621832860258\n",
      "    Eval loss: 3.78113668311463\n",
      "\n",
      "Learning rate: [4.421619474753154e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 195 complete:\n",
      "    Train loss: 3.780621472685889\n",
      "    Eval loss: 3.781139545590247\n",
      "\n",
      "Learning rate: [4.200538501015496e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 196 complete:\n",
      "    Train loss: 3.7806227967337795\n",
      "    Eval loss: 3.781136986115994\n",
      "\n",
      "Learning rate: [3.990511575964721e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 197 complete:\n",
      "    Train loss: 3.780623717949275\n",
      "    Eval loss: 3.7811384363697442\n",
      "\n",
      "Learning rate: [3.790985997166485e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 198 complete:\n",
      "    Train loss: 3.7806221826112316\n",
      "    Eval loss: 3.781138789593149\n",
      "\n",
      "Learning rate: [3.60143669730816e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "\n",
      "Epoch 199 complete:\n",
      "    Train loss: 3.7806213276747602\n",
      "    Eval loss: 3.7811373038144067\n",
      "\n",
      "Learning rate: [3.421364862442752e-07]\n",
      "Peak GPU memory usage:\n",
      "0.03476 GB\n",
      "Completed training.\n",
      "Unloaded datasets.\n",
      "Unloaded datasets.\n",
      "Making binned sets dataset.\n",
      "Number of NA values: \n",
      " q_squared          0\n",
      "costheta_mu      168\n",
      "costheta_K       706\n",
      "chi              706\n",
      "dc9                0\n",
      "dc9_bin_index      0\n",
      "dtype: int64\n",
      "Removed rows that have a NaN.\n",
      "Applying standand scale.\n",
      "Applied standard scale.\n",
      "Shuffled dataframe.\n",
      "Saved tensor of shape: torch.Size([2200, 24000, 4]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_det_q2v_loose\\24000_eval_features.pt\n",
      "Saved tensor of shape: torch.Size([2200]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_det_q2v_loose\\24000_eval_labels.pt\n",
      "Saved tensor of shape: torch.Size([44]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_det_q2v_loose\\24000_eval_bin_map.pt\n",
      "Made binned sets dataset.\n",
      "Loaded tensor of shape: torch.Size([2200, 24000, 4]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_det_q2v_loose\\24000_eval_features.pt\n",
      "Loaded tensor of shape: torch.Size([2200]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_det_q2v_loose\\24000_eval_labels.pt\n",
      "Loaded tensor of shape: torch.Size([44]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_det_q2v_loose\\24000_eval_bin_map.pt\n",
      "Making binned sets dataset.\n",
      "Number of NA values: \n",
      " q_squared          0\n",
      "costheta_mu      168\n",
      "costheta_K       706\n",
      "chi              706\n",
      "dc9                0\n",
      "dc9_bin_index      0\n",
      "dtype: int64\n",
      "Removed rows that have a NaN.\n",
      "Applying standand scale.\n",
      "Applied standard scale.\n",
      "Shuffled dataframe.\n",
      "Saved tensor of shape: torch.Size([2000, 24000, 4]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_det_q2v_loose\\24000_eval_sens_features.pt\n",
      "Saved tensor of shape: torch.Size([2000]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_det_q2v_loose\\24000_eval_sens_labels.pt\n",
      "Saved tensor of shape: torch.Size([44]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_det_q2v_loose\\24000_eval_sens_bin_map.pt\n",
      "Made binned sets dataset.\n",
      "Loaded tensor of shape: torch.Size([2000, 24000, 4]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_det_q2v_loose\\24000_eval_sens_features.pt\n",
      "Loaded tensor of shape: torch.Size([2000]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_det_q2v_loose\\24000_eval_sens_labels.pt\n",
      "Loaded tensor of shape: torch.Size([44]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_det_q2v_loose\\24000_eval_sens_bin_map.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tetha\\Desktop\\btokstll\\logic\\scripts\\helpers\\experiment\\results_table.py:49: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  self.table.loc[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unloaded datasets.\n",
      "Unloaded datasets.\n"
     ]
    }
   ],
   "source": [
    "event_by_event_group = Event_by_Event_Group(\n",
    "    num_evaluation_sets_per_label=50,\n",
    "    num_evaluation_sets_per_label_sensitivity=2_000,\n",
    "    q_squared_veto=Names_of_q_Squared_Vetos().loose,\n",
    "    std_scale=True,\n",
    "    shuffle=True,\n",
    "    loss_fn=CrossEntropyLoss(),\n",
    "    learning_rate=3e-3,\n",
    "    learning_rate_scheduler_reduction_factor=0.95,\n",
    "    size_of_training_batch=10_000,\n",
    "    size_of_evaluation_batch=10_000,\n",
    "    number_of_epochs=200,\n",
    "    number_of_epochs_between_checkpoints=5,\n",
    "    results_table=results_table,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "event_by_event_group.train_subset(levels=[Names_of_Levels.detector,], remake_datasets=True)\n",
    "\n",
    "# event_by_event_group.train_all(remake_datasets=True)\n",
    "# event_by_event_group.evaluate_all(remake_datasets=False)\n",
    "\n",
    "event_by_event_group.evaluate_subset(\n",
    "    levels=[Names_of_Levels().detector,], \n",
    "    nums_events_per_set=[24_000,], \n",
    "    remake_datasets=True,\n",
    "    epoch=\"final\"\n",
    ")\n",
    "\n",
    "# ebe = event_by_event_group.get_individual(level=Names_of_Levels().detector)\n",
    "# evaluate_model(\n",
    "#     model=ebe.model,\n",
    "#     evaluation_dataset=Binned_Sets_Dataset(\n",
    "#         settings=Binned_Sets_Dataset_Settings(\n",
    "#             level=Names_of_Levels().detector,\n",
    "#             split=Names_of_Splits().train,\n",
    "#             num_events_per_set=24_000,\n",
    "#             num_sets_per_label=50,\n",
    "#             is_sensitivity_study=False,\n",
    "#             q_squared_veto=Names_of_q_Squared_Vetos().loose,\n",
    "#             std_scale=True,\n",
    "#             shuffle=True,\n",
    "#             path_to_main_datasets_dir=Paths_to_Directories().path_to_main_datasets_dir,\n",
    "#             path_to_raw_signal_dir=Paths_to_Directories().path_to_raw_signal_dir,\n",
    "#             path_to_raw_bkg_dir=Paths_to_Directories().path_to_raw_bkg_dir\n",
    "#         ), \n",
    "#         remake=False\n",
    "#     ),\n",
    "#     sensitivity_evaluation_dataset=Binned_Sets_Dataset(\n",
    "#         ebe._get_sensitivity_evaluation_set_dataset_settings(num_events_per_set=24_000), \n",
    "#         remake=False\n",
    "#     ),\n",
    "#     results_table=results_table,\n",
    "#     device=device,\n",
    "#     epoch=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "pandas.read_pickle(\"../../state/new_physics/data/raw/bkg/mu_sideb_generic_charge_eval.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
