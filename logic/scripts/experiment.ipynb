{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import MSELoss, CrossEntropyLoss\n",
    "\n",
    "from helpers.datasets.make_and_save.aggregated_signal import Aggregated_Signal_Dataframe_Handler\n",
    "from helpers.datasets.constants import Names_of_Levels, Names_of_q_Squared_Vetos, Raw_Signal_Trial_Ranges, Numbers_of_Events_per_Set, Names_of_Splits\n",
    "from helpers.experiment.experiment import CNN_Group, Deep_Sets_Group, Event_by_Event_Group\n",
    "from helpers.experiment.results_table import Results_Table\n",
    "from helpers.experiment.constants import Paths_to_Directories\n",
    "from helpers.models.hardware_util import select_device\n",
    "from helpers.experiment.experiment import evaluate_model\n",
    "from helpers.datasets.settings.settings import Binned_Sets_Dataset_Settings\n",
    "from helpers.datasets.datasets import Unbinned_Sets_Dataset, Binned_Sets_Dataset\n",
    "\n",
    "results_table = Results_Table()\n",
    "device = select_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  level in (Names_of_Levels().generator, Names_of_Levels().detector):\n",
    "    for trial_range in Raw_Signal_Trial_Ranges().tuple_:\n",
    "        \n",
    "        Aggregated_Signal_Dataframe_Handler(\n",
    "            path_to_main_datasets_dir=Paths_to_Directories().path_to_main_datasets_dir,\n",
    "            level=level,\n",
    "            trial_range=trial_range\n",
    "        ).make_and_save(Paths_to_Directories().path_to_raw_signal_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tensor of shape: torch.Size([2200, 24000, 4]) from ..\\..\\state\\new_physics\\data\\processed\\sets_unbinned_det_bkg_q2v_loose\\24000_train_features.pt\n",
      "Loaded tensor of shape: torch.Size([2200]) from ..\\..\\state\\new_physics\\data\\processed\\sets_unbinned_det_bkg_q2v_loose\\24000_train_labels.pt\n",
      "Loaded tensor of shape: torch.Size([2200, 24000, 4]) from ..\\..\\state\\new_physics\\data\\processed\\sets_unbinned_det_bkg_q2v_loose\\24000_eval_features.pt\n",
      "Loaded tensor of shape: torch.Size([2200]) from ..\\..\\state\\new_physics\\data\\processed\\sets_unbinned_det_bkg_q2v_loose\\24000_eval_labels.pt\n",
      "\n",
      "Epoch 0 complete:\n",
      "    Train loss: 0.9777434163168557\n",
      "    Eval loss: 0.8838631075309026\n",
      "\n",
      "Learning rate: [0.0003]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 1 complete:\n",
      "    Train loss: 0.8324634086811116\n",
      "    Eval loss: 0.8211960046856297\n",
      "\n",
      "Learning rate: [0.0003]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 2 complete:\n",
      "    Train loss: 0.8263077862864292\n",
      "    Eval loss: 0.8203532902108084\n",
      "\n",
      "Learning rate: [0.0003]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 3 complete:\n",
      "    Train loss: 0.821666666588748\n",
      "    Eval loss: 0.8243137556379719\n",
      "\n",
      "Learning rate: [0.00029699999999999996]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 4 complete:\n",
      "    Train loss: 0.8227848956841017\n",
      "    Eval loss: 0.8172386295620323\n",
      "\n",
      "Learning rate: [0.00029699999999999996]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 5 complete:\n",
      "    Train loss: 0.8221728336608851\n",
      "    Eval loss: 0.8235755880847146\n",
      "\n",
      "Learning rate: [0.00029403]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 6 complete:\n",
      "    Train loss: 0.8208223430925082\n",
      "    Eval loss: 0.8222083235164896\n",
      "\n",
      "Learning rate: [0.0002910897]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 7 complete:\n",
      "    Train loss: 0.8203705124928532\n",
      "    Eval loss: 0.8194613333802081\n",
      "\n",
      "Learning rate: [0.000288178803]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 8 complete:\n",
      "    Train loss: 0.8208387665999524\n",
      "    Eval loss: 0.8183287329243775\n",
      "\n",
      "Learning rate: [0.00028529701496999996]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 9 complete:\n",
      "    Train loss: 0.818974688627616\n",
      "    Eval loss: 0.8153628209369237\n",
      "\n",
      "Learning rate: [0.00028529701496999996]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 10 complete:\n",
      "    Train loss: 0.8213046460785645\n",
      "    Eval loss: 0.8148079264013086\n",
      "\n",
      "Learning rate: [0.00028529701496999996]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 11 complete:\n",
      "    Train loss: 0.8161176593323755\n",
      "    Eval loss: 0.8143407968310706\n",
      "\n",
      "Learning rate: [0.00028529701496999996]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 12 complete:\n",
      "    Train loss: 0.8143248025440061\n",
      "    Eval loss: 0.812680373858171\n",
      "\n",
      "Learning rate: [0.00028529701496999996]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 13 complete:\n",
      "    Train loss: 0.8174263140076372\n",
      "    Eval loss: 0.8102037769284486\n",
      "\n",
      "Learning rate: [0.00028529701496999996]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 14 complete:\n",
      "    Train loss: 0.812006179280813\n",
      "    Eval loss: 0.8102043361221001\n",
      "\n",
      "Learning rate: [0.00028244404482029995]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 15 complete:\n",
      "    Train loss: 0.808728437929145\n",
      "    Eval loss: 0.804033263914823\n",
      "\n",
      "Learning rate: [0.00028244404482029995]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 16 complete:\n",
      "    Train loss: 0.8024423002834612\n",
      "    Eval loss: 0.8051304138180881\n",
      "\n",
      "Learning rate: [0.00027961960437209696]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 17 complete:\n",
      "    Train loss: 0.8026466703354702\n",
      "    Eval loss: 0.8000975420436923\n",
      "\n",
      "Learning rate: [0.00027961960437209696]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 18 complete:\n",
      "    Train loss: 0.7927791489853075\n",
      "    Eval loss: 0.7955546805032476\n",
      "\n",
      "Learning rate: [0.00027961960437209696]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 19 complete:\n",
      "    Train loss: 0.7861307345975793\n",
      "    Eval loss: 0.7830291633643862\n",
      "\n",
      "Learning rate: [0.00027961960437209696]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 20 complete:\n",
      "    Train loss: 0.7708324828921531\n",
      "    Eval loss: 0.7576027330448295\n",
      "\n",
      "Learning rate: [0.00027961960437209696]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 21 complete:\n",
      "    Train loss: 0.7311968357181866\n",
      "    Eval loss: 0.6989374234152682\n",
      "\n",
      "Learning rate: [0.00027961960437209696]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 22 complete:\n",
      "    Train loss: 0.6295241311652598\n",
      "    Eval loss: 0.5415184809708325\n",
      "\n",
      "Learning rate: [0.00027961960437209696]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 23 complete:\n",
      "    Train loss: 0.4122347864553581\n",
      "    Eval loss: 0.25314152564456077\n",
      "\n",
      "Learning rate: [0.00027961960437209696]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 24 complete:\n",
      "    Train loss: 0.17081282652919347\n",
      "    Eval loss: 0.5129557722659699\n",
      "\n",
      "Learning rate: [0.000276823408328376]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 25 complete:\n",
      "    Train loss: 0.11360481433319028\n",
      "    Eval loss: 0.27548046203012233\n",
      "\n",
      "Learning rate: [0.0002740551742450922]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 26 complete:\n",
      "    Train loss: 0.1045238658499864\n",
      "    Eval loss: 0.19767119608255185\n",
      "\n",
      "Learning rate: [0.0002740551742450922]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 27 complete:\n",
      "    Train loss: 0.09429655014964387\n",
      "    Eval loss: 0.37046171989528387\n",
      "\n",
      "Learning rate: [0.00027131462250264127]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 28 complete:\n",
      "    Train loss: 0.08701016372351751\n",
      "    Eval loss: 0.24507919775237783\n",
      "\n",
      "Learning rate: [0.0002686014762776149]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 29 complete:\n",
      "    Train loss: 0.08718444883215873\n",
      "    Eval loss: 0.2902622493575829\n",
      "\n",
      "Learning rate: [0.00026591546151483875]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 30 complete:\n",
      "    Train loss: 0.08272453766003049\n",
      "    Eval loss: 0.3330583926191973\n",
      "\n",
      "Learning rate: [0.00026325630689969036]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 31 complete:\n",
      "    Train loss: 0.08960460670890599\n",
      "    Eval loss: 0.3426710446641549\n",
      "\n",
      "Learning rate: [0.00026062374383069347]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 32 complete:\n",
      "    Train loss: 0.08379884573247275\n",
      "    Eval loss: 0.3020804278659271\n",
      "\n",
      "Learning rate: [0.00025801750639238655]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 33 complete:\n",
      "    Train loss: 0.08549757598221228\n",
      "    Eval loss: 0.3771740690696134\n",
      "\n",
      "Learning rate: [0.0002554373313284627]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 34 complete:\n",
      "    Train loss: 0.08037201052337711\n",
      "    Eval loss: 0.2628044817004342\n",
      "\n",
      "Learning rate: [0.00025288295801517805]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 35 complete:\n",
      "    Train loss: 0.08424455418884227\n",
      "    Eval loss: 0.28169798583165717\n",
      "\n",
      "Learning rate: [0.0002503541284350263]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 36 complete:\n",
      "    Train loss: 0.08708039983643634\n",
      "    Eval loss: 0.13958187753054696\n",
      "\n",
      "Learning rate: [0.0002503541284350263]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 37 complete:\n",
      "    Train loss: 0.08329540347783587\n",
      "    Eval loss: 0.25858735839262015\n",
      "\n",
      "Learning rate: [0.00024785058715067604]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 38 complete:\n",
      "    Train loss: 0.08784638334523905\n",
      "    Eval loss: 0.15088771741063367\n",
      "\n",
      "Learning rate: [0.00024537208127916926]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 39 complete:\n",
      "    Train loss: 0.0821807608427587\n",
      "    Eval loss: 0.25782035573862877\n",
      "\n",
      "Learning rate: [0.00024291836046637757]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 40 complete:\n",
      "    Train loss: 0.07829239802455609\n",
      "    Eval loss: 0.24806739471035358\n",
      "\n",
      "Learning rate: [0.00024048917686171379]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 41 complete:\n",
      "    Train loss: 0.08580707434583883\n",
      "    Eval loss: 0.15499036546868364\n",
      "\n",
      "Learning rate: [0.00023808428509309665]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 42 complete:\n",
      "    Train loss: 0.08545564433998631\n",
      "    Eval loss: 0.2031233231172676\n",
      "\n",
      "Learning rate: [0.0002357034422421657]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 43 complete:\n",
      "    Train loss: 0.07820649033865835\n",
      "    Eval loss: 0.2708569149458725\n",
      "\n",
      "Learning rate: [0.00023334640781974404]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 44 complete:\n",
      "    Train loss: 0.08046721494345031\n",
      "    Eval loss: 0.1984994660149966\n",
      "\n",
      "Learning rate: [0.00023101294374154658]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 45 complete:\n",
      "    Train loss: 0.07846138163688142\n",
      "    Eval loss: 0.15741336089061483\n",
      "\n",
      "Learning rate: [0.0002287028143041311]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 46 complete:\n",
      "    Train loss: 0.07894252838467579\n",
      "    Eval loss: 0.2983331237264583\n",
      "\n",
      "Learning rate: [0.00022641578616108978]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 47 complete:\n",
      "    Train loss: 0.07964686537728162\n",
      "    Eval loss: 0.30037596290657764\n",
      "\n",
      "Learning rate: [0.00022415162829947887]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 48 complete:\n",
      "    Train loss: 0.07793890306834236\n",
      "    Eval loss: 0.29736580068699503\n",
      "\n",
      "Learning rate: [0.00022191011201648408]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 49 complete:\n",
      "    Train loss: 0.07860774798447451\n",
      "    Eval loss: 0.19519631718834365\n",
      "\n",
      "Learning rate: [0.00021969101089631924]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 50 complete:\n",
      "    Train loss: 0.07730666342260892\n",
      "    Eval loss: 0.33858854861668486\n",
      "\n",
      "Learning rate: [0.00021749410078735604]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 51 complete:\n",
      "    Train loss: 0.07886306520772329\n",
      "    Eval loss: 0.3073565789321656\n",
      "\n",
      "Learning rate: [0.00021531915977948248]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 52 complete:\n",
      "    Train loss: 0.07621116910048037\n",
      "    Eval loss: 0.31183359285873014\n",
      "\n",
      "Learning rate: [0.00021316596818168765]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 53 complete:\n",
      "    Train loss: 0.07917624902130627\n",
      "    Eval loss: 0.28169375408663017\n",
      "\n",
      "Learning rate: [0.00021103430849987078]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 54 complete:\n",
      "    Train loss: 0.0782853089481741\n",
      "    Eval loss: 0.2001685847661888\n",
      "\n",
      "Learning rate: [0.00020892396541487206]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 55 complete:\n",
      "    Train loss: 0.07976264179592953\n",
      "    Eval loss: 0.3162396596760995\n",
      "\n",
      "Learning rate: [0.00020683472576072333]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 56 complete:\n",
      "    Train loss: 0.07778272197406277\n",
      "    Eval loss: 0.27665837278789446\n",
      "\n",
      "Learning rate: [0.0002047663785031161]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 57 complete:\n",
      "    Train loss: 0.07715846058276328\n",
      "    Eval loss: 0.3123374181669141\n",
      "\n",
      "Learning rate: [0.00020271871471808493]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 58 complete:\n",
      "    Train loss: 0.07698496828802305\n",
      "    Eval loss: 0.22220867680182965\n",
      "\n",
      "Learning rate: [0.00020069152757090408]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 59 complete:\n",
      "    Train loss: 0.07685351672294036\n",
      "    Eval loss: 0.24366879718326578\n",
      "\n",
      "Learning rate: [0.00019868461229519504]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 60 complete:\n",
      "    Train loss: 0.08098148647741772\n",
      "    Eval loss: 0.2787319743951741\n",
      "\n",
      "Learning rate: [0.0001966977661722431]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 61 complete:\n",
      "    Train loss: 0.07585071380778151\n",
      "    Eval loss: 0.30212607434318994\n",
      "\n",
      "Learning rate: [0.00019473078851052066]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 62 complete:\n",
      "    Train loss: 0.07583324333100788\n",
      "    Eval loss: 0.2606491925275758\n",
      "\n",
      "Learning rate: [0.00019278348062541544]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 63 complete:\n",
      "    Train loss: 0.0745517755812569\n",
      "    Eval loss: 0.37231547788713487\n",
      "\n",
      "Learning rate: [0.00019085564581916128]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 64 complete:\n",
      "    Train loss: 0.07899090418437653\n",
      "    Eval loss: 0.3012888344886476\n",
      "\n",
      "Learning rate: [0.00018894708936096965]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 65 complete:\n",
      "    Train loss: 0.08028732345999137\n",
      "    Eval loss: 0.24545676875046366\n",
      "\n",
      "Learning rate: [0.00018705761846735995]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 66 complete:\n",
      "    Train loss: 0.08012552963873504\n",
      "    Eval loss: 0.34254029389293256\n",
      "\n",
      "Learning rate: [0.00018518704228268635]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 67 complete:\n",
      "    Train loss: 0.07653035123200232\n",
      "    Eval loss: 0.3740303612327269\n",
      "\n",
      "Learning rate: [0.00018333517185985948]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 68 complete:\n",
      "    Train loss: 0.07656751334903301\n",
      "    Eval loss: 0.26629103225912826\n",
      "\n",
      "Learning rate: [0.00018150182014126088]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 69 complete:\n",
      "    Train loss: 0.0757376845934565\n",
      "    Eval loss: 0.26125426896613674\n",
      "\n",
      "Learning rate: [0.00017968680193984827]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 70 complete:\n",
      "    Train loss: 0.07555709192367484\n",
      "    Eval loss: 0.18329207487249838\n",
      "\n",
      "Learning rate: [0.0001778899339204498]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 71 complete:\n",
      "    Train loss: 0.08548913528759848\n",
      "    Eval loss: 0.26884139908102905\n",
      "\n",
      "Learning rate: [0.0001761110345812453]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 72 complete:\n",
      "    Train loss: 0.07465058791481632\n",
      "    Eval loss: 0.2130419215875122\n",
      "\n",
      "Learning rate: [0.00017434992423543284]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 73 complete:\n",
      "    Train loss: 0.07827796263588359\n",
      "    Eval loss: 0.4831295910414886\n",
      "\n",
      "Learning rate: [0.0001726064249930785]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 74 complete:\n",
      "    Train loss: 0.07564732316877522\n",
      "    Eval loss: 0.24657394167600824\n",
      "\n",
      "Learning rate: [0.00017088036074314772]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 75 complete:\n",
      "    Train loss: 0.07711347140234026\n",
      "    Eval loss: 0.3413690178387876\n",
      "\n",
      "Learning rate: [0.00016917155713571625]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 76 complete:\n",
      "    Train loss: 0.07601073799482903\n",
      "    Eval loss: 0.3205962418438511\n",
      "\n",
      "Learning rate: [0.00016747984156435908]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 77 complete:\n",
      "    Train loss: 0.07647287334710236\n",
      "    Eval loss: 0.31384545162044425\n",
      "\n",
      "Learning rate: [0.0001658050431487155]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 78 complete:\n",
      "    Train loss: 0.07351384119079657\n",
      "    Eval loss: 0.2672018416562005\n",
      "\n",
      "Learning rate: [0.00016414699271722832]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 79 complete:\n",
      "    Train loss: 0.07437638380402252\n",
      "    Eval loss: 0.3188728828701539\n",
      "\n",
      "Learning rate: [0.00016250552279005603]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 80 complete:\n",
      "    Train loss: 0.07547521525983997\n",
      "    Eval loss: 0.3206564154044426\n",
      "\n",
      "Learning rate: [0.00016088046756215546]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 81 complete:\n",
      "    Train loss: 0.07826700760804359\n",
      "    Eval loss: 0.40553029133719926\n",
      "\n",
      "Learning rate: [0.0001592716628865339]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 82 complete:\n",
      "    Train loss: 0.07456241928543118\n",
      "    Eval loss: 0.3286271214367545\n",
      "\n",
      "Learning rate: [0.00015767894625766857]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 83 complete:\n",
      "    Train loss: 0.07366297582548628\n",
      "    Eval loss: 0.23351119892902208\n",
      "\n",
      "Learning rate: [0.0001561021567950919]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 84 complete:\n",
      "    Train loss: 0.07454021859832403\n",
      "    Eval loss: 0.24425399610377355\n",
      "\n",
      "Learning rate: [0.00015454113522714096]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 85 complete:\n",
      "    Train loss: 0.07606917865895227\n",
      "    Eval loss: 0.3663256543919575\n",
      "\n",
      "Learning rate: [0.00015299572387486956]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 86 complete:\n",
      "    Train loss: 0.07716184512446922\n",
      "    Eval loss: 0.2993459032357614\n",
      "\n",
      "Learning rate: [0.00015146576663612087]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 87 complete:\n",
      "    Train loss: 0.0745589233575137\n",
      "    Eval loss: 0.22733215794357667\n",
      "\n",
      "Learning rate: [0.00014995110896975965]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 88 complete:\n",
      "    Train loss: 0.07438438122335614\n",
      "    Eval loss: 0.2649250008517021\n",
      "\n",
      "Learning rate: [0.00014845159788006205]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 89 complete:\n",
      "    Train loss: 0.07411900436032894\n",
      "    Eval loss: 0.22839589440082422\n",
      "\n",
      "Learning rate: [0.00014696708190126143]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 90 complete:\n",
      "    Train loss: 0.07691969614822448\n",
      "    Eval loss: 0.2835739980938902\n",
      "\n",
      "Learning rate: [0.0001454974110822488]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 91 complete:\n",
      "    Train loss: 0.07777601337249568\n",
      "    Eval loss: 0.34214650129549573\n",
      "\n",
      "Learning rate: [0.00014404243697142632]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 92 complete:\n",
      "    Train loss: 0.0750133264997828\n",
      "    Eval loss: 0.2490438862751258\n",
      "\n",
      "Learning rate: [0.00014260201260171205]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 93 complete:\n",
      "    Train loss: 0.07400335408352224\n",
      "    Eval loss: 0.269153034336221\n",
      "\n",
      "Learning rate: [0.00014117599247569492]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 94 complete:\n",
      "    Train loss: 0.07315859066543362\n",
      "    Eval loss: 0.26779499818320834\n",
      "\n",
      "Learning rate: [0.00013976423255093796]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 95 complete:\n",
      "    Train loss: 0.07468287907599337\n",
      "    Eval loss: 0.3573497892358228\n",
      "\n",
      "Learning rate: [0.00013836659022542857]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 96 complete:\n",
      "    Train loss: 0.07271798375358536\n",
      "    Eval loss: 0.26908828278565816\n",
      "\n",
      "Learning rate: [0.0001369829243231743]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 97 complete:\n",
      "    Train loss: 0.07278095994521074\n",
      "    Eval loss: 0.32666907759619473\n",
      "\n",
      "Learning rate: [0.00013561309507994257]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 98 complete:\n",
      "    Train loss: 0.07539191392853495\n",
      "    Eval loss: 0.3459632238911524\n",
      "\n",
      "Learning rate: [0.00013425696412914314]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 99 complete:\n",
      "    Train loss: 0.08163004318610625\n",
      "    Eval loss: 0.263185095130665\n",
      "\n",
      "Learning rate: [0.0001329143944878517]\n",
      "Peak GPU memory usage:\n",
      "0.78508 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "Completed training.\n",
      "Unloaded datasets.\n",
      "Unloaded datasets.\n",
      "Loaded tensor of shape: torch.Size([2200, 24000, 4]) from ..\\..\\state\\new_physics\\data\\processed\\sets_unbinned_det_bkg_q2v_loose\\24000_eval_features.pt\n",
      "Loaded tensor of shape: torch.Size([2200]) from ..\\..\\state\\new_physics\\data\\processed\\sets_unbinned_det_bkg_q2v_loose\\24000_eval_labels.pt\n",
      "Loaded tensor of shape: torch.Size([2000, 24000, 4]) from ..\\..\\state\\new_physics\\data\\processed\\sets_unbinned_det_bkg_q2v_loose\\24000_eval_sens_features.pt\n",
      "Loaded tensor of shape: torch.Size([2000]) from ..\\..\\state\\new_physics\\data\\processed\\sets_unbinned_det_bkg_q2v_loose\\24000_eval_sens_labels.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tetha\\Desktop\\btokstll\\logic\\scripts\\helpers\\experiment\\results_table.py:49: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  self.table.loc[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unloaded datasets.\n",
      "Unloaded datasets.\n"
     ]
    }
   ],
   "source": [
    "# from helpers.experiment.experiment import evaluate_model\n",
    "# from helpers.datasets.datasets import Unbinned_Sets_Dataset\n",
    "\n",
    "deep_sets_group = Deep_Sets_Group(\n",
    "    num_sets_per_label=50,\n",
    "    num_sets_per_label_sensitivity=2_000,\n",
    "    q_squared_veto=Names_of_q_Squared_Vetos().loose,\n",
    "    std_scale=True,\n",
    "    shuffle=True,\n",
    "    loss_fn=MSELoss(),\n",
    "    learning_rate=3e-4, # 3e-4\n",
    "    learning_rate_scheduler_reduction_factor=0.99, # 0.9\n",
    "    size_of_training_batch=32,\n",
    "    size_of_evaluation_batch=32,\n",
    "    number_of_epochs=100, # 100\n",
    "    number_of_epochs_between_checkpoints=1,\n",
    "    results_table=results_table,\n",
    "    device=device,\n",
    "    bkg_fraction=0.5,\n",
    "    bkg_charge_fraction=0.5\n",
    ")\n",
    "\n",
    "deep_sets_group.train_subset(\n",
    "    levels=(Names_of_Levels().detector_and_background,),\n",
    "    nums_events_per_set=(24_000,),\n",
    "    remake_datasets=False\n",
    ")\n",
    "deep_sets_group.evaluate_subset(\n",
    "    levels=(Names_of_Levels().detector_and_background,), \n",
    "    nums_events_per_set=(24_000,), \n",
    "    remake_datasets=False,\n",
    "    epoch=\"final\"\n",
    ")\n",
    "\n",
    "\n",
    "# evaluate_model(\n",
    "#     model=deep_sets.model,\n",
    "#     evaluation_dataset=Unbinned_Sets_Dataset(deep_sets.training_dataset_settings, remake=True),\n",
    "#     sensitivity_evaluation_dataset=Unbinned_Sets_Dataset(deep_sets.sensitivity_evaluation_dataset_settings, remake=False),\n",
    "#     results_table=results_table,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "\n",
    "# deep_sets_group.train_all(remake_datasets=False)\n",
    "# deep_sets_group.evaluate_all(remake_datasets=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_group = CNN_Group(\n",
    "    num_sets_per_label=50,\n",
    "    num_sets_per_label_sensitivity=2_000,\n",
    "    num_bins_per_dimension=10,\n",
    "    q_squared_veto=Names_of_q_Squared_Vetos().loose,\n",
    "    std_scale=True,\n",
    "    shuffle=True,\n",
    "    loss_fn=MSELoss(),\n",
    "    learning_rate=3e-4,\n",
    "    learning_rate_scheduler_reduction_factor=0.9,\n",
    "    size_of_training_batch=32,\n",
    "    size_of_evaluation_batch=32,\n",
    "    number_of_epochs=100,\n",
    "    number_of_epochs_between_checkpoints=5,\n",
    "    results_table=results_table,\n",
    "    device=device,\n",
    "    bkg_fraction=0.5,\n",
    "    bkg_charge_fraction=0.5\n",
    ")\n",
    "\n",
    "# cnn_group.train_all(remake_datasets=True)\n",
    "# cnn_group.evaluate_all(remake_datasets=True)\n",
    "cnn_group.plot_image_examples_all(remake_datasets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making binned events dataset.\n",
      "Number of NA values: \n",
      " q_squared        0\n",
      "costheta_mu      0\n",
      "costheta_K       0\n",
      "chi              0\n",
      "dc9              0\n",
      "dc9_bin_index    0\n",
      "dtype: int64\n",
      "Removed rows that have a NaN.\n",
      "Applying standand scale.\n",
      "Applied standard scale.\n",
      "Shuffled dataframe.\n",
      "Saved tensor of shape: torch.Size([21021610, 4]) to ..\\..\\state\\new_physics\\data\\processed\\events_binned_gen_q2v_loose\\train_features.pt\n",
      "Saved tensor of shape: torch.Size([21021610]) to ..\\..\\state\\new_physics\\data\\processed\\events_binned_gen_q2v_loose\\train_labels.pt\n",
      "Saved tensor of shape: torch.Size([44]) to ..\\..\\state\\new_physics\\data\\processed\\events_binned_gen_q2v_loose\\train_bin_map.pt\n",
      "Made binned events dataset.\n",
      "Loaded tensor of shape: torch.Size([21021610, 4]) from ..\\..\\state\\new_physics\\data\\processed\\events_binned_gen_q2v_loose\\train_features.pt\n",
      "Loaded tensor of shape: torch.Size([21021610]) from ..\\..\\state\\new_physics\\data\\processed\\events_binned_gen_q2v_loose\\train_labels.pt\n",
      "Loaded tensor of shape: torch.Size([44]) from ..\\..\\state\\new_physics\\data\\processed\\events_binned_gen_q2v_loose\\train_bin_map.pt\n",
      "Making binned events dataset.\n",
      "Number of NA values: \n",
      " q_squared        0\n",
      "costheta_mu      0\n",
      "costheta_K       0\n",
      "chi              0\n",
      "dc9              0\n",
      "dc9_bin_index    0\n",
      "dtype: int64\n",
      "Removed rows that have a NaN.\n",
      "Applying standand scale.\n",
      "Applied standard scale.\n",
      "Shuffled dataframe.\n",
      "Saved tensor of shape: torch.Size([21093663, 4]) to ..\\..\\state\\new_physics\\data\\processed\\events_binned_gen_q2v_loose\\eval_features.pt\n",
      "Saved tensor of shape: torch.Size([21093663]) to ..\\..\\state\\new_physics\\data\\processed\\events_binned_gen_q2v_loose\\eval_labels.pt\n",
      "Saved tensor of shape: torch.Size([44]) to ..\\..\\state\\new_physics\\data\\processed\\events_binned_gen_q2v_loose\\eval_bin_map.pt\n",
      "Made binned events dataset.\n",
      "Loaded tensor of shape: torch.Size([21093663, 4]) from ..\\..\\state\\new_physics\\data\\processed\\events_binned_gen_q2v_loose\\eval_features.pt\n",
      "Loaded tensor of shape: torch.Size([21093663]) from ..\\..\\state\\new_physics\\data\\processed\\events_binned_gen_q2v_loose\\eval_labels.pt\n",
      "Loaded tensor of shape: torch.Size([44]) from ..\\..\\state\\new_physics\\data\\processed\\events_binned_gen_q2v_loose\\eval_bin_map.pt\n",
      "\n",
      "Epoch 0 complete:\n",
      "    Train loss: 3.7806539458679724\n",
      "    Eval loss: 3.780393855469071\n",
      "\n",
      "Learning rate: [0.003]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 1 complete:\n",
      "    Train loss: 3.7801259778041842\n",
      "    Eval loss: 3.7802637723297687\n",
      "\n",
      "Learning rate: [0.003]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 2 complete:\n",
      "    Train loss: 3.7800149797574063\n",
      "    Eval loss: 3.7802816785593523\n",
      "\n",
      "Learning rate: [0.00291]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 3 complete:\n",
      "    Train loss: 3.779942949610002\n",
      "    Eval loss: 3.780138744390611\n",
      "\n",
      "Learning rate: [0.00291]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 4 complete:\n",
      "    Train loss: 3.779912677902499\n",
      "    Eval loss: 3.780208689572864\n",
      "\n",
      "Learning rate: [0.0028226999999999996]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 5 complete:\n",
      "    Train loss: 3.7798964044050773\n",
      "    Eval loss: 3.7801388125344677\n",
      "\n",
      "Learning rate: [0.0027380189999999995]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 6 complete:\n",
      "    Train loss: 3.779881409738539\n",
      "    Eval loss: 3.7801243865430845\n",
      "\n",
      "Learning rate: [0.0027380189999999995]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 7 complete:\n",
      "    Train loss: 3.779872961384546\n",
      "    Eval loss: 3.780198888718778\n",
      "\n",
      "Learning rate: [0.0026558784299999996]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 8 complete:\n",
      "    Train loss: 3.7798636511120827\n",
      "    Eval loss: 3.7801764185556848\n",
      "\n",
      "Learning rate: [0.0025762020770999997]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 9 complete:\n",
      "    Train loss: 3.7798481154927903\n",
      "    Eval loss: 3.7800851037378522\n",
      "\n",
      "Learning rate: [0.0025762020770999997]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 10 complete:\n",
      "    Train loss: 3.7798456204523774\n",
      "    Eval loss: 3.78011798109953\n",
      "\n",
      "Learning rate: [0.0024989160147869996]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 11 complete:\n",
      "    Train loss: 3.7798300668514853\n",
      "    Eval loss: 3.7801442622652535\n",
      "\n",
      "Learning rate: [0.0024239485343433894]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 12 complete:\n",
      "    Train loss: 3.779822530442101\n",
      "    Eval loss: 3.780093412630186\n",
      "\n",
      "Learning rate: [0.0023512300783130875]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 13 complete:\n",
      "    Train loss: 3.7798133816406696\n",
      "    Eval loss: 3.7801145591419303\n",
      "\n",
      "Learning rate: [0.002280693175963695]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 14 complete:\n",
      "    Train loss: 3.779809724985093\n",
      "    Eval loss: 3.7800395212830082\n",
      "\n",
      "Learning rate: [0.002280693175963695]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 15 complete:\n",
      "    Train loss: 3.7798034795054347\n",
      "    Eval loss: 3.7800412421704435\n",
      "\n",
      "Learning rate: [0.002212272380684784]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 16 complete:\n",
      "    Train loss: 3.7797962448264655\n",
      "    Eval loss: 3.7799991592248796\n",
      "\n",
      "Learning rate: [0.002212272380684784]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 17 complete:\n",
      "    Train loss: 3.779791201337977\n",
      "    Eval loss: 3.780029051734858\n",
      "\n",
      "Learning rate: [0.0021459042092642406]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 18 complete:\n",
      "    Train loss: 3.779786464221582\n",
      "    Eval loss: 3.780091053548267\n",
      "\n",
      "Learning rate: [0.0020815270829863133]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 19 complete:\n",
      "    Train loss: 3.7797886053056122\n",
      "    Eval loss: 3.7800442693278766\n",
      "\n",
      "Learning rate: [0.002019081270496724]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 20 complete:\n",
      "    Train loss: 3.7797739634050296\n",
      "    Eval loss: 3.7801016021351477\n",
      "\n",
      "Learning rate: [0.001958508832381822]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 21 complete:\n",
      "    Train loss: 3.7797689384431967\n",
      "    Eval loss: 3.7800420649108974\n",
      "\n",
      "Learning rate: [0.0018997535674103675]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 22 complete:\n",
      "    Train loss: 3.77976270432704\n",
      "    Eval loss: 3.7800089656967404\n",
      "\n",
      "Learning rate: [0.0018427609603880563]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 23 complete:\n",
      "    Train loss: 3.7797631779794374\n",
      "    Eval loss: 3.7800387899367918\n",
      "\n",
      "Learning rate: [0.0017874781315764146]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 24 complete:\n",
      "    Train loss: 3.779758390854007\n",
      "    Eval loss: 3.7799994147303093\n",
      "\n",
      "Learning rate: [0.0017338537876291221]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 25 complete:\n",
      "    Train loss: 3.7797518093402127\n",
      "    Eval loss: 3.780050457847781\n",
      "\n",
      "Learning rate: [0.0016818381740002484]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 26 complete:\n",
      "    Train loss: 3.779747357674318\n",
      "    Eval loss: 3.78000756644527\n",
      "\n",
      "Learning rate: [0.001631383028780241]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 27 complete:\n",
      "    Train loss: 3.7797412514479642\n",
      "    Eval loss: 3.7800521788969204\n",
      "\n",
      "Learning rate: [0.0015824415379168337]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 28 complete:\n",
      "    Train loss: 3.779737855075197\n",
      "    Eval loss: 3.780048731030694\n",
      "\n",
      "Learning rate: [0.0015349682917793286]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 29 complete:\n",
      "    Train loss: 3.779735211439815\n",
      "    Eval loss: 3.7800231730953002\n",
      "\n",
      "Learning rate: [0.0014889192430259488]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 30 complete:\n",
      "    Train loss: 3.7797327015273003\n",
      "    Eval loss: 3.780026639975848\n",
      "\n",
      "Learning rate: [0.0014442516657351702]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 31 complete:\n",
      "    Train loss: 3.7797292481554945\n",
      "    Eval loss: 3.780025729396701\n",
      "\n",
      "Learning rate: [0.001400924115763115]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 32 complete:\n",
      "    Train loss: 3.779721499050565\n",
      "    Eval loss: 3.7799914810908177\n",
      "\n",
      "Learning rate: [0.001400924115763115]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 33 complete:\n",
      "    Train loss: 3.7797216685815225\n",
      "    Eval loss: 3.780019526022064\n",
      "\n",
      "Learning rate: [0.0013588963922902216]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 34 complete:\n",
      "    Train loss: 3.7797225775919614\n",
      "    Eval loss: 3.780006113267646\n",
      "\n",
      "Learning rate: [0.001318129500521515]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 35 complete:\n",
      "    Train loss: 3.779718315744767\n",
      "    Eval loss: 3.779979107268636\n",
      "\n",
      "Learning rate: [0.001318129500521515]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 36 complete:\n",
      "    Train loss: 3.779715330995961\n",
      "    Eval loss: 3.780050764248669\n",
      "\n",
      "Learning rate: [0.0012785856155058694]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 37 complete:\n",
      "    Train loss: 3.779712042256332\n",
      "    Eval loss: 3.7799861122315144\n",
      "\n",
      "Learning rate: [0.0012402280470406933]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 38 complete:\n",
      "    Train loss: 3.7797091606201376\n",
      "    Eval loss: 3.7800078758328177\n",
      "\n",
      "Learning rate: [0.0012030212056294725]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 39 complete:\n",
      "    Train loss: 3.7797059273383837\n",
      "    Eval loss: 3.779987779192836\n",
      "\n",
      "Learning rate: [0.0011669305694605883]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 40 complete:\n",
      "    Train loss: 3.7797036481338457\n",
      "    Eval loss: 3.7799694338195486\n",
      "\n",
      "Learning rate: [0.0011669305694605883]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 41 complete:\n",
      "    Train loss: 3.779701169015357\n",
      "    Eval loss: 3.7800127189308266\n",
      "\n",
      "Learning rate: [0.0011319226523767707]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 42 complete:\n",
      "    Train loss: 3.77969841221755\n",
      "    Eval loss: 3.7799377116716157\n",
      "\n",
      "Learning rate: [0.0011319226523767707]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 43 complete:\n",
      "    Train loss: 3.7797017932171313\n",
      "    Eval loss: 3.7799879683034123\n",
      "\n",
      "Learning rate: [0.0010979649728054676]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 44 complete:\n",
      "    Train loss: 3.779699931523104\n",
      "    Eval loss: 3.779973850285641\n",
      "\n",
      "Learning rate: [0.0010650260236213034]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 45 complete:\n",
      "    Train loss: 3.7796937642735724\n",
      "    Eval loss: 3.780013796510396\n",
      "\n",
      "Learning rate: [0.0010330752429126642]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 46 complete:\n",
      "    Train loss: 3.7796921147371663\n",
      "    Eval loss: 3.779999725469386\n",
      "\n",
      "Learning rate: [0.0010020829856252844]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 47 complete:\n",
      "    Train loss: 3.779689495038962\n",
      "    Eval loss: 3.77999794421384\n",
      "\n",
      "Learning rate: [0.0009720204960565258]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 48 complete:\n",
      "    Train loss: 3.7796898486024424\n",
      "    Eval loss: 3.7800233889205668\n",
      "\n",
      "Learning rate: [0.00094285988117483]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 49 complete:\n",
      "    Train loss: 3.779686562372424\n",
      "    Eval loss: 3.7799745364571504\n",
      "\n",
      "Learning rate: [0.0009145740847395851]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 50 complete:\n",
      "    Train loss: 3.779679794079695\n",
      "    Eval loss: 3.779962054387913\n",
      "\n",
      "Learning rate: [0.0008871368621973975]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 51 complete:\n",
      "    Train loss: 3.7796782318884254\n",
      "    Eval loss: 3.779975867793192\n",
      "\n",
      "Learning rate: [0.0008605227563314756]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 52 complete:\n",
      "    Train loss: 3.7796777934999737\n",
      "    Eval loss: 3.779959267006252\n",
      "\n",
      "Learning rate: [0.0008347070736415313]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 53 complete:\n",
      "    Train loss: 3.779675330090263\n",
      "    Eval loss: 3.7799811597459274\n",
      "\n",
      "Learning rate: [0.0008096658614322853]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 54 complete:\n",
      "    Train loss: 3.7796715125281533\n",
      "    Eval loss: 3.7799557095299163\n",
      "\n",
      "Learning rate: [0.0007853758855893168]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 55 complete:\n",
      "    Train loss: 3.77967292431905\n",
      "    Eval loss: 3.779969921341615\n",
      "\n",
      "Learning rate: [0.0007618146090216372]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 56 complete:\n",
      "    Train loss: 3.779669035411516\n",
      "    Eval loss: 3.7799718501062083\n",
      "\n",
      "Learning rate: [0.0007389601707509881]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 57 complete:\n",
      "    Train loss: 3.7796689226263016\n",
      "    Eval loss: 3.7799686407344746\n",
      "\n",
      "Learning rate: [0.0007167913656284584]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 58 complete:\n",
      "    Train loss: 3.7796647408136677\n",
      "    Eval loss: 3.779969260616825\n",
      "\n",
      "Learning rate: [0.0006952876246596047]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 59 complete:\n",
      "    Train loss: 3.779662603805939\n",
      "    Eval loss: 3.7799636568349633\n",
      "\n",
      "Learning rate: [0.0006744289959198166]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 60 complete:\n",
      "    Train loss: 3.779659593787817\n",
      "    Eval loss: 3.7799753960896427\n",
      "\n",
      "Learning rate: [0.000654196126042222]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 61 complete:\n",
      "    Train loss: 3.7796603140096767\n",
      "    Eval loss: 3.779967027768528\n",
      "\n",
      "Learning rate: [0.0006345702422609553]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 62 complete:\n",
      "    Train loss: 3.779659822093943\n",
      "    Eval loss: 3.7799631206243003\n",
      "\n",
      "Learning rate: [0.0006155331349931266]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 63 complete:\n",
      "    Train loss: 3.779657396419997\n",
      "    Eval loss: 3.779955368966762\n",
      "\n",
      "Learning rate: [0.0005970671409433328]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 64 complete:\n",
      "    Train loss: 3.779654934675067\n",
      "    Eval loss: 3.779934631694343\n",
      "\n",
      "Learning rate: [0.0005970671409433328]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 65 complete:\n",
      "    Train loss: 3.779654818541106\n",
      "    Eval loss: 3.7799572037735367\n",
      "\n",
      "Learning rate: [0.0005791551267150328]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 66 complete:\n",
      "    Train loss: 3.7796525662855247\n",
      "    Eval loss: 3.779980320290245\n",
      "\n",
      "Learning rate: [0.0005617804729135818]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 67 complete:\n",
      "    Train loss: 3.779650947853317\n",
      "    Eval loss: 3.7799450414395404\n",
      "\n",
      "Learning rate: [0.0005449270587261743]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 68 complete:\n",
      "    Train loss: 3.7796511252697838\n",
      "    Eval loss: 3.7799489017306507\n",
      "\n",
      "Learning rate: [0.000528579246964389]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 69 complete:\n",
      "    Train loss: 3.779648848784978\n",
      "    Eval loss: 3.779928257558418\n",
      "\n",
      "Learning rate: [0.000528579246964389]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 70 complete:\n",
      "    Train loss: 3.7796487944677986\n",
      "    Eval loss: 3.779957864482855\n",
      "\n",
      "Learning rate: [0.0005127218695554573]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 71 complete:\n",
      "    Train loss: 3.779649055031302\n",
      "    Eval loss: 3.779943329457889\n",
      "\n",
      "Learning rate: [0.0004973402134687936]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 72 complete:\n",
      "    Train loss: 3.779644551022702\n",
      "    Eval loss: 3.779949583671669\n",
      "\n",
      "Learning rate: [0.0004824200070647298]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 73 complete:\n",
      "    Train loss: 3.779645191541987\n",
      "    Eval loss: 3.77994959385171\n",
      "\n",
      "Learning rate: [0.00046794740685278787]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 74 complete:\n",
      "    Train loss: 3.7796433540185563\n",
      "    Eval loss: 3.779947267219696\n",
      "\n",
      "Learning rate: [0.0004539089846472042]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 75 complete:\n",
      "    Train loss: 3.779642442550761\n",
      "    Eval loss: 3.779941852393229\n",
      "\n",
      "Learning rate: [0.0004402917151077881]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 76 complete:\n",
      "    Train loss: 3.7796413821850794\n",
      "    Eval loss: 3.7799490846710944\n",
      "\n",
      "Learning rate: [0.00042708296365455444]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 77 complete:\n",
      "    Train loss: 3.7796393661071073\n",
      "    Eval loss: 3.779963011838755\n",
      "\n",
      "Learning rate: [0.0004142704747449178]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 78 complete:\n",
      "    Train loss: 3.779637775854331\n",
      "    Eval loss: 3.7799255670063054\n",
      "\n",
      "Learning rate: [0.0004142704747449178]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 79 complete:\n",
      "    Train loss: 3.779637133655961\n",
      "    Eval loss: 3.7799212184604856\n",
      "\n",
      "Learning rate: [0.0004142704747449178]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 80 complete:\n",
      "    Train loss: 3.779638986091944\n",
      "    Eval loss: 3.779949044222303\n",
      "\n",
      "Learning rate: [0.00040184236050257025]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 81 complete:\n",
      "    Train loss: 3.7796353456861618\n",
      "    Eval loss: 3.779957606423054\n",
      "\n",
      "Learning rate: [0.0003897870896874931]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 82 complete:\n",
      "    Train loss: 3.7796368958437885\n",
      "    Eval loss: 3.779957423060584\n",
      "\n",
      "Learning rate: [0.0003780934769968683]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 83 complete:\n",
      "    Train loss: 3.779635429369336\n",
      "    Eval loss: 3.779945695355286\n",
      "\n",
      "Learning rate: [0.00036675067268696223]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 84 complete:\n",
      "    Train loss: 3.7796350480909555\n",
      "    Eval loss: 3.779932281656317\n",
      "\n",
      "Learning rate: [0.0003557481525063533]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 85 complete:\n",
      "    Train loss: 3.779634903545527\n",
      "    Eval loss: 3.77994939619015\n",
      "\n",
      "Learning rate: [0.0003450757079311627]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 86 complete:\n",
      "    Train loss: 3.7796323847331097\n",
      "    Eval loss: 3.779935457521567\n",
      "\n",
      "Learning rate: [0.00033472343669322784]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 87 complete:\n",
      "    Train loss: 3.7796320677910855\n",
      "    Eval loss: 3.7799336121760443\n",
      "\n",
      "Learning rate: [0.000324681733592431]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 88 complete:\n",
      "    Train loss: 3.779631907310089\n",
      "    Eval loss: 3.779935320016727\n",
      "\n",
      "Learning rate: [0.00031494128158465805]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 89 complete:\n",
      "    Train loss: 3.7796301601459015\n",
      "    Eval loss: 3.779924750510615\n",
      "\n",
      "Learning rate: [0.0003054930431371183]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 90 complete:\n",
      "    Train loss: 3.7796286836353796\n",
      "    Eval loss: 3.779937248656569\n",
      "\n",
      "Learning rate: [0.00029632825184300473]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 91 complete:\n",
      "    Train loss: 3.7796289786555155\n",
      "    Eval loss: 3.7799376895813586\n",
      "\n",
      "Learning rate: [0.0002874384042877146]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 92 complete:\n",
      "    Train loss: 3.779627532979588\n",
      "    Eval loss: 3.7799340691875543\n",
      "\n",
      "Learning rate: [0.00027881525215908313]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 93 complete:\n",
      "    Train loss: 3.7796266772812297\n",
      "    Eval loss: 3.7799350983027677\n",
      "\n",
      "Learning rate: [0.00027045079459431064]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 94 complete:\n",
      "    Train loss: 3.779625420702529\n",
      "    Eval loss: 3.779933681315492\n",
      "\n",
      "Learning rate: [0.00026233727075648133]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 95 complete:\n",
      "    Train loss: 3.7796252702151962\n",
      "    Eval loss: 3.779922522093817\n",
      "\n",
      "Learning rate: [0.0002544671526337869]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 96 complete:\n",
      "    Train loss: 3.7796240599249025\n",
      "    Eval loss: 3.7799177229721694\n",
      "\n",
      "Learning rate: [0.0002544671526337869]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 97 complete:\n",
      "    Train loss: 3.779624571070888\n",
      "    Eval loss: 3.779932098053746\n",
      "\n",
      "Learning rate: [0.00024683313805477325]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 98 complete:\n",
      "    Train loss: 3.779623421773987\n",
      "    Eval loss: 3.779933993850032\n",
      "\n",
      "Learning rate: [0.00023942814391313006]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 99 complete:\n",
      "    Train loss: 3.7796224362501873\n",
      "    Eval loss: 3.7799479080887046\n",
      "\n",
      "Learning rate: [0.00023224529959573614]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 100 complete:\n",
      "    Train loss: 3.779622776676614\n",
      "    Eval loss: 3.7799344626115854\n",
      "\n",
      "Learning rate: [0.00022527794060786405]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 101 complete:\n",
      "    Train loss: 3.7796216158574607\n",
      "    Eval loss: 3.779932782287276\n",
      "\n",
      "Learning rate: [0.00021851960238962811]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 102 complete:\n",
      "    Train loss: 3.779622018049247\n",
      "    Eval loss: 3.779930511221913\n",
      "\n",
      "Learning rate: [0.00021196401431793926]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 103 complete:\n",
      "    Train loss: 3.779620701430654\n",
      "    Eval loss: 3.779927391520588\n",
      "\n",
      "Learning rate: [0.00020560509388840108]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 104 complete:\n",
      "    Train loss: 3.7796193801059883\n",
      "    Eval loss: 3.7799337127800103\n",
      "\n",
      "Learning rate: [0.00019943694107174903]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 105 complete:\n",
      "    Train loss: 3.7796190387083985\n",
      "    Eval loss: 3.779924938403623\n",
      "\n",
      "Learning rate: [0.00019345383283959656]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 106 complete:\n",
      "    Train loss: 3.779619563041693\n",
      "    Eval loss: 3.7799226169761724\n",
      "\n",
      "Learning rate: [0.00018765021785440865]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 107 complete:\n",
      "    Train loss: 3.7796190303406414\n",
      "    Eval loss: 3.7799243042313715\n",
      "\n",
      "Learning rate: [0.0001820207113187764]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 108 complete:\n",
      "    Train loss: 3.779617733837075\n",
      "    Eval loss: 3.7799213324791023\n",
      "\n",
      "Learning rate: [0.0001765600899792131]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 109 complete:\n",
      "    Train loss: 3.7796171928922364\n",
      "    Eval loss: 3.7799317984703884\n",
      "\n",
      "Learning rate: [0.0001712632872798367]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 110 complete:\n",
      "    Train loss: 3.779616826395939\n",
      "    Eval loss: 3.7799322039478547\n",
      "\n",
      "Learning rate: [0.00016612538866144159]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 111 complete:\n",
      "    Train loss: 3.779616589558622\n",
      "    Eval loss: 3.7799236791370525\n",
      "\n",
      "Learning rate: [0.00016114162700159833]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 112 complete:\n",
      "    Train loss: 3.7796164155978307\n",
      "    Eval loss: 3.7799224025656692\n",
      "\n",
      "Learning rate: [0.00015630737819155038]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 113 complete:\n",
      "    Train loss: 3.779615785380691\n",
      "    Eval loss: 3.779929438275369\n",
      "\n",
      "Learning rate: [0.00015161815684580385]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 114 complete:\n",
      "    Train loss: 3.7796148423000626\n",
      "    Eval loss: 3.779930877444679\n",
      "\n",
      "Learning rate: [0.00014706961214042973]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 115 complete:\n",
      "    Train loss: 3.7796143734475525\n",
      "    Eval loss: 3.779928145070269\n",
      "\n",
      "Learning rate: [0.00014265752377621684]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 116 complete:\n",
      "    Train loss: 3.7796148841965067\n",
      "    Eval loss: 3.779925191180428\n",
      "\n",
      "Learning rate: [0.00013837779806293032]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 117 complete:\n",
      "    Train loss: 3.779614054000604\n",
      "    Eval loss: 3.77992155804547\n",
      "\n",
      "Learning rate: [0.00013422646412104242]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 118 complete:\n",
      "    Train loss: 3.779613480489404\n",
      "    Eval loss: 3.7799270561223333\n",
      "\n",
      "Learning rate: [0.00013019967019741114]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 119 complete:\n",
      "    Train loss: 3.779613084929737\n",
      "    Eval loss: 3.779925485386238\n",
      "\n",
      "Learning rate: [0.0001262936800914888]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 120 complete:\n",
      "    Train loss: 3.7796125708884403\n",
      "    Eval loss: 3.779927660538989\n",
      "\n",
      "Learning rate: [0.00012250486968874412]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 121 complete:\n",
      "    Train loss: 3.7796125898378716\n",
      "    Eval loss: 3.779931110562824\n",
      "\n",
      "Learning rate: [0.0001188297235980818]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 122 complete:\n",
      "    Train loss: 3.7796119926416525\n",
      "    Eval loss: 3.7799248102359577\n",
      "\n",
      "Learning rate: [0.00011526483189013935]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 123 complete:\n",
      "    Train loss: 3.779611451831425\n",
      "    Eval loss: 3.779933972430324\n",
      "\n",
      "Learning rate: [0.00011180688693343516]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 124 complete:\n",
      "    Train loss: 3.7796109049909568\n",
      "    Eval loss: 3.7799215552253433\n",
      "\n",
      "Learning rate: [0.00010845268032543211]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 125 complete:\n",
      "    Train loss: 3.7796106609294164\n",
      "    Eval loss: 3.779917982091374\n",
      "\n",
      "Learning rate: [0.00010519909991566914]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 126 complete:\n",
      "    Train loss: 3.7796100080667347\n",
      "    Eval loss: 3.77993804290912\n",
      "\n",
      "Learning rate: [0.00010204312691819907]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 127 complete:\n",
      "    Train loss: 3.7796102511127265\n",
      "    Eval loss: 3.7799252792302327\n",
      "\n",
      "Learning rate: [9.898183311065309e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 128 complete:\n",
      "    Train loss: 3.77961066738477\n",
      "    Eval loss: 3.7799198437446813\n",
      "\n",
      "Learning rate: [9.601237811733349e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 129 complete:\n",
      "    Train loss: 3.779608908770085\n",
      "    Eval loss: 3.7799250149083723\n",
      "\n",
      "Learning rate: [9.313200677381349e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 130 complete:\n",
      "    Train loss: 3.779609377699248\n",
      "    Eval loss: 3.7799174995365474\n",
      "\n",
      "Learning rate: [9.313200677381349e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 131 complete:\n",
      "    Train loss: 3.7796088730908366\n",
      "    Eval loss: 3.7799204296196343\n",
      "\n",
      "Learning rate: [9.033804657059908e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 132 complete:\n",
      "    Train loss: 3.779608327357814\n",
      "    Eval loss: 3.7799290398161185\n",
      "\n",
      "Learning rate: [8.76279051734811e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 133 complete:\n",
      "    Train loss: 3.7796091037688604\n",
      "    Eval loss: 3.779926680449815\n",
      "\n",
      "Learning rate: [8.499906801827666e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 134 complete:\n",
      "    Train loss: 3.7796089985404704\n",
      "    Eval loss: 3.7799242915537294\n",
      "\n",
      "Learning rate: [8.244909597772835e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 135 complete:\n",
      "    Train loss: 3.7796083277708394\n",
      "    Eval loss: 3.779933717631565\n",
      "\n",
      "Learning rate: [7.99756230983965e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 136 complete:\n",
      "    Train loss: 3.7796077691833028\n",
      "    Eval loss: 3.7799240237082596\n",
      "\n",
      "Learning rate: [7.75763544054446e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 137 complete:\n",
      "    Train loss: 3.7796080110940617\n",
      "    Eval loss: 3.7799223034012788\n",
      "\n",
      "Learning rate: [7.524906377328126e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 138 complete:\n",
      "    Train loss: 3.779607209856395\n",
      "    Eval loss: 3.779931957481563\n",
      "\n",
      "Learning rate: [7.299159186008283e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 139 complete:\n",
      "    Train loss: 3.7796073247027295\n",
      "    Eval loss: 3.779923116655851\n",
      "\n",
      "Learning rate: [7.080184410428034e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 140 complete:\n",
      "    Train loss: 3.7796067845171404\n",
      "    Eval loss: 3.7799230979206313\n",
      "\n",
      "Learning rate: [6.867778878115193e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 141 complete:\n",
      "    Train loss: 3.7796071782612493\n",
      "    Eval loss: 3.779919589810636\n",
      "\n",
      "Learning rate: [6.661745511771737e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 142 complete:\n",
      "    Train loss: 3.7796064095717585\n",
      "    Eval loss: 3.7799250353553107\n",
      "\n",
      "Learning rate: [6.461893146418585e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 143 complete:\n",
      "    Train loss: 3.7796064914568896\n",
      "    Eval loss: 3.779920822895822\n",
      "\n",
      "Learning rate: [6.268036352026027e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 144 complete:\n",
      "    Train loss: 3.7796060036762973\n",
      "    Eval loss: 3.77992235774186\n",
      "\n",
      "Learning rate: [6.079995261465246e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 145 complete:\n",
      "    Train loss: 3.7796061413611333\n",
      "    Eval loss: 3.7799228240172167\n",
      "\n",
      "Learning rate: [5.897595403621289e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 146 complete:\n",
      "    Train loss: 3.7796063587143203\n",
      "    Eval loss: 3.7799234197560336\n",
      "\n",
      "Learning rate: [5.72066754151265e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 147 complete:\n",
      "    Train loss: 3.779605077023119\n",
      "    Eval loss: 3.7799225650498705\n",
      "\n",
      "Learning rate: [5.54904751526727e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 148 complete:\n",
      "    Train loss: 3.779605747188038\n",
      "    Eval loss: 3.779924901124102\n",
      "\n",
      "Learning rate: [5.382576089809252e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 149 complete:\n",
      "    Train loss: 3.779605441909502\n",
      "    Eval loss: 3.779924623531529\n",
      "\n",
      "Learning rate: [5.2210988071149744e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 150 complete:\n",
      "    Train loss: 3.779605129176615\n",
      "    Eval loss: 3.7799233833666492\n",
      "\n",
      "Learning rate: [5.064465842901525e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 151 complete:\n",
      "    Train loss: 3.7796054187374537\n",
      "    Eval loss: 3.7799249441005864\n",
      "\n",
      "Learning rate: [4.912531867614479e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 152 complete:\n",
      "    Train loss: 3.7796049195713137\n",
      "    Eval loss: 3.7799224289350772\n",
      "\n",
      "Learning rate: [4.7651559115860444e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 153 complete:\n",
      "    Train loss: 3.779604496643935\n",
      "    Eval loss: 3.779922799636994\n",
      "\n",
      "Learning rate: [4.622201234238463e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 154 complete:\n",
      "    Train loss: 3.7796044261739796\n",
      "    Eval loss: 3.779920068839316\n",
      "\n",
      "Learning rate: [4.483535197211309e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 155 complete:\n",
      "    Train loss: 3.7796041566564713\n",
      "    Eval loss: 3.779921140352529\n",
      "\n",
      "Learning rate: [4.34902914129497e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 156 complete:\n",
      "    Train loss: 3.7796041174294652\n",
      "    Eval loss: 3.7799232583735805\n",
      "\n",
      "Learning rate: [4.2185582670561204e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 157 complete:\n",
      "    Train loss: 3.779603880106821\n",
      "    Eval loss: 3.7799204523899297\n",
      "\n",
      "Learning rate: [4.092001519044437e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 158 complete:\n",
      "    Train loss: 3.779603673097594\n",
      "    Eval loss: 3.779924028914036\n",
      "\n",
      "Learning rate: [3.969241473473104e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 159 complete:\n",
      "    Train loss: 3.779603947377306\n",
      "    Eval loss: 3.779923945849793\n",
      "\n",
      "Learning rate: [3.850164229268911e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 160 complete:\n",
      "    Train loss: 3.77960384688007\n",
      "    Eval loss: 3.779920840434755\n",
      "\n",
      "Learning rate: [3.734659302390843e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 161 complete:\n",
      "    Train loss: 3.7796033610727466\n",
      "    Eval loss: 3.7799239084986223\n",
      "\n",
      "Learning rate: [3.622619523319118e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 162 complete:\n",
      "    Train loss: 3.7796032520980085\n",
      "    Eval loss: 3.7799221536398826\n",
      "\n",
      "Learning rate: [3.513940937619544e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 163 complete:\n",
      "    Train loss: 3.7796033367851933\n",
      "    Eval loss: 3.7799212296059257\n",
      "\n",
      "Learning rate: [3.408522709490958e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 164 complete:\n",
      "    Train loss: 3.779603309319149\n",
      "    Eval loss: 3.7799222334688842\n",
      "\n",
      "Learning rate: [3.306267028206229e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 165 complete:\n",
      "    Train loss: 3.779603018099167\n",
      "    Eval loss: 3.7799225693620815\n",
      "\n",
      "Learning rate: [3.207079017360042e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 166 complete:\n",
      "    Train loss: 3.7796026633053157\n",
      "    Eval loss: 3.7799222106268604\n",
      "\n",
      "Learning rate: [3.110866646839241e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 167 complete:\n",
      "    Train loss: 3.779602766876628\n",
      "    Eval loss: 3.779922365671512\n",
      "\n",
      "Learning rate: [3.0175406474340638e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 168 complete:\n",
      "    Train loss: 3.7796027766352904\n",
      "    Eval loss: 3.7799216574508536\n",
      "\n",
      "Learning rate: [2.9270144280110418e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 169 complete:\n",
      "    Train loss: 3.7796026603341315\n",
      "    Eval loss: 3.7799230719443115\n",
      "\n",
      "Learning rate: [2.8392039951707106e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 170 complete:\n",
      "    Train loss: 3.7796023130950456\n",
      "    Eval loss: 3.779923287784785\n",
      "\n",
      "Learning rate: [2.754027875315589e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 171 complete:\n",
      "    Train loss: 3.7796023897384554\n",
      "    Eval loss: 3.7799233259683493\n",
      "\n",
      "Learning rate: [2.6714070390561214e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 172 complete:\n",
      "    Train loss: 3.7796023218242163\n",
      "    Eval loss: 3.779922299538117\n",
      "\n",
      "Learning rate: [2.5912648278844376e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 173 complete:\n",
      "    Train loss: 3.779602211802895\n",
      "    Eval loss: 3.779922456221106\n",
      "\n",
      "Learning rate: [2.5135268830479042e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 174 complete:\n",
      "    Train loss: 3.779601912763376\n",
      "    Eval loss: 3.779923332847548\n",
      "\n",
      "Learning rate: [2.438121076556467e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 175 complete:\n",
      "    Train loss: 3.7796020230433136\n",
      "    Eval loss: 3.779921988519754\n",
      "\n",
      "Learning rate: [2.364977444259773e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 176 complete:\n",
      "    Train loss: 3.7796015712204114\n",
      "    Eval loss: 3.779922364771847\n",
      "\n",
      "Learning rate: [2.2940281209319798e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 177 complete:\n",
      "    Train loss: 3.779601829722282\n",
      "    Eval loss: 3.7799219758944735\n",
      "\n",
      "Learning rate: [2.2252072773040205e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 178 complete:\n",
      "    Train loss: 3.7796014332270835\n",
      "    Eval loss: 3.77992247336325\n",
      "\n",
      "Learning rate: [2.1584510589848998e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 179 complete:\n",
      "    Train loss: 3.7796016829155286\n",
      "    Eval loss: 3.779923280217604\n",
      "\n",
      "Learning rate: [2.0936975272153527e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 180 complete:\n",
      "    Train loss: 3.779601488130001\n",
      "    Eval loss: 3.7799219033762577\n",
      "\n",
      "Learning rate: [2.030886601398892e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 181 complete:\n",
      "    Train loss: 3.779601376742481\n",
      "    Eval loss: 3.779921546608355\n",
      "\n",
      "Learning rate: [1.969960003356925e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 182 complete:\n",
      "    Train loss: 3.7796014106814373\n",
      "    Eval loss: 3.779921766480032\n",
      "\n",
      "Learning rate: [1.9108612032562174e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 183 complete:\n",
      "    Train loss: 3.7796012909712395\n",
      "    Eval loss: 3.7799219793349508\n",
      "\n",
      "Learning rate: [1.853535367158531e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 184 complete:\n",
      "    Train loss: 3.7796015305501185\n",
      "    Eval loss: 3.7799216855697706\n",
      "\n",
      "Learning rate: [1.797929306143775e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 185 complete:\n",
      "    Train loss: 3.77960114294047\n",
      "    Eval loss: 3.7799232032105166\n",
      "\n",
      "Learning rate: [1.7439914269594616e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 186 complete:\n",
      "    Train loss: 3.7796010758761382\n",
      "    Eval loss: 3.7799228715732953\n",
      "\n",
      "Learning rate: [1.6916716841506776e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 187 complete:\n",
      "    Train loss: 3.779601014524447\n",
      "    Eval loss: 3.7799221732250103\n",
      "\n",
      "Learning rate: [1.6409215336261573e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 188 complete:\n",
      "    Train loss: 3.779600956373596\n",
      "    Eval loss: 3.779922596355814\n",
      "\n",
      "Learning rate: [1.5916938876173724e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 189 complete:\n",
      "    Train loss: 3.7796008489877306\n",
      "    Eval loss: 3.779921596912824\n",
      "\n",
      "Learning rate: [1.5439430709888513e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 190 complete:\n",
      "    Train loss: 3.779600464530588\n",
      "    Eval loss: 3.7799221304343993\n",
      "\n",
      "Learning rate: [1.4976247788591857e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 191 complete:\n",
      "    Train loss: 3.7796010571506558\n",
      "    Eval loss: 3.7799220274664966\n",
      "\n",
      "Learning rate: [1.4526960354934102e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 192 complete:\n",
      "    Train loss: 3.7796005867790257\n",
      "    Eval loss: 3.779921917617699\n",
      "\n",
      "Learning rate: [1.4091151544286078e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 193 complete:\n",
      "    Train loss: 3.7796005634399115\n",
      "    Eval loss: 3.7799229045582217\n",
      "\n",
      "Learning rate: [1.3668416997957495e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 194 complete:\n",
      "    Train loss: 3.779600644907416\n",
      "    Eval loss: 3.7799219906658656\n",
      "\n",
      "Learning rate: [1.325836448801877e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 195 complete:\n",
      "    Train loss: 3.779600498753612\n",
      "    Eval loss: 3.7799220149824246\n",
      "\n",
      "Learning rate: [1.2860613553378206e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 196 complete:\n",
      "    Train loss: 3.7796004164076695\n",
      "    Eval loss: 3.7799225648604606\n",
      "\n",
      "Learning rate: [1.247479514677686e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 197 complete:\n",
      "    Train loss: 3.779600534559086\n",
      "    Eval loss: 3.779921824686761\n",
      "\n",
      "Learning rate: [1.2100551292373554e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 198 complete:\n",
      "    Train loss: 3.7796000754959596\n",
      "    Eval loss: 3.7799221068029873\n",
      "\n",
      "Learning rate: [1.1737534753602347e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 199 complete:\n",
      "    Train loss: 3.7796002954283066\n",
      "    Eval loss: 3.7799210749699403\n",
      "\n",
      "Learning rate: [1.1385408710994277e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 200 complete:\n",
      "    Train loss: 3.7796002982492722\n",
      "    Eval loss: 3.779921838634709\n",
      "\n",
      "Learning rate: [1.1043846449664449e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 201 complete:\n",
      "    Train loss: 3.7796002576172834\n",
      "    Eval loss: 3.779921647515818\n",
      "\n",
      "Learning rate: [1.0712531056174515e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 202 complete:\n",
      "    Train loss: 3.7796004363701745\n",
      "    Eval loss: 3.7799219957049606\n",
      "\n",
      "Learning rate: [1.039115512448928e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 203 complete:\n",
      "    Train loss: 3.7796003540465897\n",
      "    Eval loss: 3.77992167688639\n",
      "\n",
      "Learning rate: [1.00794204707546e-05]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 204 complete:\n",
      "    Train loss: 3.779600361336494\n",
      "    Eval loss: 3.7799216237063233\n",
      "\n",
      "Learning rate: [9.777037856631961e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 205 complete:\n",
      "    Train loss: 3.7796000229797677\n",
      "    Eval loss: 3.779921925157434\n",
      "\n",
      "Learning rate: [9.483726720933002e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 206 complete:\n",
      "    Train loss: 3.779600303575725\n",
      "    Eval loss: 3.7799219176659116\n",
      "\n",
      "Learning rate: [9.199214919305012e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 207 complete:\n",
      "    Train loss: 3.779599869869197\n",
      "    Eval loss: 3.779922197435528\n",
      "\n",
      "Learning rate: [8.92323847172586e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 208 complete:\n",
      "    Train loss: 3.7796001277919533\n",
      "    Eval loss: 3.779921900565995\n",
      "\n",
      "Learning rate: [8.655541317574084e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 209 complete:\n",
      "    Train loss: 3.7795998625438325\n",
      "    Eval loss: 3.779921746256835\n",
      "\n",
      "Learning rate: [8.395875078046861e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 210 complete:\n",
      "    Train loss: 3.7795996978448123\n",
      "    Eval loss: 3.7799220242947844\n",
      "\n",
      "Learning rate: [8.143998825705455e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 211 complete:\n",
      "    Train loss: 3.779599837664216\n",
      "    Eval loss: 3.7799213482008525\n",
      "\n",
      "Learning rate: [7.89967886093429e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 212 complete:\n",
      "    Train loss: 3.7796000518554544\n",
      "    Eval loss: 3.7799216099691373\n",
      "\n",
      "Learning rate: [7.662688495106263e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 213 complete:\n",
      "    Train loss: 3.7795996729945682\n",
      "    Eval loss: 3.779921587300108\n",
      "\n",
      "Learning rate: [7.432807840253075e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 214 complete:\n",
      "    Train loss: 3.779599652389552\n",
      "    Eval loss: 3.779921973886789\n",
      "\n",
      "Learning rate: [7.209823605045482e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 215 complete:\n",
      "    Train loss: 3.7795994560927793\n",
      "    Eval loss: 3.77992182852668\n",
      "\n",
      "Learning rate: [6.993528896894118e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 216 complete:\n",
      "    Train loss: 3.779599804794014\n",
      "    Eval loss: 3.7799222253413367\n",
      "\n",
      "Learning rate: [6.783723029987294e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 217 complete:\n",
      "    Train loss: 3.779599553727898\n",
      "    Eval loss: 3.7799220957897197\n",
      "\n",
      "Learning rate: [6.580211339087675e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 218 complete:\n",
      "    Train loss: 3.7795993934424095\n",
      "    Eval loss: 3.7799219159794437\n",
      "\n",
      "Learning rate: [6.382804998915045e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 219 complete:\n",
      "    Train loss: 3.779599707258465\n",
      "    Eval loss: 3.7799218892145436\n",
      "\n",
      "Learning rate: [6.191320848947594e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 220 complete:\n",
      "    Train loss: 3.779599562628995\n",
      "    Eval loss: 3.7799223116442886\n",
      "\n",
      "Learning rate: [6.0055812234791664e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 221 complete:\n",
      "    Train loss: 3.779599554361968\n",
      "    Eval loss: 3.7799218111895594\n",
      "\n",
      "Learning rate: [5.825413786774791e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 222 complete:\n",
      "    Train loss: 3.7795996893614516\n",
      "    Eval loss: 3.7799223396247594\n",
      "\n",
      "Learning rate: [5.6506513731715475e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 223 complete:\n",
      "    Train loss: 3.779599849117704\n",
      "    Eval loss: 3.7799218788400193\n",
      "\n",
      "Learning rate: [5.481131831976401e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 224 complete:\n",
      "    Train loss: 3.7795996475476112\n",
      "    Eval loss: 3.7799215394806245\n",
      "\n",
      "Learning rate: [5.3166978770171085e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 225 complete:\n",
      "    Train loss: 3.779599389210968\n",
      "    Eval loss: 3.779921627732555\n",
      "\n",
      "Learning rate: [5.157196940706595e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 226 complete:\n",
      "    Train loss: 3.779599871991949\n",
      "    Eval loss: 3.779921732396356\n",
      "\n",
      "Learning rate: [5.002481032485397e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 227 complete:\n",
      "    Train loss: 3.7795995312257378\n",
      "    Eval loss: 3.7799216892070193\n",
      "\n",
      "Learning rate: [4.8524066015108354e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 228 complete:\n",
      "    Train loss: 3.7795994074330412\n",
      "    Eval loss: 3.779922190196908\n",
      "\n",
      "Learning rate: [4.706834403465511e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 229 complete:\n",
      "    Train loss: 3.779599370004334\n",
      "    Eval loss: 3.779922718717538\n",
      "\n",
      "Learning rate: [4.565629371361545e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 230 complete:\n",
      "    Train loss: 3.7795992336998414\n",
      "    Eval loss: 3.779922017982168\n",
      "\n",
      "Learning rate: [4.428660490220699e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 231 complete:\n",
      "    Train loss: 3.7795993573585775\n",
      "    Eval loss: 3.7799215134143114\n",
      "\n",
      "Learning rate: [4.2958006755140775e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 232 complete:\n",
      "    Train loss: 3.7795993469764704\n",
      "    Eval loss: 3.7799217997161083\n",
      "\n",
      "Learning rate: [4.166926655248655e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 233 complete:\n",
      "    Train loss: 3.7795995699736875\n",
      "    Eval loss: 3.779921460990703\n",
      "\n",
      "Learning rate: [4.041918855591195e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 234 complete:\n",
      "    Train loss: 3.7795992412949624\n",
      "    Eval loss: 3.7799211319357178\n",
      "\n",
      "Learning rate: [3.920661289923459e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 235 complete:\n",
      "    Train loss: 3.779599566372407\n",
      "    Eval loss: 3.779921558883909\n",
      "\n",
      "Learning rate: [3.8030414512257554e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 236 complete:\n",
      "    Train loss: 3.7795996896655715\n",
      "    Eval loss: 3.7799222058183144\n",
      "\n",
      "Learning rate: [3.6889502076889826e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 237 complete:\n",
      "    Train loss: 3.7795991857569455\n",
      "    Eval loss: 3.7799218834785355\n",
      "\n",
      "Learning rate: [3.578281701458313e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 238 complete:\n",
      "    Train loss: 3.7795992380528687\n",
      "    Eval loss: 3.7799216262705047\n",
      "\n",
      "Learning rate: [3.4709332504145635e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 239 complete:\n",
      "    Train loss: 3.7795994332648863\n",
      "    Eval loss: 3.779921823554256\n",
      "\n",
      "Learning rate: [3.3668052529021266e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 240 complete:\n",
      "    Train loss: 3.7795992331243617\n",
      "    Eval loss: 3.7799222676307203\n",
      "\n",
      "Learning rate: [3.2658010953150628e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 241 complete:\n",
      "    Train loss: 3.779599276047996\n",
      "    Eval loss: 3.779922246977538\n",
      "\n",
      "Learning rate: [3.1678270624556107e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 242 complete:\n",
      "    Train loss: 3.779599116230462\n",
      "    Eval loss: 3.779922192837611\n",
      "\n",
      "Learning rate: [3.0727922505819422e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 243 complete:\n",
      "    Train loss: 3.779599180422981\n",
      "    Eval loss: 3.7799220787347707\n",
      "\n",
      "Learning rate: [2.980608483064484e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 244 complete:\n",
      "    Train loss: 3.779599040454026\n",
      "    Eval loss: 3.77992197230767\n",
      "\n",
      "Learning rate: [2.8911902285725495e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 245 complete:\n",
      "    Train loss: 3.7795988942759755\n",
      "    Eval loss: 3.7799218697464156\n",
      "\n",
      "Learning rate: [2.8044545217153728e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 246 complete:\n",
      "    Train loss: 3.779599195410212\n",
      "    Eval loss: 3.7799214702050743\n",
      "\n",
      "Learning rate: [2.7203208860639114e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 247 complete:\n",
      "    Train loss: 3.779599333814075\n",
      "    Eval loss: 3.7799213929528563\n",
      "\n",
      "Learning rate: [2.638711259481994e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 248 complete:\n",
      "    Train loss: 3.7795987186548747\n",
      "    Eval loss: 3.7799219031742184\n",
      "\n",
      "Learning rate: [2.559549921697534e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 249 complete:\n",
      "    Train loss: 3.7795990183920423\n",
      "    Eval loss: 3.7799218693664365\n",
      "\n",
      "Learning rate: [2.482763424046608e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 250 complete:\n",
      "    Train loss: 3.7795992385131667\n",
      "    Eval loss: 3.7799217834805483\n",
      "\n",
      "Learning rate: [2.4082805213252097e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 251 complete:\n",
      "    Train loss: 3.7795992426343554\n",
      "    Eval loss: 3.779921981905503\n",
      "\n",
      "Learning rate: [2.3360321056854534e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 252 complete:\n",
      "    Train loss: 3.779599287359701\n",
      "    Eval loss: 3.779922105864465\n",
      "\n",
      "Learning rate: [2.2659511425148897e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 253 complete:\n",
      "    Train loss: 3.779598698122726\n",
      "    Eval loss: 3.7799219052779893\n",
      "\n",
      "Learning rate: [2.1979726082394428e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 254 complete:\n",
      "    Train loss: 3.7795992398158873\n",
      "    Eval loss: 3.779921870356154\n",
      "\n",
      "Learning rate: [2.1320334299922594e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 255 complete:\n",
      "    Train loss: 3.7795993189864237\n",
      "    Eval loss: 3.7799213856465532\n",
      "\n",
      "Learning rate: [2.0680724270924915e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 256 complete:\n",
      "    Train loss: 3.7795993016178278\n",
      "    Eval loss: 3.779921812280223\n",
      "\n",
      "Learning rate: [2.0060302542797167e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 257 complete:\n",
      "    Train loss: 3.779598893788288\n",
      "    Eval loss: 3.7799224707757295\n",
      "\n",
      "Learning rate: [1.9458493466513253e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 258 complete:\n",
      "    Train loss: 3.779599116374716\n",
      "    Eval loss: 3.7799222935386045\n",
      "\n",
      "Learning rate: [1.8874738662517854e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 259 complete:\n",
      "    Train loss: 3.779598919411104\n",
      "    Eval loss: 3.7799222548759346\n",
      "\n",
      "Learning rate: [1.8308496502642317e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 260 complete:\n",
      "    Train loss: 3.779598995344302\n",
      "    Eval loss: 3.7799217702109655\n",
      "\n",
      "Learning rate: [1.7759241607563047e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 261 complete:\n",
      "    Train loss: 3.779598753189222\n",
      "    Eval loss: 3.779922102420933\n",
      "\n",
      "Learning rate: [1.7226464359336154e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 262 complete:\n",
      "    Train loss: 3.7795988824963747\n",
      "    Eval loss: 3.7799222364093668\n",
      "\n",
      "Learning rate: [1.6709670428556069e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 263 complete:\n",
      "    Train loss: 3.779598631073943\n",
      "    Eval loss: 3.7799216253710646\n",
      "\n",
      "Learning rate: [1.6208380315699386e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 264 complete:\n",
      "    Train loss: 3.779598926874258\n",
      "    Eval loss: 3.7799225892704467\n",
      "\n",
      "Learning rate: [1.5722128906228404e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 265 complete:\n",
      "    Train loss: 3.7795988098233186\n",
      "    Eval loss: 3.7799221907102347\n",
      "\n",
      "Learning rate: [1.5250465039041552e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 266 complete:\n",
      "    Train loss: 3.779599084941968\n",
      "    Eval loss: 3.779921755537417\n",
      "\n",
      "Learning rate: [1.4792951087870304e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 267 complete:\n",
      "    Train loss: 3.779598805433551\n",
      "    Eval loss: 3.7799216744291146\n",
      "\n",
      "Learning rate: [1.4349162555234194e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 268 complete:\n",
      "    Train loss: 3.779599012938042\n",
      "    Eval loss: 3.779921400473923\n",
      "\n",
      "Learning rate: [1.3918687678577166e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 269 complete:\n",
      "    Train loss: 3.7795987064619427\n",
      "    Eval loss: 3.779922313224058\n",
      "\n",
      "Learning rate: [1.350112704821985e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 270 complete:\n",
      "    Train loss: 3.7795990137341495\n",
      "    Eval loss: 3.779922253056921\n",
      "\n",
      "Learning rate: [1.3096093236773254e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 271 complete:\n",
      "    Train loss: 3.7795991311059005\n",
      "    Eval loss: 3.7799221011638435\n",
      "\n",
      "Learning rate: [1.2703210439670056e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 272 complete:\n",
      "    Train loss: 3.779598907298403\n",
      "    Eval loss: 3.7799218340356524\n",
      "\n",
      "Learning rate: [1.2322114126479954e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 273 complete:\n",
      "    Train loss: 3.779599480411595\n",
      "    Eval loss: 3.779922123020178\n",
      "\n",
      "Learning rate: [1.1952450702685554e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 274 complete:\n",
      "    Train loss: 3.779598877504365\n",
      "    Eval loss: 3.7799220728426204\n",
      "\n",
      "Learning rate: [1.1593877181604988e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 275 complete:\n",
      "    Train loss: 3.779598749312328\n",
      "    Eval loss: 3.7799215754904774\n",
      "\n",
      "Learning rate: [1.1246060866156837e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 276 complete:\n",
      "    Train loss: 3.7795991131968405\n",
      "    Eval loss: 3.779921225551255\n",
      "\n",
      "Learning rate: [1.0908679040172132e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 277 complete:\n",
      "    Train loss: 3.779598708994924\n",
      "    Eval loss: 3.7799214387070235\n",
      "\n",
      "Learning rate: [1.0581418668966967e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 278 complete:\n",
      "    Train loss: 3.7795987039287224\n",
      "    Eval loss: 3.7799226022543095\n",
      "\n",
      "Learning rate: [1.0263976108897959e-06]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 279 complete:\n",
      "    Train loss: 3.779599205433599\n",
      "    Eval loss: 3.779921821842047\n",
      "\n",
      "Learning rate: [9.95605682563102e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 280 complete:\n",
      "    Train loss: 3.7795988155261795\n",
      "    Eval loss: 3.779921832174623\n",
      "\n",
      "Learning rate: [9.657375120862087e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 281 complete:\n",
      "    Train loss: 3.7795991438153163\n",
      "    Eval loss: 3.779921133721031\n",
      "\n",
      "Learning rate: [9.367653867236225e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 282 complete:\n",
      "    Train loss: 3.779598874552647\n",
      "    Eval loss: 3.779921602157902\n",
      "\n",
      "Learning rate: [9.086624251219137e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 283 complete:\n",
      "    Train loss: 3.779598592248865\n",
      "    Eval loss: 3.7799216762540766\n",
      "\n",
      "Learning rate: [8.814025523682562e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 284 complete:\n",
      "    Train loss: 3.7795989307998217\n",
      "    Eval loss: 3.77992208349173\n",
      "\n",
      "Learning rate: [8.549604757972085e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 285 complete:\n",
      "    Train loss: 3.779598806126865\n",
      "    Eval loss: 3.7799217279643065\n",
      "\n",
      "Learning rate: [8.293116615232922e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 286 complete:\n",
      "    Train loss: 3.7795988905779283\n",
      "    Eval loss: 3.779921516273866\n",
      "\n",
      "Learning rate: [8.044323116775934e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 287 complete:\n",
      "    Train loss: 3.7795992887018848\n",
      "    Eval loss: 3.7799220367929847\n",
      "\n",
      "Learning rate: [7.802993423272656e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 288 complete:\n",
      "    Train loss: 3.7795986559499437\n",
      "    Eval loss: 3.7799214681750217\n",
      "\n",
      "Learning rate: [7.568903620574476e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 289 complete:\n",
      "    Train loss: 3.7795987127099298\n",
      "    Eval loss: 3.7799222772784056\n",
      "\n",
      "Learning rate: [7.341836511957241e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 290 complete:\n",
      "    Train loss: 3.7795991858763127\n",
      "    Eval loss: 3.779921685366459\n",
      "\n",
      "Learning rate: [7.121581416598523e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 291 complete:\n",
      "    Train loss: 3.779598758192215\n",
      "    Eval loss: 3.779921611567771\n",
      "\n",
      "Learning rate: [6.907933974100568e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 292 complete:\n",
      "    Train loss: 3.7795988059912395\n",
      "    Eval loss: 3.7799216443358303\n",
      "\n",
      "Learning rate: [6.700695954877551e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 293 complete:\n",
      "    Train loss: 3.7795987868985903\n",
      "    Eval loss: 3.7799221143253883\n",
      "\n",
      "Learning rate: [6.499675076231224e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 294 complete:\n",
      "    Train loss: 3.7795990432715865\n",
      "    Eval loss: 3.7799215847969974\n",
      "\n",
      "Learning rate: [6.304684823944288e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 295 complete:\n",
      "    Train loss: 3.779599084399317\n",
      "    Eval loss: 3.77992150625635\n",
      "\n",
      "Learning rate: [6.115544279225959e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed checkpoint at epoch: {ep}\n",
      "\n",
      "Epoch 296 complete:\n",
      "    Train loss: 3.779598752777655\n",
      "    Eval loss: 3.7799219846858305\n",
      "\n",
      "Learning rate: [5.93207795084918e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 297 complete:\n",
      "    Train loss: 3.779598917323434\n",
      "    Eval loss: 3.779922167851516\n",
      "\n",
      "Learning rate: [5.754115612323705e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 298 complete:\n",
      "    Train loss: 3.7795988977773005\n",
      "    Eval loss: 3.7799217552669737\n",
      "\n",
      "Learning rate: [5.581492143953993e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "\n",
      "Epoch 299 complete:\n",
      "    Train loss: 3.779598888362213\n",
      "    Eval loss: 3.7799217866537087\n",
      "\n",
      "Learning rate: [5.414047379635373e-07]\n",
      "Peak GPU memory usage:\n",
      "0.06745 GB\n",
      "Completed training.\n",
      "Unloaded datasets.\n",
      "Unloaded datasets.\n",
      "Making binned sets dataset.\n",
      "Number of NA values: \n",
      " q_squared        0\n",
      "costheta_mu      0\n",
      "costheta_K       0\n",
      "chi              0\n",
      "dc9              0\n",
      "dc9_bin_index    0\n",
      "dtype: int64\n",
      "Removed rows that have a NaN.\n",
      "Applying standand scale.\n",
      "Applied standard scale.\n",
      "Shuffled dataframe.\n",
      "Saved tensor of shape: torch.Size([2200, 24000, 4]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\24000_eval_features.pt\n",
      "Saved tensor of shape: torch.Size([2200]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\24000_eval_labels.pt\n",
      "Saved tensor of shape: torch.Size([44]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\24000_eval_bin_map.pt\n",
      "Made binned sets dataset.\n",
      "Loaded tensor of shape: torch.Size([2200, 24000, 4]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\24000_eval_features.pt\n",
      "Loaded tensor of shape: torch.Size([2200]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\24000_eval_labels.pt\n",
      "Loaded tensor of shape: torch.Size([44]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\24000_eval_bin_map.pt\n",
      "Making binned sets dataset.\n",
      "Number of NA values: \n",
      " q_squared        0\n",
      "costheta_mu      0\n",
      "costheta_K       0\n",
      "chi              0\n",
      "dc9              0\n",
      "dc9_bin_index    0\n",
      "dtype: int64\n",
      "Removed rows that have a NaN.\n",
      "Applying standand scale.\n",
      "Applied standard scale.\n",
      "Shuffled dataframe.\n",
      "Saved tensor of shape: torch.Size([2000, 24000, 4]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\24000_eval_sens_features.pt\n",
      "Saved tensor of shape: torch.Size([2000]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\24000_eval_sens_labels.pt\n",
      "Saved tensor of shape: torch.Size([44]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\24000_eval_sens_bin_map.pt\n",
      "Made binned sets dataset.\n",
      "Loaded tensor of shape: torch.Size([2000, 24000, 4]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\24000_eval_sens_features.pt\n",
      "Loaded tensor of shape: torch.Size([2000]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\24000_eval_sens_labels.pt\n",
      "Loaded tensor of shape: torch.Size([44]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\24000_eval_sens_bin_map.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tetha\\Desktop\\btokstll\\logic\\scripts\\helpers\\experiment\\results_table.py:49: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  self.table.loc[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unloaded datasets.\n",
      "Unloaded datasets.\n",
      "Making binned sets dataset.\n",
      "Number of NA values: \n",
      " q_squared        0\n",
      "costheta_mu      0\n",
      "costheta_K       0\n",
      "chi              0\n",
      "dc9              0\n",
      "dc9_bin_index    0\n",
      "dtype: int64\n",
      "Removed rows that have a NaN.\n",
      "Applying standand scale.\n",
      "Applied standard scale.\n",
      "Shuffled dataframe.\n",
      "Saved tensor of shape: torch.Size([2200, 6000, 4]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\6000_eval_features.pt\n",
      "Saved tensor of shape: torch.Size([2200]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\6000_eval_labels.pt\n",
      "Saved tensor of shape: torch.Size([44]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\6000_eval_bin_map.pt\n",
      "Made binned sets dataset.\n",
      "Loaded tensor of shape: torch.Size([2200, 6000, 4]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\6000_eval_features.pt\n",
      "Loaded tensor of shape: torch.Size([2200]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\6000_eval_labels.pt\n",
      "Loaded tensor of shape: torch.Size([44]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\6000_eval_bin_map.pt\n",
      "Making binned sets dataset.\n",
      "Number of NA values: \n",
      " q_squared        0\n",
      "costheta_mu      0\n",
      "costheta_K       0\n",
      "chi              0\n",
      "dc9              0\n",
      "dc9_bin_index    0\n",
      "dtype: int64\n",
      "Removed rows that have a NaN.\n",
      "Applying standand scale.\n",
      "Applied standard scale.\n",
      "Shuffled dataframe.\n",
      "Saved tensor of shape: torch.Size([2000, 6000, 4]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\6000_eval_sens_features.pt\n",
      "Saved tensor of shape: torch.Size([2000]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\6000_eval_sens_labels.pt\n",
      "Saved tensor of shape: torch.Size([44]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\6000_eval_sens_bin_map.pt\n",
      "Made binned sets dataset.\n",
      "Loaded tensor of shape: torch.Size([2000, 6000, 4]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\6000_eval_sens_features.pt\n",
      "Loaded tensor of shape: torch.Size([2000]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\6000_eval_sens_labels.pt\n",
      "Loaded tensor of shape: torch.Size([44]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\6000_eval_sens_bin_map.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tetha\\Desktop\\btokstll\\logic\\scripts\\helpers\\experiment\\results_table.py:49: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  self.table.loc[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unloaded datasets.\n",
      "Unloaded datasets.\n",
      "Making binned sets dataset.\n",
      "Number of NA values: \n",
      " q_squared        0\n",
      "costheta_mu      0\n",
      "costheta_K       0\n",
      "chi              0\n",
      "dc9              0\n",
      "dc9_bin_index    0\n",
      "dtype: int64\n",
      "Removed rows that have a NaN.\n",
      "Applying standand scale.\n",
      "Applied standard scale.\n",
      "Shuffled dataframe.\n",
      "Saved tensor of shape: torch.Size([2200, 70000, 4]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\70000_eval_features.pt\n",
      "Saved tensor of shape: torch.Size([2200]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\70000_eval_labels.pt\n",
      "Saved tensor of shape: torch.Size([44]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\70000_eval_bin_map.pt\n",
      "Made binned sets dataset.\n",
      "Loaded tensor of shape: torch.Size([2200, 70000, 4]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\70000_eval_features.pt\n",
      "Loaded tensor of shape: torch.Size([2200]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\70000_eval_labels.pt\n",
      "Loaded tensor of shape: torch.Size([44]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\70000_eval_bin_map.pt\n",
      "Making binned sets dataset.\n",
      "Number of NA values: \n",
      " q_squared        0\n",
      "costheta_mu      0\n",
      "costheta_K       0\n",
      "chi              0\n",
      "dc9              0\n",
      "dc9_bin_index    0\n",
      "dtype: int64\n",
      "Removed rows that have a NaN.\n",
      "Applying standand scale.\n",
      "Applied standard scale.\n",
      "Shuffled dataframe.\n",
      "Saved tensor of shape: torch.Size([2000, 70000, 4]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\70000_eval_sens_features.pt\n",
      "Saved tensor of shape: torch.Size([2000]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\70000_eval_sens_labels.pt\n",
      "Saved tensor of shape: torch.Size([44]) to ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\70000_eval_sens_bin_map.pt\n",
      "Made binned sets dataset.\n",
      "Loaded tensor of shape: torch.Size([2000, 70000, 4]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\70000_eval_sens_features.pt\n",
      "Loaded tensor of shape: torch.Size([2000]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\70000_eval_sens_labels.pt\n",
      "Loaded tensor of shape: torch.Size([44]) from ..\\..\\state\\new_physics\\data\\processed\\sets_binned_gen_q2v_loose\\70000_eval_sens_bin_map.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tetha\\Desktop\\btokstll\\logic\\scripts\\helpers\\experiment\\results_table.py:49: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  self.table.loc[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unloaded datasets.\n",
      "Unloaded datasets.\n"
     ]
    }
   ],
   "source": [
    "event_by_event_group = Event_by_Event_Group(\n",
    "    num_evaluation_sets_per_label=50,\n",
    "    num_evaluation_sets_per_label_sensitivity=2_000,\n",
    "    q_squared_veto=Names_of_q_Squared_Vetos().loose,\n",
    "    std_scale=True,\n",
    "    shuffle=True,\n",
    "    loss_fn=CrossEntropyLoss(),\n",
    "    learning_rate=3e-3,\n",
    "    learning_rate_scheduler_reduction_factor=0.97,\n",
    "    size_of_training_batch=10_000,\n",
    "    size_of_evaluation_batch=10_000,\n",
    "    number_of_epochs=300,\n",
    "    number_of_epochs_between_checkpoints=5,\n",
    "    results_table=results_table,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "event_by_event_group.train_subset(levels=[Names_of_Levels.generator,], remake_datasets=True)\n",
    "\n",
    "# event_by_event_group.train_all(remake_datasets=True)\n",
    "# event_by_event_group.evaluate_all(remake_datasets=False)\n",
    "\n",
    "event_by_event_group.evaluate_subset(\n",
    "    levels=[Names_of_Levels().generator,], \n",
    "    nums_events_per_set=[24_000, 6_000, 70_000], \n",
    "    remake_datasets=True,\n",
    "    epoch=\"final\"\n",
    ")\n",
    "\n",
    "# ebe = event_by_event_group.get_individual(level=Names_of_Levels().detector)\n",
    "# evaluate_model(\n",
    "#     model=ebe.model,\n",
    "#     evaluation_dataset=Binned_Sets_Dataset(\n",
    "#         settings=Binned_Sets_Dataset_Settings(\n",
    "#             level=Names_of_Levels().detector,\n",
    "#             split=Names_of_Splits().train,\n",
    "#             num_events_per_set=24_000,\n",
    "#             num_sets_per_label=50,\n",
    "#             is_sensitivity_study=False,\n",
    "#             q_squared_veto=Names_of_q_Squared_Vetos().loose,\n",
    "#             std_scale=True,\n",
    "#             shuffle=True,\n",
    "#             path_to_main_datasets_dir=Paths_to_Directories().path_to_main_datasets_dir,\n",
    "#             path_to_raw_signal_dir=Paths_to_Directories().path_to_raw_signal_dir,\n",
    "#             path_to_raw_bkg_dir=Paths_to_Directories().path_to_raw_bkg_dir\n",
    "#         ), \n",
    "#         remake=False\n",
    "#     ),\n",
    "#     sensitivity_evaluation_dataset=Binned_Sets_Dataset(\n",
    "#         ebe._get_sensitivity_evaluation_set_dataset_settings(num_events_per_set=24_000), \n",
    "#         remake=False\n",
    "#     ),\n",
    "#     results_table=results_table,\n",
    "#     device=device,\n",
    "#     epoch=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "pandas.read_pickle(\"../../state/new_physics/data/raw/bkg/mu_sideb_generic_charge_eval.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
