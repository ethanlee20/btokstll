{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import scipy.special\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from library.datasets import (\n",
    "    Signal_Images_Dataset, \n",
    "    Bootstrapped_Unbinned_Signal_Dataset, \n",
    "    Binned_Signal_Dataset\n",
    ")\n",
    "from library.models import CNN_Res, Deep_Sets, Event_By_Event_NN\n",
    "from library.nn_training import select_device, train_and_eval\n",
    "from library.plotting import (\n",
    "    plot_loss_curves, \n",
    "    setup_high_quality_mpl_params, \n",
    "    plot_prediction_linearity, \n",
    "    make_plot_note, \n",
    "    plot_volume_slices\n",
    ")\n",
    "from library.util import bootstrap_labeled_sets, get_num_per_unique_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select device (cuda if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = select_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup fancy plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_high_quality_mpl_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_save_dir = \"../../state/new_physics/data/processed\"\n",
    "raw_signal_dir = \"../../state/new_physics/data/raw/signal\"\n",
    "model_dir = \"../../state/new_physics/models\"\n",
    "\n",
    "std_scale = True\n",
    "q_squared_veto = True\n",
    "balanced_classes = True\n",
    "\n",
    "set_sizes = [70_000, 24_000, 6_000]\n",
    "\n",
    "new_physics_delta_c9_value = -0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"gen\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shawn's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_image_bins = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = {\n",
    "    num_events_per_set : Signal_Images_Dataset(\n",
    "        level=level, \n",
    "        split=\"train\", \n",
    "        save_dir=dataset_save_dir,\n",
    "        extra_description=num_events_per_set,\n",
    "    ) \n",
    "    for num_events_per_set in set_sizes\n",
    "}\n",
    "\n",
    "eval_datasets = {\n",
    "    num_events_per_set : Signal_Images_Dataset(\n",
    "        level=level, \n",
    "        split=\"eval\", \n",
    "        save_dir=dataset_save_dir,\n",
    "        extra_description=num_events_per_set,\n",
    "    ) \n",
    "    for num_events_per_set in set_sizes\n",
    "}\n",
    "\n",
    "single_label_eval_datasets = {\n",
    "    num_events_per_set : Signal_Images_Dataset(\n",
    "        level=level, \n",
    "        split=\"eval\", \n",
    "        save_dir=dataset_save_dir,\n",
    "        extra_description=f\"{num_events_per_set}_single\",\n",
    "    ) \n",
    "    for num_events_per_set in set_sizes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_events_per_set in set_sizes:\n",
    "\n",
    "    train_datasets[num_events_per_set].generate(\n",
    "        raw_trials=range(1,21), \n",
    "        raw_signal_dir=raw_signal_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=50,\n",
    "        n_bins=num_image_bins,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "    )\n",
    "\n",
    "    eval_datasets[num_events_per_set].generate(\n",
    "        raw_trials=range(21,41), \n",
    "        raw_signal_dir=raw_signal_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=50,\n",
    "        n_bins=num_image_bins,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "    )\n",
    "\n",
    "    single_label_eval_datasets[num_events_per_set].generate(\n",
    "        raw_trials=range(21,41), \n",
    "        raw_signal_dir=raw_signal_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=2000,\n",
    "        n_bins=num_image_bins,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "        labels_to_sample=[new_physics_delta_c9_value],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peek at features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_events_per_set = 24_000\n",
    "\n",
    "dset = train_datasets[num_events_per_set]\n",
    "dset.load()\n",
    "\n",
    "plot_volume_slices(\n",
    "     dset.features[0], \n",
    "     n_slices=3, \n",
    "     note=r\"$\\delta C_9$ : \"+f\"{dset.labels[0]}\"\n",
    ")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    num_events_per_set : CNN_Res(\n",
    "        model_dir, \n",
    "        extra_description=f\"v2_{num_events_per_set}\"\n",
    "    )\n",
    "    for num_events_per_set in set_sizes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 4e-4\n",
    "epochs = 80\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "\n",
    "for num_events_per_set in set_sizes:\n",
    "\n",
    "    model = models[num_events_per_set]\n",
    "\n",
    "    loss_fn = nn.L1Loss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataset = train_datasets[num_events_per_set]\n",
    "    eval_dataset = eval_datasets[num_events_per_set]\n",
    "    train_dataset.load()\n",
    "    eval_dataset.load()\n",
    "\n",
    "    train_and_eval(\n",
    "        model, \n",
    "        train_dataset, \n",
    "        eval_dataset, \n",
    "        loss_fn, \n",
    "        optimizer, \n",
    "        epochs, \n",
    "        train_batch_size, \n",
    "        eval_batch_size, \n",
    "        device, \n",
    "        move_data=True,\n",
    "        scheduler=ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            factor=0.9, \n",
    "            patience=1\n",
    "        ),\n",
    "        checkpoint_epochs=5,\n",
    "    )\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "    plot_loss_curves(\n",
    "        model.loss_table,\n",
    "        ax,\n",
    "        start_epoch=0,\n",
    "        log_scale=True\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_events_per_set in set_sizes:\n",
    "    \n",
    "    model = models[num_events_per_set]\n",
    "    model.load_final()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    eval_dataset = eval_datasets[num_events_per_set]\n",
    "    eval_dataset.load()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        predictions = []\n",
    "        for set_features in eval_dataset.features:\n",
    "            prediction = model(set_features.unsqueeze(0).to(device))\n",
    "        predictions.append(prediction)\n",
    "        predictions = torch.tensor(predictions)\n",
    "\n",
    "        num_sets_per_label = get_num_per_unique_label(eval_dataset.labels)\n",
    "        # DANGER: Below assumes data sorted by labels!\n",
    "        avg_yhat_per_label = predictions.reshape(-1, num_sets_per_label).mean(dim=1).detach().cpu().numpy()\n",
    "        std_yhat_per_label = predictions.reshape(-1, num_sets_per_label).std(dim=1).detach().cpu().numpy()\n",
    "        unique_labels = torch.unique(eval_dataset.labels)\n",
    "\n",
    "        mse = torch.nn.functional.mse_loss(predictions, eval_dataset.labels)\n",
    "        mae = torch.nn.functional.l1_loss(predictions, eval_dataset.labels)\n",
    "\n",
    "    print(\"mse:\", mse)\n",
    "    print(\"mae:\", mae)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_prediction_linearity(\n",
    "        ax,\n",
    "        unique_labels,\n",
    "        avg_yhat_per_label,\n",
    "        std_yhat_per_label,\n",
    "    )\n",
    "    make_plot_note(\n",
    "        ax, \n",
    "        (\n",
    "            f\"Images ({num_image_bins} bins), {level}., \"\n",
    "            + f\"{num_sets_per_label} boots., \"\n",
    "            + f\"{num_events_per_set} events/boots.\"\n",
    "        ), \n",
    "    )\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'single_label_eval_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num_events_per_set \u001b[38;5;129;01min\u001b[39;00m set_sizes:\n\u001b[1;32m----> 3\u001b[0m     eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[43msingle_label_eval_datasets\u001b[49m[num_events_per_set]\n\u001b[0;32m      4\u001b[0m     eval_dataset\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m      6\u001b[0m     nickname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn_res_v2_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_events_per_set\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'single_label_eval_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "for num_events_per_set in set_sizes:\n",
    "    \n",
    "    eval_dataset = single_label_eval_datasets[num_events_per_set]\n",
    "    eval_dataset.load()\n",
    "\n",
    "    model = models[num_events_per_set]\n",
    "    model.load_final()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        for set_features in eval_dataset.features:\n",
    "            prediction = model(set_features.unsqueeze(0).to(device))\n",
    "            predictions.append(prediction)\n",
    "        predictions = torch.tensor(predictions)\n",
    "        mean = predictions.mean()\n",
    "        std = predictions.std()\n",
    "        bias = mean - new_physics_delta_c9_value\n",
    "\n",
    "    print(\"bias: \", bias.round(3))\n",
    "    print(\"std: \", std.round(3))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    bins = 50\n",
    "    xbounds = (-1.5, 0)\n",
    "    ybounds = (0, 200)\n",
    "    std_marker_height = 20\n",
    "\n",
    "    ax.hist(predictions, bins=bins, range=xbounds)\n",
    "    ax.vlines(\n",
    "        new_physics_delta_c9_value, \n",
    "        0, \n",
    "        ybounds[1], \n",
    "        color=\"red\", \n",
    "        label=f\"Target ({new_physics_delta_c9_value})\"\n",
    "    )\n",
    "    ax.vlines(\n",
    "        mean, \n",
    "        0, \n",
    "        ybounds[1], \n",
    "        color=\"red\", \n",
    "        linestyles=\"--\", \n",
    "        label=r\"$\\mu = $ \"+f\"{mean.round(3)}\"\n",
    "    )\n",
    "    ax.hlines(\n",
    "        std_marker_height, \n",
    "        mean, \n",
    "        mean+std, \n",
    "        color=\"orange\", \n",
    "        linestyles=\"dashdot\", \n",
    "        label=r\"$\\sigma = $ \"+f\"{std.round(3)}\"\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel(r\"Predicted $\\delta C_9$\")\n",
    "    \n",
    "    ax.set_xbound(*xbounds)\n",
    "    ax.set_ybound(*ybounds)\n",
    "    \n",
    "    ax.legend()\n",
    "    make_plot_note(\n",
    "        ax, \n",
    "        (\n",
    "            f\"Images ({num_image_bins} bins), {level}.,\" \n",
    "            + f\"{num_sets_per_label} boots.,\" \n",
    "            + f\"{num_events_per_set} events/boots.\"\n",
    "        ), \n",
    "        fontsize=\"medium\"\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"gen\"\n",
    "save_dir = \"../../state/new_physics/data/processed\"\n",
    "raw_signal_dir = \"../../state/new_physics/data/raw/signal\"\n",
    "std_scale = True\n",
    "q_squared_veto = True\n",
    "balanced_classes = True\n",
    "num_sets_per_label = 50\n",
    "\n",
    "for num_events_per_set in [70_000, 24_000, 6_000]:\n",
    "\n",
    "    name = f\"unbinned_sets_{num_events_per_set}\"\n",
    "\n",
    "    datasets = {\n",
    "        \"train\": Bootstrapped_Unbinned_Signal_Dataset(name=name, level=level, split=\"train\", save_dir=save_dir),\n",
    "        \"eval\": Bootstrapped_Unbinned_Signal_Dataset(name=name, level=level, split=\"eval\", save_dir=save_dir),\n",
    "    }\n",
    "\n",
    "    datasets[\"train\"].generate(\n",
    "        raw_trials=range(1,21), \n",
    "        raw_signal_dir=raw_signal_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=num_sets_per_label,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "    )\n",
    "    datasets[\"eval\"].generate(\n",
    "        raw_trials=range(21,41), \n",
    "        raw_signal_dir=raw_signal_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=num_sets_per_label,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 4e-4\n",
    "epochs = 80\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "\n",
    "for num_events_per_set in [70_000, 24_000, 6_000]:\n",
    "\n",
    "    nickname = f\"deep_sets_{num_events_per_set}\"\n",
    "    print(\"Training: \", nickname)\n",
    "    model = Deep_Sets(nickname, \"../../state/new_physics/models\")\n",
    "\n",
    "    loss_fn = nn.L1Loss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_dataset = Bootstrapped_Unbinned_Signal_Dataset(\n",
    "        name=f\"unbinned_sets_{num_events_per_set}\", \n",
    "        level=\"gen\", split=\"train\", \n",
    "        save_dir=\"../../state/new_physics/data/processed\"\n",
    "    )\n",
    "    eval_dataset = Bootstrapped_Unbinned_Signal_Dataset(\n",
    "        name=f\"unbinned_sets_{num_events_per_set}\", \n",
    "        level=\"gen\", split=\"eval\", \n",
    "        save_dir=\"../../state/new_physics/data/processed\"\n",
    "    )\n",
    "    train_dataset.load()\n",
    "    eval_dataset.load()\n",
    "    train_dataset.to(device)\n",
    "    eval_dataset.to(device)\n",
    "\n",
    "    train_and_eval(\n",
    "        model, \n",
    "        train_dataset, eval_dataset, \n",
    "        loss_fn, optimizer, \n",
    "        epochs, \n",
    "        train_batch_size, eval_batch_size, \n",
    "        device, \n",
    "        move_data=False,\n",
    "        scheduler=ReduceLROnPlateau(optimizer, factor=0.9, patience=1),\n",
    "        checkpoint_epochs=5,\n",
    "    )\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "    plot_epoch_start = 0\n",
    "    plot_loss_curves(\n",
    "        model.loss_table[\"epoch\"][plot_epoch_start:], \n",
    "        model.loss_table[\"train_loss\"][plot_epoch_start:], \n",
    "        model.loss_table[\"eval_loss\"][plot_epoch_start:], \n",
    "        ax\n",
    "    )\n",
    "    ax.set_yscale(\"log\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sets_per_label = 50\n",
    "\n",
    "for num_events_per_set in [70_000, 24_000, 6_000]:\n",
    "    \n",
    "    nickname = f\"deep_sets_{num_events_per_set}\"\n",
    "    model = Deep_Sets(nickname, \"../../state/new_physics/models\")\n",
    "    model.load_final()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    eval_dataset = Bootstrapped_Unbinned_Signal_Dataset(\n",
    "        name=f\"unbinned_sets_{num_events_per_set}\", \n",
    "        level=\"gen\", split=\"eval\", \n",
    "        save_dir=\"../../state/new_physics/data/processed\"\n",
    "    )\n",
    "    eval_dataset.load()\n",
    "    eval_dataset.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        predictions = []\n",
    "        for set_features in eval_dataset.features:\n",
    "            set_features = set_features.unsqueeze(0)\n",
    "            prediction = model(set_features)\n",
    "            predictions.append(prediction)\n",
    "        predictions = torch.tensor(predictions)\n",
    "\n",
    "        avgs = predictions.reshape(-1, num_sets_per_label).mean(1).detach().cpu().numpy()\n",
    "        stds = predictions.reshape(-1, num_sets_per_label).std(1).detach().cpu().numpy()\n",
    "\n",
    "        ys = eval_dataset.labels\n",
    "        unique_y = ys.reshape(-1, num_sets_per_label).mean(1).detach().cpu().numpy()\n",
    "\n",
    "        def calc_mse():\n",
    "            dif = predictions.detach().cpu().numpy() - ys.detach().cpu().numpy()\n",
    "            dif_sq = dif**2\n",
    "            out = dif_sq.mean()\n",
    "            return out\n",
    "        mse = calc_mse()\n",
    "\n",
    "        def calc_mae():\n",
    "            dif = predictions.detach().cpu().numpy() - ys.detach().cpu().numpy()\n",
    "            abs_dif = np.abs(dif)\n",
    "            out = abs_dif.mean()\n",
    "            return out\n",
    "        mae = calc_mae()\n",
    "        \n",
    "    print(\"mse:\", mse)\n",
    "    print(\"mae:\", mae)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    plot_prediction_linearity(\n",
    "        ax,\n",
    "        unique_y,\n",
    "        avgs,\n",
    "        stds,\n",
    "        ref_line_buffer=0.05,\n",
    "        xlim=(-2.25, 1.35),\n",
    "        ylim=(-2.25, 1.35),\n",
    "        xlabel=r\"Actual $\\delta C_9$\", \n",
    "        ylabel=r\"Predicted $\\delta C_9$\"\n",
    "    )\n",
    "    make_plot_note(\n",
    "        ax, \n",
    "        (\n",
    "            \"Deep Sets, Gen., \"\n",
    "            + f\"{num_sets_per_label} boots., \"\n",
    "            + f\"{num_events_per_set} events/boots.\"\n",
    "        ), \n",
    "        fontsize=\"medium\"\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate single event dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_signal_dir = \"../../state/new_physics/data/raw/signal\"\n",
    "std_scale = True\n",
    "q_squared_veto = True\n",
    "balanced_classes = True\n",
    "num_sets_per_label = 2_000\n",
    "\n",
    "for num_events_per_set in [70_000, 24_000, 6_000]:\n",
    "    single_label_eval_dataset = Bootstrapped_Unbinned_Signal_Dataset(\n",
    "            name=f\"unbinned_sets_{num_events_per_set}_at_-0.82\", \n",
    "            level=\"gen\", split=\"eval\", \n",
    "            save_dir=\"../../state/new_physics/data/processed\"\n",
    "    )\n",
    "    single_label_eval_dataset.generate(\n",
    "        raw_trials=range(21,41), \n",
    "        raw_signal_dir=raw_signal_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=num_sets_per_label,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "        labels_to_sample=[-0.82],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run sensitivity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sets_per_label = 2000\n",
    "\n",
    "for num_events_per_set in [70_000, 24_000, 6_000]:\n",
    "    single_label_eval_dataset = Bootstrapped_Unbinned_Signal_Dataset(\n",
    "            name=f\"unbinned_sets_{num_events_per_set}_at_-0.82\", \n",
    "            level=\"gen\", split=\"eval\", \n",
    "            save_dir=\"../../state/new_physics/data/processed\"\n",
    "    )\n",
    "    single_label_eval_dataset.load()\n",
    "    single_label_eval_dataset.to(device)\n",
    "\n",
    "    nickname = f\"deep_sets_{num_events_per_set}\"\n",
    "    model = Deep_Sets(nickname, \"../../state/new_physics/models\")\n",
    "    model.load_final()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        for set_features in single_label_eval_dataset.features:\n",
    "            prediction = model(set_features.unsqueeze(0))\n",
    "            predictions.append(prediction.detach().cpu())\n",
    "        predictions = np.array(predictions)\n",
    "        mean = predictions.mean()\n",
    "        std = predictions.std()\n",
    "        bias = mean - -0.82\n",
    "\n",
    "    print(\"bias: \", bias.round(3))\n",
    "    print(\"std: \", std.round(3))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    xbounds = (-1.5, 0)\n",
    "    ybounds = (0, 200)\n",
    "\n",
    "    ax.hist(predictions, bins=50, range=xbounds)\n",
    "    ax.vlines(-0.82, 0, ybounds[1], color=\"red\", label=\"Target (-0.82)\")\n",
    "    ax.vlines(mean, 0, ybounds[1], color=\"red\", linestyles=\"--\", label=r\"$\\mu = $ \"+f\"{mean.round(3)}\")\n",
    "    ax.hlines(20, mean, mean+std, color=\"orange\", linestyles=\"dashdot\", label=r\"$\\sigma = $ \"+f\"{std.round(3)}\")\n",
    "    ax.set_xlabel(r\"Predicted $\\delta C_9$\")\n",
    "    make_plot_note(ax, f\"Deep Sets, Gen., {num_sets_per_label} boots., {num_events_per_set} events/boots.\", fontsize=\"medium\")\n",
    "    ax.set_xbound(*xbounds)\n",
    "    ax.set_ybound(*ybounds)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event-by-event Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"gen\"\n",
    "save_dir = \"../../state/new_physics/data/processed\"\n",
    "raw_signal_dir = \"../../state/new_physics/data/raw/signal\"\n",
    "\n",
    "q_squared_veto = True\n",
    "std_scale = True\n",
    "balanced_classes = True\n",
    "\n",
    "name = \"binned_signal\"\n",
    "\n",
    "datasets = {\n",
    "    \"train\": Binned_Signal_Dataset(name, level=level, split=\"train\", save_dir=save_dir),\n",
    "    \"eval\": Binned_Signal_Dataset(name, level=level, split=\"eval\", save_dir=save_dir),\n",
    "}\n",
    "\n",
    "datasets[\"train\"].generate(\n",
    "    raw_trials=range(1,21), \n",
    "    raw_signal_dir=raw_signal_dir, \n",
    "    q_squared_veto=q_squared_veto,\n",
    "    std_scale=std_scale, \n",
    "    balanced_classes=balanced_classes\n",
    ")\n",
    "datasets[\"eval\"].generate(\n",
    "    raw_trials=range(21,41), \n",
    "    raw_signal_dir=raw_signal_dir, \n",
    "    q_squared_veto=q_squared_veto,\n",
    "    std_scale=std_scale, \n",
    "    balanced_classes=balanced_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Event_By_Event_NN(\"event_by_event_nn\", \"../../state/new_physics/models\")\n",
    "\n",
    "learning_rate = 3e-3\n",
    "epochs = 200\n",
    "train_batch_size = 10_000\n",
    "eval_batch_size = 10_000\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "dataset_name = \"binned_signal\"\n",
    "training_dataset = Binned_Signal_Dataset(dataset_name, level=\"gen\", split=\"train\", save_dir=save_dir)\n",
    "eval_dataset = Binned_Signal_Dataset(dataset_name, level=\"gen\", split=\"eval\", save_dir=save_dir)\n",
    "training_dataset.load()\n",
    "eval_dataset.load()\n",
    "\n",
    "loss_table = train_and_eval(\n",
    "    model, \n",
    "    training_dataset, eval_dataset, \n",
    "    loss_fn, optimizer, \n",
    "    epochs, \n",
    "    train_batch_size, eval_batch_size, \n",
    "    device, \n",
    "    move_data=True,\n",
    "    scheduler= ReduceLROnPlateau(optimizer, factor=0.95, threshold=0, patience=0, eps=1e-9),\n",
    "    checkpoint_epochs=5,\n",
    ")\n",
    "_, ax = plt.subplots()\n",
    "plot_epoch_start = 0\n",
    "plot_loss_curves(loss_table[\"epoch\"][plot_epoch_start:], loss_table[\"train_loss\"][plot_epoch_start:], loss_table[\"eval_loss\"][plot_epoch_start:], ax)\n",
    "ax.set_yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_mse():\n",
    "#     dif = expected_values_all_trials - datasets[\"eval\"].bin_values\n",
    "#     dif_sq = dif**2\n",
    "#     out = dif_sq.mean()\n",
    "#     return out\n",
    "\n",
    "# def calc_mae():\n",
    "#     dif = expected_values_all_trials - datasets[\"eval\"].bin_values\n",
    "#     abs_dif = np.abs(dif)\n",
    "#     out = abs_dif.mean()\n",
    "#     return out\n",
    "\n",
    "dataset_name = \"binned_signal\"\n",
    "eval_dataset = Binned_Signal_Dataset(dataset_name, level=\"gen\", split=\"eval\", save_dir=\"../../state/new_physics/data/processed\")\n",
    "eval_dataset.load()\n",
    "\n",
    "model = Event_By_Event_NN(\"event_by_event_nn\", \"../../state/new_physics/models\")\n",
    "model.load_final()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "num_sets_per_label = 50\n",
    "\n",
    "for num_events_per_set in [70_000, 24_000, 6_000]:\n",
    "\n",
    "    boot_x, boot_y_bin_indices = bootstrap_labeled_sets(\n",
    "        eval_dataset.features, \n",
    "        eval_dataset.labels, \n",
    "        num_events_per_set, num_sets_per_label,\n",
    "        reduce_labels=True,\n",
    "    )\n",
    "    boot_y = eval_dataset.bin_values[boot_y_bin_indices]\n",
    "\n",
    "    predictions = []\n",
    "    for set_features in boot_x:\n",
    "        set_features = set_features.to(device)\n",
    "        bin_values = eval_dataset.bin_values.to(device)\n",
    "        expected_value = model.calculate_expected_value(set_features, bin_values)\n",
    "        predictions.append(expected_value)\n",
    "    predictions = torch.tensor(predictions)\n",
    "    assert predictions.shape == boot_y.shape\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mse = torch.mean((predictions - boot_y)**2)\n",
    "        mae = torch.mean(torch.abs(predictions - boot_y))\n",
    "        yhat_avgs = predictions.reshape(-1, num_sets_per_label).mean(1)\n",
    "        yhat_stds = predictions.reshape(-1, num_sets_per_label).std(1)\n",
    "    print(\"mse:\", mse)\n",
    "    print(\"mae:\", mae)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_prediction_linearity(\n",
    "        ax,\n",
    "        eval_dataset.bin_values.numpy(),\n",
    "        yhat_avgs,\n",
    "        yhat_stds,\n",
    "        ref_line_buffer=0.05,\n",
    "        xlim=(-2.25, 1.35),\n",
    "        ylim=(-2.25, 1.35),\n",
    "        xlabel=r\"Actual $\\delta C_9$\", \n",
    "        ylabel=r\"Predicted $\\delta C_9$\"\n",
    "    )\n",
    "    make_plot_note(ax, f\"Event-by-event, Gen., {num_sets_per_label} boots., {num_events_per_set} events/boots.\", fontsize=\"medium\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_events_per_set = 6_000\n",
    "n_sets = 2000\n",
    "\n",
    "target_bin_index = np.argwhere(datasets[\"eval\"].bin_values==-0.82).item()\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "pool_x = datasets[\"eval\"].features[datasets[\"eval\"].labels==target_bin_index]\n",
    "pool_y = datasets[\"eval\"].labels[datasets[\"eval\"].labels==target_bin_index]\n",
    "\n",
    "predictions = []\n",
    "for _ in range(n_sets):\n",
    "    selection_indices = rng.choice(len(pool_x), n_events_per_set)\n",
    "    boot_x = pool_x[selection_indices]\n",
    "    boot_y = pool_y[selection_indices]\n",
    "\n",
    "    log_probs = predict_log_probabilities(boot_x, model)\n",
    "    bin_values = datasets[\"eval\"].bin_values + 5\n",
    "    log_bin_values = np.log(bin_values)\n",
    "    lse = logsumexp(log_bin_values + log_probs.cpu().numpy())\n",
    "    prediction = np.exp(lse) - 5\n",
    "    predictions.append(prediction)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "mean= predictions.mean()\n",
    "std = predictions.std()\n",
    "\n",
    "setup_high_quality_mpl_params()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "xbounds = (-1.5, 0)\n",
    "ybounds = (0, 200)\n",
    "\n",
    "ax.hist(predictions, bins=50, range=xbounds)\n",
    "ax.vlines(-0.82, 0, ybounds[1], color=\"red\", label=\"Target (-0.82)\")\n",
    "ax.vlines(mean, 0, ybounds[1], color=\"red\", linestyles=\"--\", label=r\"$\\mu = $ \"+f\"{mean.round(3)}\")\n",
    "ax.hlines(20, mean, mean+std, color=\"orange\", linestyles=\"dashdot\", label=r\"$\\sigma = $ \"+f\"{std.round(3)}\")\n",
    "ax.set_xlabel(r\"Predicted $\\delta C_9$\")\n",
    "make_plot_note(ax, f\"Event-by-event, Gen., {n_sets} boots., {n_events_per_set} events/boots.\", fontsize=\"medium\")\n",
    "ax.set_xbound(*xbounds)\n",
    "ax.set_ybound(*ybounds)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detector Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shawn's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event-by-event Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detector Level with Backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shawn's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event-by-event Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_log_probabilities_by_label(x, y, model):\n",
    "    \"\"\"\n",
    "    Predict the log probability of each class for each subset of same labeled events.\n",
    "    \n",
    "    x : A torch tensor of features of events (from multiple labels).\n",
    "    y : A torch tensor of event labels.\n",
    "    \"\"\"\n",
    "    labels = np.unique(y.cpu())\n",
    "    log_probabilities = []\n",
    "    for label in labels:\n",
    "        features_label = x[y==label]\n",
    "        log_probabilities_label = predict_log_probabilities(features_label, model).cpu().numpy()\n",
    "        log_probabilities.append(np.expand_dims(log_probabilities_label, axis=0))\n",
    "    log_probabilities = np.concatenate(log_probabilities, axis=0)\n",
    "    assert log_probabilities.shape[0] == len(labels)\n",
    "    return log_probabilities\n",
    "\n",
    "\n",
    "def calculate_predicted_expected_value_by_label(predictions, bin_values):\n",
    "    \"\"\"\n",
    "    Calculate the predicted expected binned value for each subset of same labeled events, given \n",
    "    the predicted probability distribution for each subset.\n",
    "\n",
    "    predictions : numpy array of predicted probability distributions for each label.\n",
    "    bin_values : numpy array of the value corresponding to each bin.\n",
    "    \"\"\"\n",
    "    bin_value_shift = np.abs(np.min(bin_values)) + 1\n",
    "    shifted_bin_values = bin_values + bin_value_shift\n",
    "    log_shifted_bin_values = np.tile(np.log(shifted_bin_values), (len(shifted_bin_values), 1))\n",
    "    log_shifted_expected_values = scipy.special.logsumexp(predictions + log_shifted_bin_values, axis=1)\n",
    "    shifted_expected_values = np.exp(log_shifted_expected_values)\n",
    "    expected_values = shifted_expected_values - bin_value_shift\n",
    "    return expected_values\n",
    "\n",
    "\n",
    "def plot_log_probabilities_over_labels(fig, ax, predictions, bin_values, cmap=plt.cm.viridis):\n",
    "    \"\"\"\n",
    "    Plot the predicted log probability of each class for each subset of same labeled events.\n",
    "\n",
    "    predictions : A numpy array of set probabilities (rows correspond to labels, columns correspond to class predictions).\n",
    "    bin_values : A numpy array of the value each bin represents. \n",
    "    \"\"\"\n",
    "\n",
    "    color_bounds = np.append(bin_values, bin_values[-1] + (bin_values[-1] - bin_values[-2]))\n",
    "    color_norm = mpl.colors.BoundaryNorm(color_bounds, cmap.N)\n",
    "\n",
    "    for value, curve in zip(bin_values, predictions):\n",
    "        ax.plot(bin_values, curve, color=cmap(color_norm(value)))\n",
    "\n",
    "    fig.colorbar(mpl.cm.ScalarMappable(norm=color_norm, cmap=cmap), ax=ax, label=r\"Actual $\\delta C_9$\")\n",
    "    ax.set_xlabel(r\"$\\delta C_9$\")\n",
    "    ax.set_ylabel(r\"$\\log p(\\delta C_9 | x_1, ..., x_N)$\")\n",
    "\n",
    "\n",
    "def plot_expected_value_over_labels(ax, expected_values, bin_values):\n",
    "    \"\"\"\n",
    "    Plot the predicted expected value for each label.\n",
    "    \"\"\"\n",
    "    ax.scatter(bin_values, expected_values, label=\"Prediction\", color=\"firebrick\", s=16, zorder=5)\n",
    "    ax.plot(\n",
    "        bin_values, bin_values,\n",
    "        label=\"Ref. Line (Slope = 1)\",\n",
    "        color=\"grey\",\n",
    "        linewidth=0.5,\n",
    "        zorder=0\n",
    "    )\n",
    "    ax.set_xlabel(r\"Actual $\\delta C_9$\")\n",
    "    ax.set_ylabel(r\"Predicted $\\delta C_9$\")\n",
    "    ax.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maybe_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
