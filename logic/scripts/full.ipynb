{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from library.datasets import (\n",
    "    Signal_Images_Dataset, \n",
    "    Signal_Sets_Dataset, \n",
    "    Binned_Signal_Dataset\n",
    ")\n",
    "from library.models import (\n",
    "    CNN_Res, \n",
    "    Deep_Sets, \n",
    "    Event_By_Event_NN\n",
    ")\n",
    "from library.nn_training import (\n",
    "    select_device, \n",
    "    train_and_eval\n",
    ")\n",
    "from library.predict import (\n",
    "    Summary_Table,\n",
    "    make_predictions,\n",
    "    run_linearity_test,\n",
    "    run_sensitivity_test,\n",
    "    calculate_mse_mae\n",
    ")\n",
    "from library.plotting import (\n",
    "    plot_loss_curves, \n",
    "    setup_high_quality_mpl_params, \n",
    "    plot_prediction_linearity, \n",
    "    plot_sensitivity,\n",
    "    plot_volume_slices\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select device (cuda if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = select_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup fancy plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_high_quality_mpl_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_save_dir = \"../../state/new_physics/data/processed\"\n",
    "model_dir = \"../../state/new_physics/models\"\n",
    "\n",
    "std_scale = True\n",
    "q_squared_veto = True\n",
    "balanced_classes = True\n",
    "\n",
    "set_sizes = [70_000, 24_000, 6_000]\n",
    "\n",
    "num_sets_per_label = 50\n",
    "num_sets_per_label_single = 2000\n",
    "\n",
    "num_image_bins = 10\n",
    "\n",
    "new_physics_delta_c9_value = -0.82\n",
    "\n",
    "summary_table = Summary_Table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"gen\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shawn's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate = False\n",
    "\n",
    "train_datasets = {\n",
    "    num_events_per_set : Signal_Images_Dataset(\n",
    "        level=level, \n",
    "        split=\"train\", \n",
    "        save_dir=dataset_save_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=num_sets_per_label,\n",
    "        n_bins=num_image_bins,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "        extra_description=num_events_per_set,\n",
    "        regenerate=regenerate,\n",
    "    ) \n",
    "    for num_events_per_set in set_sizes\n",
    "}\n",
    "\n",
    "eval_datasets = {\n",
    "    num_events_per_set : Signal_Images_Dataset(\n",
    "        level=level, \n",
    "        split=\"eval\", \n",
    "        save_dir=dataset_save_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=num_sets_per_label,\n",
    "        n_bins=num_image_bins,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "        extra_description=num_events_per_set,\n",
    "        regenerate=regenerate,\n",
    "    ) \n",
    "    for num_events_per_set in set_sizes\n",
    "}\n",
    "\n",
    "single_label_eval_datasets = {\n",
    "    num_events_per_set : Signal_Images_Dataset(\n",
    "        level=level, \n",
    "        split=\"eval\", \n",
    "        save_dir=dataset_save_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=num_sets_per_label_single,\n",
    "        n_bins=num_image_bins,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "        labels_to_sample=[new_physics_delta_c9_value],\n",
    "        extra_description=f\"{num_events_per_set}_single\",\n",
    "        regenerate=regenerate,\n",
    "    ) \n",
    "    for num_events_per_set in set_sizes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    num_events_per_set : CNN_Res(\n",
    "        model_dir, \n",
    "        extra_description=f\"v2_{num_events_per_set}\"\n",
    "    )\n",
    "    for num_events_per_set in set_sizes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peek at features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_events_per_set = 24_000\n",
    "\n",
    "dset = train_datasets[num_events_per_set]\n",
    "dset.load()\n",
    "\n",
    "plot_volume_slices(\n",
    "     dset.features[0], \n",
    "     n_slices=3, \n",
    "     note=r\"$\\delta C_9$ : \"+f\"{dset.labels[0]}\"\n",
    ")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 4e-4\n",
    "epochs = 80\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "\n",
    "for num_events_per_set in set_sizes:\n",
    "\n",
    "    model = models[\n",
    "        num_events_per_set\n",
    "    ]\n",
    "\n",
    "    loss_fn = nn.L1Loss()\n",
    "    optimizer = Adam(\n",
    "        model.parameters(), \n",
    "        lr=learning_rate\n",
    "    )\n",
    "\n",
    "    train_dataset = train_datasets[\n",
    "        num_events_per_set\n",
    "    ]\n",
    "    eval_dataset = eval_datasets[\n",
    "        num_events_per_set\n",
    "    ]\n",
    "    train_dataset.load()\n",
    "    eval_dataset.load()\n",
    "\n",
    "    train_and_eval(\n",
    "        model, \n",
    "        train_dataset, \n",
    "        eval_dataset, \n",
    "        loss_fn, \n",
    "        optimizer, \n",
    "        epochs, \n",
    "        train_batch_size, \n",
    "        eval_batch_size, \n",
    "        device, \n",
    "        move_data=True,\n",
    "        scheduler=ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            factor=0.9, \n",
    "            patience=1\n",
    "        ),\n",
    "        checkpoint_epochs=5,\n",
    "    )\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "    plot_loss_curves(\n",
    "        model.loss_table,\n",
    "        ax,\n",
    "        start_epoch=0,\n",
    "        log_scale=True,\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_events_per_set in set_sizes:\n",
    "    \n",
    "    model = models[\n",
    "        num_events_per_set\n",
    "    ]\n",
    "    model.load_final()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    eval_dataset = eval_datasets[\n",
    "        num_events_per_set\n",
    "    ]\n",
    "    eval_dataset.load()\n",
    "\n",
    "    predictions = make_predictions(\n",
    "        model, \n",
    "        eval_dataset.features,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    mse, mae = calculate_mse_mae(\n",
    "        predictions, \n",
    "        eval_dataset.labels,\n",
    "    )\n",
    "    summary_table.add_item(\n",
    "        \"Images\", \n",
    "        \"MSE\", \n",
    "        num_events_per_set, \n",
    "        mse,\n",
    "    )\n",
    "    summary_table.add_item(\n",
    "        \"Images\", \n",
    "        \"MAE\", \n",
    "        num_events_per_set, \n",
    "        mae,\n",
    "    )\n",
    "\n",
    "    (\n",
    "        unique_labels, \n",
    "        avgs, \n",
    "        stds,\n",
    "    ) = run_linearity_test(\n",
    "        predictions, \n",
    "        eval_dataset.labels\n",
    "    )\n",
    "\n",
    "    eval_dataset.unload()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    plot_prediction_linearity(\n",
    "        ax,\n",
    "        unique_labels.detach().cpu().numpy(),\n",
    "        avgs.detach().cpu().numpy(),\n",
    "        stds.detach().cpu().numpy(),\n",
    "        note=(\n",
    "            f\"Images ({num_image_bins} bins), {level}., \"\n",
    "            + f\"{num_sets_per_label} boots., \"\n",
    "            + f\"{num_events_per_set} events/boots.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_events_per_set in set_sizes:\n",
    "    \n",
    "    eval_dataset = single_label_eval_datasets[\n",
    "        num_events_per_set\n",
    "    ]\n",
    "    eval_dataset.load()\n",
    "\n",
    "    model = models[\n",
    "        num_events_per_set\n",
    "    ]\n",
    "    model.load_final()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predictions = make_predictions(\n",
    "        model, \n",
    "        eval_dataset.features, \n",
    "        device, \n",
    "    )\n",
    "\n",
    "    mean, std, bias = run_sensitivity_test(\n",
    "        predictions, \n",
    "        new_physics_delta_c9_value\n",
    "    )\n",
    "    summary_table.add_item(\n",
    "        \"Images\", \n",
    "        \"Mean at NP\", \n",
    "        num_events_per_set, \n",
    "        mean\n",
    "    )\n",
    "    summary_table.add_item(\n",
    "        \"Images\", \n",
    "        \"Std. at NP\", \n",
    "        num_events_per_set, \n",
    "        std\n",
    "    )\n",
    "    summary_table.add_item(\n",
    "        \"Images\", \n",
    "        \"Bias at NP\", \n",
    "        num_events_per_set, \n",
    "        bias\n",
    "    )\n",
    "\n",
    "    eval_dataset.unload()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    plot_sensitivity(\n",
    "        ax,\n",
    "        predictions,\n",
    "        new_physics_delta_c9_value,\n",
    "        note=(\n",
    "            f\"Images ({num_image_bins} bins), {level}., \" \n",
    "            + f\"{num_sets_per_label_single} boots., \" \n",
    "            + f\"{num_events_per_set} events/boots.\"\n",
    "        ), \n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    summary_table[[\"MSE\", \"MAE\"]]\n",
    "    .to_latex(float_format=\"%.3f\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate = False\n",
    "\n",
    "train_datasets = {\n",
    "    num_events_per_set : Signal_Sets_Dataset(\n",
    "        level=level,\n",
    "        split=\"train\",\n",
    "        save_dir=dataset_save_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=num_sets_per_label,\n",
    "        binned=False,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "        extra_description=f\"unbinned_{num_events_per_set}\",\n",
    "        regenerate=regenerate\n",
    "    )\n",
    "    for num_events_per_set in set_sizes\n",
    "}\n",
    "\n",
    "eval_datasets = {\n",
    "    num_events_per_set : Signal_Sets_Dataset(\n",
    "        level=level,\n",
    "        split=\"eval\",\n",
    "        save_dir=dataset_save_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=num_sets_per_label,\n",
    "        binned=False,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "        extra_description=f\"unbinned_{num_events_per_set}\",\n",
    "        regenerate=regenerate\n",
    "    )\n",
    "    for num_events_per_set in set_sizes\n",
    "}\n",
    "\n",
    "single_label_eval_datasets = {\n",
    "    num_events_per_set : Signal_Sets_Dataset(\n",
    "        level=level,\n",
    "        split=\"eval\",\n",
    "        save_dir=dataset_save_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=num_sets_per_label_single,\n",
    "        binned=False,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "        labels_to_sample=[new_physics_delta_c9_value],\n",
    "        extra_description=f\"unbinned_{num_events_per_set}_single\",\n",
    "        regenerate=regenerate\n",
    "    )\n",
    "    for num_events_per_set in set_sizes\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    num_events_per_set : Deep_Sets(\n",
    "        model_dir, \n",
    "        extra_description=num_events_per_set\n",
    "    )\n",
    "    for num_events_per_set in set_sizes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 4e-4\n",
    "epochs = 80\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "\n",
    "for num_events_per_set in set_sizes:\n",
    "\n",
    "    model = models[\n",
    "        num_events_per_set\n",
    "    ]\n",
    "\n",
    "    loss_fn = nn.MSELoss() # trained with L1 loss\n",
    "    optimizer = Adam(\n",
    "        model.parameters(), \n",
    "        lr=learning_rate,\n",
    "    )\n",
    "\n",
    "    train_dataset = train_datasets[\n",
    "        num_events_per_set\n",
    "    ]\n",
    "    eval_dataset = eval_datasets[\n",
    "        num_events_per_set\n",
    "    ]\n",
    "    train_dataset.load()\n",
    "    eval_dataset.load()\n",
    "\n",
    "    train_and_eval(\n",
    "        model, \n",
    "        train_dataset, \n",
    "        eval_dataset, \n",
    "        loss_fn, \n",
    "        optimizer, \n",
    "        epochs, \n",
    "        train_batch_size, \n",
    "        eval_batch_size, \n",
    "        device, \n",
    "        move_data=True,\n",
    "        scheduler=ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            factor=0.9, \n",
    "            patience=1\n",
    "        ),\n",
    "        checkpoint_epochs=5,\n",
    "    )\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "    plot_loss_curves(\n",
    "        model.loss_table,\n",
    "        ax,\n",
    "        start_epoch=0,\n",
    "        log_scale=True,\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_events_per_set in set_sizes:\n",
    "\n",
    "    model = models[\n",
    "        num_events_per_set\n",
    "    ]\n",
    "    model.load_final()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    eval_dataset = eval_datasets[\n",
    "        num_events_per_set\n",
    "    ]\n",
    "\n",
    "    eval_dataset.load()\n",
    "\n",
    "    predictions = make_predictions(\n",
    "        model, \n",
    "        eval_dataset.features, \n",
    "        device,\n",
    "    )\n",
    "\n",
    "    (\n",
    "        unique_labels, \n",
    "        avgs, \n",
    "        stds,\n",
    "    ) = run_linearity_test(\n",
    "        predictions, \n",
    "        eval_dataset.labels\n",
    "    )\n",
    "\n",
    "    (\n",
    "        mse, \n",
    "        mae\n",
    "    ) = calculate_mse_mae(\n",
    "        predictions, \n",
    "        eval_dataset.labels\n",
    "    )\n",
    "\n",
    "    summary_table.add_item(\"Deep Sets\", \"MSE\", num_events_per_set, mse)\n",
    "    summary_table.add_item(\"Deep Sets\", \"MAE\", num_events_per_set, mae)\n",
    "\n",
    "    eval_dataset.unload()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    plot_prediction_linearity(\n",
    "        ax,\n",
    "        unique_labels.detach().cpu().numpy(),\n",
    "        avgs.detach().cpu().numpy(),\n",
    "        stds.detach().cpu().numpy(),\n",
    "        note=(\n",
    "            f\"Deep Sets, {level}., \"\n",
    "            + f\"{num_sets_per_label} boots., \"\n",
    "            + f\"{num_events_per_set} events/boots.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_events_per_set in set_sizes:\n",
    "    \n",
    "    eval_dataset = single_label_eval_datasets[\n",
    "        num_events_per_set\n",
    "    ]\n",
    "    eval_dataset.load()\n",
    "\n",
    "    model = models[\n",
    "        num_events_per_set\n",
    "    ]\n",
    "    model.load_final()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predictions = make_predictions(\n",
    "        model, \n",
    "        eval_dataset.features, \n",
    "        device,\n",
    "    )\n",
    "\n",
    "    (\n",
    "        mean, \n",
    "        std, \n",
    "        bias,    \n",
    "    ) = run_sensitivity_test(\n",
    "        predictions, \n",
    "        new_physics_delta_c9_value\n",
    "    )\n",
    "\n",
    "    summary_table.add_item(\"Deep Sets\", \"Mean at NP\", num_events_per_set, mean)\n",
    "    summary_table.add_item(\"Deep Sets\", \"Std. at NP\", num_events_per_set, std)\n",
    "    summary_table.add_item(\"Deep Sets\", \"Bias at NP\", num_events_per_set, bias)\n",
    "\n",
    "    eval_dataset.unload()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    plot_sensitivity(\n",
    "        ax,\n",
    "        predictions,\n",
    "        new_physics_delta_c9_value,\n",
    "        note=(\n",
    "            f\"Deep Sets, {level}., \" \n",
    "            + f\"{num_sets_per_label} boots., \" \n",
    "            + f\"{num_events_per_set} events/boots.\"\n",
    "        ), \n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    summary_table[[\"MSE\", \"MAE\"]]\n",
    "    .to_latex(float_format=\"%.3f\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event-by-event Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate = False\n",
    "\n",
    "train_events_dataset = Binned_Signal_Dataset(\n",
    "    level=level,\n",
    "    split=\"train\",\n",
    "    save_dir=dataset_save_dir,\n",
    "    q_squared_veto=q_squared_veto,\n",
    "    std_scale=std_scale,\n",
    "    balanced_classes=balanced_classes,\n",
    "    shuffle=True,\n",
    "    extra_description=None,\n",
    "    regenerate=regenerate\n",
    ")\n",
    "\n",
    "eval_events_dataset = Binned_Signal_Dataset(\n",
    "    level=level,\n",
    "    split=\"eval\",\n",
    "    save_dir=dataset_save_dir,\n",
    "    q_squared_veto=q_squared_veto,\n",
    "    std_scale=std_scale,\n",
    "    balanced_classes=balanced_classes,\n",
    "    shuffle=True,\n",
    "    extra_description=None,\n",
    "    regenerate=regenerate\n",
    ")\n",
    "\n",
    "eval_sets_datasets = {\n",
    "    num_events_per_set : Signal_Sets_Dataset(\n",
    "        level=level,\n",
    "        split=\"eval\",\n",
    "        save_dir=dataset_save_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=num_sets_per_label,\n",
    "        binned=True,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "        extra_description=f\"binned_{num_events_per_set}\",\n",
    "        regenerate=regenerate\n",
    "    )\n",
    "    for num_events_per_set in set_sizes\n",
    "}\n",
    "\n",
    "single_label_eval_sets_datasets = {\n",
    "    num_events_per_set : Signal_Sets_Dataset(\n",
    "        level=level,\n",
    "        split=\"eval\",\n",
    "        save_dir=dataset_save_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=num_sets_per_label_single,\n",
    "        binned=True,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "        labels_to_sample=[new_physics_delta_c9_value],\n",
    "        extra_description=f\"binned_{num_events_per_set}_single\",\n",
    "        regenerate=regenerate\n",
    "    )\n",
    "    for num_events_per_set in set_sizes\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Event_By_Event_NN(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-3\n",
    "epochs = 200\n",
    "\n",
    "train_batch_size = 10_000\n",
    "eval_batch_size = 10_000\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "train_dataset.load()\n",
    "eval_dataset.load()\n",
    "\n",
    "loss_table = train_and_eval(\n",
    "    model, \n",
    "    train_events_dataset, \n",
    "    eval_events_dataset, \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    epochs, \n",
    "    train_batch_size, \n",
    "    eval_batch_size, \n",
    "    device, \n",
    "    move_data=True,\n",
    "    scheduler=ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        factor=0.95, \n",
    "        threshold=0, \n",
    "        patience=0, \n",
    "        eps=1e-9\n",
    "    ),\n",
    "    checkpoint_epochs=5,\n",
    ")\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "plot_loss_curves(\n",
    "    model.loss_table,\n",
    "    ax,\n",
    "    start_epoch=0,\n",
    "    log_scale=True,\n",
    ")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_final()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "for num_events_per_set in set_sizes:\n",
    "\n",
    "    eval_sets_dataset = eval_sets_datasets[\n",
    "        num_events_per_set\n",
    "    ]\n",
    "    eval_sets_dataset.load()\n",
    "\n",
    "    predictions = make_predictions(\n",
    "        model, \n",
    "        eval_sets_dataset.features, \n",
    "        device, \n",
    "        event_by_event=True, \n",
    "        bin_values=eval_sets_dataset.bin_values\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        predictions.shape \n",
    "        == eval_sets_dataset.labels.shape\n",
    "    )\n",
    "\n",
    "    unbinned_labels = (\n",
    "        eval_sets_dataset\n",
    "        .bin_values[\n",
    "            eval_sets_dataset.labels.int()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    (\n",
    "        mse, \n",
    "        mae\n",
    "    ) = calculate_mse_mae(\n",
    "        predictions, \n",
    "        unbinned_labels\n",
    "    )\n",
    "\n",
    "    summary_table.add_item(\n",
    "        \"Event by event\", \n",
    "        \"MSE\", \n",
    "        num_events_per_set, \n",
    "        mse\n",
    "    )\n",
    "\n",
    "    summary_table.add_item(\n",
    "        \"Event by event\", \n",
    "        \"MAE\", \n",
    "        num_events_per_set, \n",
    "        mae\n",
    "    )\n",
    "        \n",
    "    (\n",
    "        unique_labels, \n",
    "        avgs, \n",
    "        stds\n",
    "    ) = run_linearity_test(\n",
    "        predictions, \n",
    "        eval_sets_dataset.labels\n",
    "    )\n",
    "\n",
    "    eval_sets_dataset.unload()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    plot_prediction_linearity(\n",
    "        ax,\n",
    "        unique_labels.detach().cpu().numpy(),\n",
    "        avgs.detach().cpu().numpy(),\n",
    "        stds.detach().cpu().numpy(),\n",
    "        note=(\n",
    "            f\"Deep Sets, {level}., \"\n",
    "            + f\"{num_sets_per_label} boots., \"\n",
    "            + f\"{num_events_per_set} events/boots.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_final()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "for num_events_per_set in set_sizes:\n",
    "    \n",
    "    eval_sets_dataset = single_label_eval_sets_datasets[\n",
    "        num_events_per_set\n",
    "    ]\n",
    "    eval_sets_dataset.load()\n",
    "\n",
    "    predictions = make_predictions(\n",
    "        model, \n",
    "        eval_sets_dataset.features, \n",
    "        device, \n",
    "        event_by_event=True, \n",
    "        bin_values=eval_sets_dataset\n",
    "            .bin_values,\n",
    "    )\n",
    "    \n",
    "    (\n",
    "        mean, \n",
    "        std, \n",
    "        bias,\n",
    "    ) = run_sensitivity_test(\n",
    "        predictions, \n",
    "        new_physics_delta_c9_value\n",
    "    )\n",
    "\n",
    "    summary_table.add_item(\"Event by event\", \"Mean at NP\", num_events_per_set, mean)\n",
    "    summary_table.add_item(\"Event by event\", \"Std. at NP\", num_events_per_set, std)\n",
    "    summary_table.add_item(\"Event by event\", \"Bias at NP\", num_events_per_set, bias)\n",
    "\n",
    "    eval_sets_dataset.unload()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    plot_sensitivity(\n",
    "        ax, \n",
    "        predictions, \n",
    "        new_physics_delta_c9_value, \n",
    "        note=(\n",
    "            f\"Deep Sets, {level}., \" \n",
    "            + f\"{num_sets_per_label} boots., \" \n",
    "            + f\"{num_events_per_set} events/boots.\"\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    summary_table[[\"MSE\", \"MAE\"]]\n",
    "    .to_latex(float_format=\"%.3f\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    summary_table[[\"MSE\", \"MAE\",]]\n",
    "    .to_latex(float_format=\"%.3f\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    summary_table[[\"Std. at NP\", \"Bias at NP\"]]\n",
    "    .to_latex(float_format=\"%.3f\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table.index.unique(\"Method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linestyles = [\"-D\", \"-o\", \"-s\"]\n",
    "\n",
    "y_lims = [(0, None), (0, None), (0, None), (None, None), (-0.15, 0.15)]\n",
    "\n",
    "for col, y_lim in zip(summary_table.columns, y_lims):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for method, style in zip(summary_table.index.unique(\"Method\"), linestyles):\n",
    "   \n",
    "        y = summary_table.loc[method, col]\n",
    "        x = y.index\n",
    "        \n",
    "        ax.plot(x, y, style, label=f\"{method}\")\n",
    "\n",
    "    ax.set_ylim(y_lim)\n",
    "    ax.set_title(f\"{col}\")\n",
    "    ax.set_xlabel(\"Number of events / set\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detector Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"det\"\n",
    "\n",
    "regenerate = False\n",
    "\n",
    "train_datasets = {\n",
    "    num_events_per_set : Signal_Images_Dataset(\n",
    "        level=level, \n",
    "        split=\"train\", \n",
    "        save_dir=dataset_save_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=num_sets_per_label,\n",
    "        n_bins=num_image_bins,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "        extra_description=num_events_per_set,\n",
    "        regenerate=regenerate,\n",
    "    ) \n",
    "    for num_events_per_set in set_sizes\n",
    "}\n",
    "\n",
    "eval_datasets = {\n",
    "    num_events_per_set : Signal_Images_Dataset(\n",
    "        level=level, \n",
    "        split=\"eval\", \n",
    "        save_dir=dataset_save_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=num_sets_per_label,\n",
    "        n_bins=num_image_bins,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "        extra_description=num_events_per_set,\n",
    "        regenerate=regenerate,\n",
    "    ) \n",
    "    for num_events_per_set in set_sizes\n",
    "}\n",
    "\n",
    "single_label_eval_datasets = {\n",
    "    num_events_per_set : Signal_Images_Dataset(\n",
    "        level=level, \n",
    "        split=\"eval\", \n",
    "        save_dir=dataset_save_dir,\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=num_sets_per_label_single,\n",
    "        n_bins=num_image_bins,\n",
    "        q_squared_veto=q_squared_veto,\n",
    "        std_scale=std_scale,\n",
    "        balanced_classes=balanced_classes,\n",
    "        labels_to_sample=[new_physics_delta_c9_value],\n",
    "        extra_description=f\"{num_events_per_set}_single\",\n",
    "        regenerate=regenerate,\n",
    "    ) \n",
    "    for num_events_per_set in set_sizes\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shawn's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event-by-event Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detector Level with Backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shawn's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event-by-event Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_log_probabilities_by_label(x, y, model):\n",
    "    \"\"\"\n",
    "    Predict the log probability of each class for each subset of same labeled events.\n",
    "    \n",
    "    x : A torch tensor of features of events (from multiple labels).\n",
    "    y : A torch tensor of event labels.\n",
    "    \"\"\"\n",
    "    labels = np.unique(y.cpu())\n",
    "    log_probabilities = []\n",
    "    for label in labels:\n",
    "        features_label = x[y==label]\n",
    "        log_probabilities_label = predict_log_probabilities(features_label, model).cpu().numpy()\n",
    "        log_probabilities.append(np.expand_dims(log_probabilities_label, axis=0))\n",
    "    log_probabilities = np.concatenate(log_probabilities, axis=0)\n",
    "    assert log_probabilities.shape[0] == len(labels)\n",
    "    return log_probabilities\n",
    "\n",
    "\n",
    "def calculate_predicted_expected_value_by_label(predictions, bin_values):\n",
    "    \"\"\"\n",
    "    Calculate the predicted expected binned value for each subset of same labeled events, given \n",
    "    the predicted probability distribution for each subset.\n",
    "\n",
    "    predictions : numpy array of predicted probability distributions for each label.\n",
    "    bin_values : numpy array of the value corresponding to each bin.\n",
    "    \"\"\"\n",
    "    bin_value_shift = np.abs(np.min(bin_values)) + 1\n",
    "    shifted_bin_values = bin_values + bin_value_shift\n",
    "    log_shifted_bin_values = np.tile(np.log(shifted_bin_values), (len(shifted_bin_values), 1))\n",
    "    log_shifted_expected_values = scipy.special.logsumexp(predictions + log_shifted_bin_values, axis=1)\n",
    "    shifted_expected_values = np.exp(log_shifted_expected_values)\n",
    "    expected_values = shifted_expected_values - bin_value_shift\n",
    "    return expected_values\n",
    "\n",
    "\n",
    "def plot_log_probabilities_over_labels(fig, ax, predictions, bin_values, cmap=plt.cm.viridis):\n",
    "    \"\"\"\n",
    "    Plot the predicted log probability of each class for each subset of same labeled events.\n",
    "\n",
    "    predictions : A numpy array of set probabilities (rows correspond to labels, columns correspond to class predictions).\n",
    "    bin_values : A numpy array of the value each bin represents. \n",
    "    \"\"\"\n",
    "\n",
    "    color_bounds = np.append(bin_values, bin_values[-1] + (bin_values[-1] - bin_values[-2]))\n",
    "    color_norm = mpl.colors.BoundaryNorm(color_bounds, cmap.N)\n",
    "\n",
    "    for value, curve in zip(bin_values, predictions):\n",
    "        ax.plot(bin_values, curve, color=cmap(color_norm(value)))\n",
    "\n",
    "    fig.colorbar(mpl.cm.ScalarMappable(norm=color_norm, cmap=cmap), ax=ax, label=r\"Actual $\\delta C_9$\")\n",
    "    ax.set_xlabel(r\"$\\delta C_9$\")\n",
    "    ax.set_ylabel(r\"$\\log p(\\delta C_9 | x_1, ..., x_N)$\")\n",
    "\n",
    "\n",
    "def plot_expected_value_over_labels(ax, expected_values, bin_values):\n",
    "    \"\"\"\n",
    "    Plot the predicted expected value for each label.\n",
    "    \"\"\"\n",
    "    ax.scatter(bin_values, expected_values, label=\"Prediction\", color=\"firebrick\", s=16, zorder=5)\n",
    "    ax.plot(\n",
    "        bin_values, bin_values,\n",
    "        label=\"Ref. Line (Slope = 1)\",\n",
    "        color=\"grey\",\n",
    "        linewidth=0.5,\n",
    "        zorder=0\n",
    "    )\n",
    "    ax.set_xlabel(r\"Actual $\\delta C_9$\")\n",
    "    ax.set_ylabel(r\"Predicted $\\delta C_9$\")\n",
    "    ax.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maybe_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
