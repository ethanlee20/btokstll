{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import scipy.special\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "from library.datasets import Signal_Images_Dataset, Signal_Images_Single_Dataset\n",
    "from library.nn_training import select_device, train_and_eval, Custom_Model\n",
    "from library.plotting import plot_loss_curves, setup_high_quality_mpl_params, plot_prediction_linearity, make_plot_note\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_log_probabilities(x, model):\n",
    "    \"\"\"\n",
    "    Predict the log probability of each class, given a set of events.\n",
    "\n",
    "    x : A torch tensor of features of events.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        event_logits = model(x)\n",
    "        event_log_probabilities = torch.nn.functional.log_softmax(event_logits, dim=1)\n",
    "        set_logits = torch.sum(event_log_probabilities, dim=0)\n",
    "        set_log_probabilities = torch.nn.functional.log_softmax(set_logits, dim=0)\n",
    "    return set_log_probabilities\n",
    "\n",
    "\n",
    "def predict_log_probabilities_by_label(x, y, model):\n",
    "    \"\"\"\n",
    "    Predict the log probability of each class for each subset of same labeled events.\n",
    "    \n",
    "    x : A torch tensor of features of events (from multiple labels).\n",
    "    y : A torch tensor of event labels.\n",
    "    \"\"\"\n",
    "    labels = np.unique(y.cpu())\n",
    "    log_probabilities = []\n",
    "    for label in labels:\n",
    "        features_label = x[y==label]\n",
    "        log_probabilities_label = predict_log_probabilities(features_label, model).cpu().numpy()\n",
    "        log_probabilities.append(np.expand_dims(log_probabilities_label, axis=0))\n",
    "    log_probabilities = np.concatenate(log_probabilities, axis=0)\n",
    "    assert log_probabilities.shape[0] == len(labels)\n",
    "    return log_probabilities\n",
    "\n",
    "\n",
    "def calculate_predicted_expected_value_by_label(predictions, bin_values):\n",
    "    \"\"\"\n",
    "    Calculate the predicted expected binned value for each subset of same labeled events, given \n",
    "    the predicted probability distribution for each subset.\n",
    "\n",
    "    predictions : numpy array of predicted probability distributions for each label.\n",
    "    bin_values : numpy array of the value corresponding to each bin.\n",
    "    \"\"\"\n",
    "    bin_value_shift = np.abs(np.min(bin_values)) + 1\n",
    "    shifted_bin_values = bin_values + bin_value_shift\n",
    "    log_shifted_bin_values = np.tile(np.log(shifted_bin_values), (len(shifted_bin_values), 1))\n",
    "    log_shifted_expected_values = scipy.special.logsumexp(predictions + log_shifted_bin_values, axis=1)\n",
    "    shifted_expected_values = np.exp(log_shifted_expected_values)\n",
    "    expected_values = shifted_expected_values - bin_value_shift\n",
    "    return expected_values\n",
    "\n",
    "\n",
    "def plot_log_probabilities_over_labels(fig, ax, predictions, bin_values, cmap=plt.cm.viridis):\n",
    "    \"\"\"\n",
    "    Plot the predicted log probability of each class for each subset of same labeled events.\n",
    "\n",
    "    predictions : A numpy array of set probabilities (rows correspond to labels, columns correspond to class predictions).\n",
    "    bin_values : A numpy array of the value each bin represents. \n",
    "    \"\"\"\n",
    "\n",
    "    color_bounds = np.append(bin_values, bin_values[-1] + (bin_values[-1] - bin_values[-2]))\n",
    "    color_norm = mpl.colors.BoundaryNorm(color_bounds, cmap.N)\n",
    "\n",
    "    for value, curve in zip(bin_values, predictions):\n",
    "        ax.plot(bin_values, curve, color=cmap(color_norm(value)))\n",
    "\n",
    "    fig.colorbar(mpl.cm.ScalarMappable(norm=color_norm, cmap=cmap), ax=ax, label=r\"Actual $\\delta C_9$\")\n",
    "    ax.set_xlabel(r\"$\\delta C_9$\")\n",
    "    ax.set_ylabel(r\"$\\log p(\\delta C_9 | x_1, ..., x_N)$\")\n",
    "\n",
    "\n",
    "def plot_expected_value_over_labels(ax, expected_values, bin_values):\n",
    "    \"\"\"\n",
    "    Plot the predicted expected value for each label.\n",
    "    \"\"\"\n",
    "    ax.scatter(bin_values, expected_values, label=\"Prediction\", color=\"firebrick\", s=16, zorder=5)\n",
    "    ax.plot(\n",
    "        bin_values, bin_values,\n",
    "        label=\"Ref. Line (Slope = 1)\",\n",
    "        color=\"grey\",\n",
    "        linewidth=0.5,\n",
    "        zorder=0\n",
    "    )\n",
    "    ax.set_xlabel(r\"Actual $\\delta C_9$\")\n",
    "    ax.set_ylabel(r\"Predicted $\\delta C_9$\")\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = select_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shawn's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate = True\n",
    "\n",
    "level = \"gen\"\n",
    "save_dir = \"../../state/new_physics/data/processed\"\n",
    "\n",
    "common_generate_kwargs = {\n",
    "    \"raw_signal_dir\": \"../../state/new_physics/data/raw/signal\",\n",
    "    \"std_scale\": True,\n",
    "    \"q_squared_veto\": True,\n",
    "    \"balanced_classes\": True,\n",
    "    \"num_events_per_set\": 70_000,\n",
    "    \"num_sets_per_label\": 50,\n",
    "    \"n_bins\": 10,\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    \"train\": Signal_Images_Dataset(level=level, split=\"train\", save_dir=save_dir),\n",
    "    \"eval\": Signal_Images_Dataset(level=level, split=\"eval\", save_dir=save_dir),\n",
    "}\n",
    "\n",
    "if regenerate:\n",
    "    datasets[\"train\"].generate(\n",
    "        raw_trials=range(1,21), \n",
    "        **common_generate_kwargs\n",
    "    )\n",
    "    datasets[\"eval\"].generate(\n",
    "        raw_trials=range(21,41), \n",
    "        **common_generate_kwargs\n",
    "    )\n",
    "\n",
    "datasets[\"train\"].load(device)\n",
    "datasets[\"eval\"].load(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_volume_slices(arr, n_slices=3, cmap=plt.cm.magma, note=\"\"):\n",
    "    \"\"\"\n",
    "    Plot slices of volumetric data.\n",
    "    Slices are along the z-axis (axis 2).\n",
    "    Array arr should be a three-dimensional array.\n",
    "    Slices might not be evenly spaced along z-axis.\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax_3d = fig.add_subplot(projection=\"3d\")\n",
    "\n",
    "    var_dim = {\n",
    "        0: \"chi\",\n",
    "        1: \"costheta_mu\",\n",
    "        2: \"costheta_K\",\n",
    "    }\n",
    "\n",
    "    dim_ind_cart = { # dont change for now\n",
    "        \"x\": 1,     \n",
    "        \"y\": 2,\n",
    "        \"z\": 0,  \n",
    "    }\n",
    "\n",
    "    norm=Normalize(vmin=-1.1, vmax=1.1)\n",
    "    arr = arr.squeeze()\n",
    "    arr = arr.cpu()\n",
    "    arr = np.transpose(\n",
    "        arr, \n",
    "        (dim_ind_cart[\"x\"], dim_ind_cart[\"y\"], dim_ind_cart[\"z\"])\n",
    "    )\n",
    "    colors = cmap(norm(arr))\n",
    "    \n",
    "    cart_dim_shape = {\n",
    "        dim_name: arr.shape[dim_ind_cart[dim_name]] for dim_name in dim_ind_cart.keys()\n",
    "    }\n",
    "\n",
    "    def xy_plane(z_pos):\n",
    "        x, y = np.indices(\n",
    "            (cart_dim_shape[\"x\"] + 1, cart_dim_shape[\"y\"] + 1)\n",
    "        )\n",
    "        z = np.full(\n",
    "            (cart_dim_shape[\"x\"] + 1, cart_dim_shape[\"y\"] + 1), z_pos\n",
    "        )\n",
    "        return x, y, z\n",
    "    \n",
    "    def plot_slice(z_index):\n",
    "        x, y, z = xy_plane(z_index) \n",
    "        ax_3d.plot_surface(\n",
    "            x, y, z, \n",
    "            rstride=1, cstride=1, \n",
    "            facecolors=colors[:,:,z_index], \n",
    "            shade=False,\n",
    "        )\n",
    "\n",
    "    def plot_outline(z_index, offset=0.3):\n",
    "        x, y, z = xy_plane(z_index - offset)\n",
    "        \n",
    "        ax_3d.plot_surface(\n",
    "            x, y, z, \n",
    "            rstride=1, cstride=1, \n",
    "            shade=False,\n",
    "            color=\"#f2f2f2\",\n",
    "            edgecolor=\"#f2f2f2\", \n",
    "        )\n",
    "\n",
    "    z_indices = np.linspace(0, cart_dim_shape[\"z\"]-1, n_slices, dtype=int) # forces integer indices\n",
    "    for i in z_indices:\n",
    "        plot_outline(i)\n",
    "        plot_slice(i)\n",
    "\n",
    "    cbar = fig.colorbar(ScalarMappable(norm=norm, cmap=cmap), ax=ax_3d, location=\"left\", shrink=0.5, pad=-0.05)\n",
    "    cbar.set_label(r\"${q^2}$ (Avg.)\", size=11)\n",
    "\n",
    "    ax_labels = {\n",
    "        \"chi\": r\"$\\chi$\", \n",
    "        \"costheta_mu\": r\"$\\cos\\theta_\\mu$\",\n",
    "        \"costheta_K\": r\"$\\cos\\theta_K$\",\n",
    "    }\n",
    "\n",
    "    ax_3d.set_xlabel(ax_labels[var_dim[dim_ind_cart[\"x\"]]], labelpad=0)\n",
    "    ax_3d.set_ylabel(ax_labels[var_dim[dim_ind_cart[\"y\"]]], labelpad=0)\n",
    "    # ax_3d.zaxis.set_rotate_label(False)\n",
    "    ax_3d.set_zlabel(ax_labels[var_dim[dim_ind_cart[\"z\"]]], labelpad=-3,)#rotation=\"horizontal\") \n",
    "\n",
    "    ticks = {\n",
    "        \"costheta_mu\": [\"-1\", \"1\"],\n",
    "        \"costheta_K\": [\"-1\", \"1\"],\n",
    "        \"chi\": ['0', r\"$2\\pi$\"],\n",
    "    }      \n",
    "\n",
    "    ax_3d.set_xticks([0, arr.shape[dim_ind_cart[\"x\"]]-1], ticks[var_dim[dim_ind_cart[\"x\"]]])\n",
    "    ax_3d.set_yticks([0, arr.shape[dim_ind_cart[\"y\"]]-1], ticks[var_dim[dim_ind_cart[\"y\"]]])\n",
    "    ax_3d.set_zticks([0, arr.shape[dim_ind_cart[\"z\"]]-1], ticks[var_dim[dim_ind_cart[\"z\"]]])\n",
    "\n",
    "    ax_3d.tick_params(pad=0.3)\n",
    "\n",
    "    ax_3d.set_box_aspect(None, zoom=0.85)\n",
    "\n",
    "    ax_3d.set_title(f\"{note}\", loc=\"center\", y=1)\n",
    "\n",
    "\n",
    "plot_volume_slices(\n",
    "     datasets[\"train\"].features[0], \n",
    "     n_slices=3, \n",
    "     note=r\"$\\delta C_9$ : \"+f\"{datasets[\"train\"].labels[0]}\"\n",
    ")\n",
    "# plt.savefig(f\"{i}\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res_Block(nn.Module):\n",
    "    def __init__(self, in_out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=in_out_channels, out_channels=in_out_channels, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            # nn.BatchNorm3d(num_features=in_out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=in_out_channels, out_channels=in_out_channels, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            # nn.BatchNorm3d(num_features=in_out_channels),\n",
    "        )\n",
    "        self.last_activation = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.block(x) + x\n",
    "        x = self.last_activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv_Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block_a = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            # nn.BatchNorm3d(num_features=out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            # nn.BatchNorm3d(num_features=out_channels),\n",
    "        )\n",
    "        self.block_b = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            # nn.BatchNorm3d(num_features=out_channels),\n",
    "        )\n",
    "        self.last_activation = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        out_block_a = self.block_a(x)\n",
    "        out_block_b = self.block_b(x)\n",
    "        x = out_block_a + out_block_b\n",
    "        x = self.last_activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNN_Res(Custom_Model):\n",
    "    def __init__(self, nickname, model_dir,):\n",
    "        super().__init__(nickname, model_dir)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=\"same\", bias=False),\n",
    "            # nn.BatchNorm3d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=1, padding=1),\n",
    "            *[Res_Block(in_out_channels=16) for _ in range(3)],\n",
    "            Conv_Block(in_channels=16, out_channels=16),\n",
    "            *[Res_Block(in_out_channels=16) for _ in range(3)],\n",
    "            Conv_Block(in_channels=16, out_channels=16),\n",
    "            *[Res_Block(in_out_channels=16) for _ in range(3)],\n",
    "            # Conv_Block(in_channels=128, out_channels=128),\n",
    "            # *[Res_Block(in_out_channels=128) for _ in range(1)],\n",
    "        )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(in_features=16, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=32, out_features=1),\n",
    "        )\n",
    "        \n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.mean(x, dim=(2,3,4))\n",
    "        x = self.dense(x)\n",
    "        x = torch.squeeze(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain = False\n",
    "\n",
    "nickname = f\"cnn_res_{common_generate_kwargs[\"num_events_per_set\"]}\"\n",
    "\n",
    "model = CNN_Res(nickname, \"../../state/new_physics/models\")\n",
    "\n",
    "if retrain:\n",
    "    learning_rate = 4e-4\n",
    "    epochs = 80\n",
    "    train_batch_size = 32\n",
    "    eval_batch_size = 32\n",
    "    loss_fn = nn.L1Loss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_and_eval(\n",
    "        model, \n",
    "        datasets[\"train\"], datasets[\"eval\"], \n",
    "        loss_fn, optimizer, \n",
    "        epochs, \n",
    "        train_batch_size, eval_batch_size, \n",
    "        device, \n",
    "        move_data=False,\n",
    "        scheduler=ReduceLROnPlateau(optimizer, factor=0.9, patience=1),\n",
    "        checkpoint_epochs=5,\n",
    "    )\n",
    "    _, ax = plt.subplots()\n",
    "    plot_epoch_start = 0\n",
    "    plot_loss_curves(\n",
    "        model.loss_table[\"epoch\"][plot_epoch_start:], \n",
    "        model.loss_table[\"train_loss\"][plot_epoch_start:], \n",
    "        model.loss_table[\"eval_loss\"][plot_epoch_start:], \n",
    "        ax\n",
    "    )\n",
    "    ax.set_yscale(\"log\")\n",
    "    plt.show()\n",
    "else:\n",
    "    pass\n",
    "    # model.load_final()\n",
    "    # model.load_checkpoint(epoch_number=10)\n",
    "    # model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Res(nickname, \"../../state/new_physics/models\")\n",
    "\n",
    "print(\"final:\")\n",
    "\n",
    "model.load_final()\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    yhat = model(datasets[\"eval\"].features)\n",
    "    avgs = yhat.reshape(-1, common_generate_kwargs[\"num_sets_per_label\"]).mean(1).detach().cpu().numpy()\n",
    "    stds = yhat.reshape(-1, common_generate_kwargs[\"num_sets_per_label\"]).std(1).detach().cpu().numpy()\n",
    "\n",
    "    y = datasets[\"eval\"].labels\n",
    "    unique_y = y.reshape(-1, common_generate_kwargs[\"num_sets_per_label\"]).mean(1).detach().cpu().numpy()\n",
    "    unique_y\n",
    "\n",
    "    def calc_mse():\n",
    "        dif = yhat.detach().cpu().numpy() - y.detach().cpu().numpy()\n",
    "        dif_sq = dif**2\n",
    "        out = dif_sq.mean()\n",
    "        return out\n",
    "\n",
    "    def calc_mae():\n",
    "        dif = yhat.detach().cpu().numpy() - y.detach().cpu().numpy()\n",
    "        abs_dif = np.abs(dif)\n",
    "        out = abs_dif.mean()\n",
    "        return out\n",
    "    \n",
    "    print(\"mse:\", calc_mse())\n",
    "    print(\"mae:\", calc_mae())\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "plot_prediction_linearity(\n",
    "    ax,\n",
    "    unique_y,\n",
    "    avgs,\n",
    "    stds,\n",
    "    ref_line_buffer=0.05,\n",
    "    xlim=(-2.25, 1.35),\n",
    "    ylim=(-2.25, 1.35),\n",
    "    xlabel=r\"Actual $\\delta C_9$\", \n",
    "    ylabel=r\"Predicted $\\delta C_9$\"\n",
    ")\n",
    "make_plot_note(ax, f\"Image (10 bin), Gen., {common_generate_kwargs[\"num_sets_per_label\"]} boots., {common_generate_kwargs[\"num_events_per_set\"]} events/boots.\", fontsize=\"medium\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate = True\n",
    "\n",
    "level = \"gen\"\n",
    "save_dir = \"../../state/new_physics/data/processed\"\n",
    "num_events_per_set = 6_000\n",
    "num_sets_per_label = 2_000\n",
    "\n",
    "single_eval_dset = Signal_Images_Single_Dataset(level=level, split=\"eval\", save_dir=save_dir)\n",
    "\n",
    "if regenerate:\n",
    "    single_eval_dset.generate(\n",
    "        raw_trials=range(21,41), \n",
    "        raw_signal_dir=\"../../state/new_physics/data/raw/signal\",\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=num_sets_per_label,\n",
    "        n_bins=10,\n",
    "        value=-0.82,\n",
    "    )\n",
    "\n",
    "single_eval_dset.load()\n",
    "\n",
    "nickname =f\"cnn_res_{num_events_per_set}\"\n",
    "model = CNN_Res(nickname, \"../../state/new_physics/models\")\n",
    "model.load_final()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    yhats = []\n",
    "    for s in single_eval_dset.features:\n",
    "        yhat = model(s.unsqueeze(0).to(device))\n",
    "        yhats.append(yhat.detach().cpu())\n",
    "    yhats = np.array(yhats)\n",
    "\n",
    "    mean = yhats.mean()\n",
    "    std = yhats.std()\n",
    "\n",
    "setup_high_quality_mpl_params()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "xbounds = (-1.5, 0)\n",
    "ybounds = (0, 200)\n",
    "\n",
    "ax.hist(yhats, bins=50, range=xbounds)\n",
    "ax.vlines(-0.82, 0, ybounds[1], color=\"red\", label=\"Target (-0.82)\")\n",
    "ax.vlines(mean, 0, ybounds[1], color=\"red\", linestyles=\"--\", label=r\"$\\mu = $ \"+f\"{mean.round(3)}\")\n",
    "ax.hlines(20, mean, mean+std, color=\"orange\", linestyles=\"dashdot\", label=r\"$\\sigma = $ \"+f\"{std.round(3)}\")\n",
    "ax.set_xlabel(r\"Predicted $\\delta C_9$\")\n",
    "make_plot_note(ax, f\"Images (10 bins), Gen., {num_sets_per_label} boots., {num_events_per_set} events/boots.\", fontsize=\"medium\")\n",
    "ax.set_xbound(*xbounds)\n",
    "ax.set_ybound(*ybounds)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate = False\n",
    "\n",
    "level = \"gen\"\n",
    "save_dir = \"../../state/new_physics/data/processed\"\n",
    "\n",
    "common_generate_kwargs = {\n",
    "    \"raw_signal_dir\": \"../../state/new_physics/data/raw/signal\",\n",
    "    \"std_scale\": True,\n",
    "    \"q_squared_veto\": True,\n",
    "    \"balanced_classes\": True,\n",
    "    \"num_events_per_set\": 6_000,\n",
    "    \"num_sets_per_label\": 50,\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    \"train\": Bootstrapped_Signal_Unbinned_Dataset(level=level, split=\"train\", save_dir=save_dir),\n",
    "    \"eval\": Bootstrapped_Signal_Unbinned_Dataset(level=level, split=\"eval\", save_dir=save_dir),\n",
    "}\n",
    "\n",
    "if regenerate:\n",
    "    datasets[\"train\"].generate(\n",
    "        raw_trials=range(1,21), \n",
    "        **common_generate_kwargs\n",
    "    )\n",
    "    datasets[\"eval\"].generate(\n",
    "        raw_trials=range(21,41), \n",
    "        **common_generate_kwargs\n",
    "    )\n",
    "\n",
    "datasets[\"train\"].load(device)\n",
    "datasets[\"eval\"].load(device)\n",
    "\n",
    "# datasets[\"train\"].features = torch.transpose(datasets[\"train\"].features, dim0=1, dim1=2)\n",
    "# datasets[\"eval\"].features = torch.transpose(datasets[\"eval\"].features, dim0=1, dim1=2)\n",
    "\n",
    "# print(f\"new train features shape: {datasets[\"train\"].features.shape}\")\n",
    "# print(f\"new eval features shape: {datasets[\"eval\"].features.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res_Block_Event(nn.Module):\n",
    "    def __init__(self, in_out_features):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_features=in_out_features, out_features=in_out_features),\n",
    "            # nn.LayerNorm(normalized_shape=(num_events_per_set, in_out_features)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=in_out_features, out_features=in_out_features),\n",
    "            # nn.LayerNorm(normalized_shape=(num_events_per_set, in_out_features)),\n",
    "        )\n",
    "        self.last_activation = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.block(x) #+ x\n",
    "        x = self.last_activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Res_Block_Set(nn.Module):\n",
    "    def __init__(self, in_out_features):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_features=in_out_features, out_features=in_out_features),\n",
    "            # nn.BatchNorm1d(num_features=in_out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=in_out_features, out_features=in_out_features),\n",
    "            # nn.BatchNorm1d(num_features=in_out_features),\n",
    "        )\n",
    "        self.last_activation = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.block(x) #+ x\n",
    "        x = self.last_activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Deep_Sets(Custom_Model):\n",
    "    def __init__(self, nickname, model_dir):\n",
    "        super().__init__(nickname, model_dir)\n",
    "\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(in_features=4, out_features=32),\n",
    "            # nn.LayerNorm(normalized_shape=(num_events_per_set, 32)),\n",
    "            nn.ReLU(),\n",
    "            *[Res_Block_Event(in_out_features=32) for _ in range(3)],\n",
    "            nn.Linear(in_features=32, out_features=32),\n",
    "            # nn.LayerNorm(normalized_shape=(num_events_per_set,32)),\n",
    "        )\n",
    "\n",
    "        self.g = nn.Sequential(\n",
    "            *[Res_Block_Set(in_out_features=32) for _ in range(3)],\n",
    "            nn.Linear(in_features=32, out_features=32),\n",
    "            # nn.BatchNorm1d(num_features=32), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=1),\n",
    "        )\n",
    "        \n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.g(x)\n",
    "        x = torch.squeeze(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain = False\n",
    "\n",
    "nickname = f\"deep_sets_{common_generate_kwargs[\"num_events_per_set\"]}\"\n",
    "\n",
    "model = Deep_Sets(nickname, \"../../state/new_physics/models\")\n",
    "\n",
    "if retrain:\n",
    "    learning_rate = 4e-4\n",
    "    epochs = 80\n",
    "    train_batch_size = 32\n",
    "    eval_batch_size = 32\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_and_eval(\n",
    "        model, \n",
    "        datasets[\"train\"], datasets[\"eval\"], \n",
    "        loss_fn, optimizer, \n",
    "        epochs, \n",
    "        train_batch_size, eval_batch_size, \n",
    "        device, \n",
    "        move_data=False,\n",
    "        scheduler=ReduceLROnPlateau(optimizer, factor=0.9, patience=1),\n",
    "        checkpoint_epochs=5,\n",
    "    )\n",
    "    _, ax = plt.subplots()\n",
    "    plot_epoch_start = 0\n",
    "    plot_loss_curves(\n",
    "        model.loss_table[\"epoch\"][plot_epoch_start:], \n",
    "        model.loss_table[\"train_loss\"][plot_epoch_start:], \n",
    "        model.loss_table[\"eval_loss\"][plot_epoch_start:], \n",
    "        ax\n",
    "    )\n",
    "    ax.set_yscale(\"log\")\n",
    "    plt.show()\n",
    "else:\n",
    "    pass\n",
    "    # model.load_final()\n",
    "    # model.load_checkpoint(epoch_number=10)\n",
    "    # model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final: \")\n",
    "\n",
    "setup_high_quality_mpl_params()\n",
    "\n",
    "model.load_final()\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    yhats = []\n",
    "    for feat in datasets[\"eval\"].features:\n",
    "        feat = feat.unsqueeze(0)\n",
    "        yhat = model(feat)\n",
    "        yhats.append(yhat)\n",
    "\n",
    "    yhats = torch.tensor(yhats)\n",
    "    avgs = yhats.reshape(-1, common_generate_kwargs[\"num_sets_per_label\"]).mean(1).detach().cpu().numpy()\n",
    "    stds = yhats.reshape(-1, common_generate_kwargs[\"num_sets_per_label\"]).std(1).detach().cpu().numpy()\n",
    "\n",
    "    y = datasets[\"eval\"].labels\n",
    "    unique_y = y.reshape(-1, common_generate_kwargs[\"num_sets_per_label\"]).mean(1).detach().cpu().numpy()\n",
    "    unique_y\n",
    "\n",
    "    def calc_mse():\n",
    "        dif = yhats.detach().cpu().numpy() - y.detach().cpu().numpy()\n",
    "        dif_sq = dif**2\n",
    "        out = dif_sq.mean()\n",
    "        return out\n",
    "\n",
    "    def calc_mae():\n",
    "        dif = yhats.detach().cpu().numpy() - y.detach().cpu().numpy()\n",
    "        abs_dif = np.abs(dif)\n",
    "        out = abs_dif.mean()\n",
    "        return out\n",
    "    \n",
    "    print(\"mse:\", calc_mse())\n",
    "    print(\"mae:\", calc_mae())\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "plot_prediction_linearity(\n",
    "    ax,\n",
    "    unique_y,\n",
    "    avgs,\n",
    "    stds,\n",
    "    ref_line_buffer=0.05,\n",
    "    xlim=(-2.25, 1.35),\n",
    "    ylim=(-2.25, 1.35),\n",
    "    xlabel=r\"Actual $\\delta C_9$\", \n",
    "    ylabel=r\"Predicted $\\delta C_9$\"\n",
    ")\n",
    "make_plot_note(ax, f\"Deep Sets, Gen., {common_generate_kwargs[\"num_sets_per_label\"]} boots., {common_generate_kwargs[\"num_events_per_set\"]} events/boots.\", fontsize=\"medium\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Bootstrapped_Signal_Unbinned_Single_Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m num_events_per_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24_000\u001b[39m\n\u001b[0;32m      6\u001b[0m num_sets_per_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2_000\u001b[39m\n\u001b[1;32m----> 8\u001b[0m single_eval_dset \u001b[38;5;241m=\u001b[39m \u001b[43mBootstrapped_Signal_Unbinned_Single_Dataset\u001b[49m(level\u001b[38;5;241m=\u001b[39mlevel, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_dir\u001b[38;5;241m=\u001b[39msave_dir)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m regenerate:\n\u001b[0;32m     11\u001b[0m     single_eval_dset\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     12\u001b[0m         raw_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m21\u001b[39m,\u001b[38;5;241m41\u001b[39m), \n\u001b[0;32m     13\u001b[0m         raw_signal_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../state/new_physics/data/raw/signal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m         label_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.82\u001b[39m,\n\u001b[0;32m     17\u001b[0m     )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Bootstrapped_Signal_Unbinned_Single_Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "regenerate = True\n",
    "\n",
    "level = \"gen\"\n",
    "save_dir = \"../../state/new_physics/data/processed\"\n",
    "num_events_per_set = 24_000\n",
    "num_sets_per_label = 2_000\n",
    "\n",
    "single_eval_dset = Bootstrapped_Signal_Unbinned_Single_Dataset(level=level, split=\"eval\", save_dir=save_dir)\n",
    "\n",
    "if regenerate:\n",
    "    single_eval_dset.generate(\n",
    "        raw_trials=range(21,41), \n",
    "        raw_signal_dir=\"../../state/new_physics/data/raw/signal\",\n",
    "        num_events_per_set=num_events_per_set,\n",
    "        num_sets_per_label=num_sets_per_label,\n",
    "        label_value=-0.82,\n",
    "    )\n",
    "\n",
    "single_eval_dset.load()\n",
    "\n",
    "with torch.no_grad():\n",
    "    yhats = []\n",
    "    for s in single_eval_dset.features:\n",
    "        yhat = model(s.unsqueeze(0).to(device))\n",
    "        yhats.append(yhat.detach().cpu())\n",
    "    yhats = np.array(yhats)\n",
    "\n",
    "    mean = yhats.mean()\n",
    "    std = yhats.std()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "xbounds = (-1.5, 0)\n",
    "ybounds = (0, 200)\n",
    "\n",
    "ax.hist(yhats, bins=50, range=xbounds)\n",
    "ax.vlines(-0.82, 0, ybounds[1], color=\"red\", label=\"Target (-0.82)\")\n",
    "ax.vlines(mean, 0, ybounds[1], color=\"red\", linestyles=\"--\", label=r\"$\\mu = $ \"+f\"{mean.round(3)}\")\n",
    "ax.hlines(100, mean, mean+std, color=\"orange\", linestyles=\"dashdot\", label=r\"$\\sigma = $ \"+f\"{std.round(3)}\")\n",
    "ax.set_xlabel(r\"Predicted $\\delta C_9$\")\n",
    "make_plot_note(ax, f\"Deep Sets, Gen., {num_sets_per_label} boots., {num_events_per_set} events/boots.\", fontsize=\"medium\")\n",
    "ax.set_xbound(*xbounds)\n",
    "ax.set_ybound(*ybounds)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event-by-event Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate = False\n",
    "\n",
    "level = \"gen\"\n",
    "save_dir = \"../../state/new_physics/data/processed\"\n",
    "raw_signal_dir = \"../../state/new_physics/data/raw/signal\"\n",
    "\n",
    "std_scale = True\n",
    "q_squared_veto = True\n",
    "\n",
    "datasets = {\n",
    "    \"train\": Aggregated_Signal_Binned_Dataset(level=level, split=\"train\", save_dir=save_dir),\n",
    "    \"eval\": Aggregated_Signal_Binned_Dataset(level=level, split=\"eval\", save_dir=save_dir),\n",
    "}\n",
    "\n",
    "if regenerate:\n",
    "    datasets[\"train\"].generate(\n",
    "        raw_trials=range(1,20), \n",
    "        raw_signal_dir=raw_signal_dir, \n",
    "        std_scale=std_scale, \n",
    "        q_squared_veto=q_squared_veto\n",
    "    )\n",
    "    datasets[\"eval\"].generate(\n",
    "        raw_trials=range(20,40), \n",
    "        raw_signal_dir=raw_signal_dir, \n",
    "        std_scale=std_scale, \n",
    "        q_squared_veto=q_squared_veto\n",
    "    )\n",
    "\n",
    "datasets[\"train\"].load(device)\n",
    "datasets[\"eval\"].load(device)\n",
    "\n",
    "np.testing.assert_equal(datasets[\"train\"].bin_values, datasets[\"eval\"].bin_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event_By_Event_NN(Custom_Model):\n",
    "    def __init__(self, nickname, model_dir):\n",
    "        super().__init__(nickname, model_dir)\n",
    "\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(4, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 44),\n",
    "         )\n",
    "        \n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        event_logits = self.base(x)\n",
    "        return event_logits\n",
    "\n",
    "\n",
    "retrain = False\n",
    "\n",
    "model = Event_By_Event_NN(\"event_by_event_nn\", \"../../state/new_physics/models\")\n",
    "\n",
    "if retrain:\n",
    "    learning_rate = 3e-3\n",
    "    epochs = 200\n",
    "    train_batch_size = 10_000\n",
    "    eval_batch_size = 10_000\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_table = train_and_eval(\n",
    "        model, \n",
    "        datasets[\"train\"], datasets[\"eval\"], \n",
    "        loss_fn, optimizer, \n",
    "        epochs, \n",
    "        train_batch_size, eval_batch_size, \n",
    "        device, \n",
    "        move_data=True,\n",
    "        scheduler= ReduceLROnPlateau(optimizer, factor=0.95, threshold=0, patience=0, eps=1e-9),\n",
    "        checkpoint_epochs=5,\n",
    "    )\n",
    "    _, ax = plt.subplots()\n",
    "    plot_epoch_start = 0\n",
    "    plot_loss_curves(loss_table[\"epoch\"][plot_epoch_start:], loss_table[\"train_loss\"][plot_epoch_start:], loss_table[\"eval_loss\"][plot_epoch_start:], ax)\n",
    "    ax.set_yscale(\"log\")\n",
    "    plt.show()\n",
    "else:\n",
    "    model.load_final()\n",
    "    # model.load_checkpoint(epoch_number=10)\n",
    "    model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 2000\n",
    "n_events_per_trial = 70_000\n",
    "\n",
    "expected_values_all_trials = []\n",
    "for _ in range(n_trials):\n",
    "\n",
    "    boot_x, boot_y = bootstrap_over_bins(\n",
    "        datasets[\"eval\"].features.cpu().numpy(), \n",
    "        datasets[\"eval\"].labels.cpu().numpy(), \n",
    "        n_events_per_trial,\n",
    "    )\n",
    "    boot_x = torch.from_numpy(boot_x).to(device)\n",
    "    boot_y = torch.from_numpy(boot_y).to(device)\n",
    "\n",
    "    log_probs = predict_log_probabilities_by_label(boot_x, boot_y, model)\n",
    "    expected_values = calculate_predicted_expected_value_by_label(log_probs, datasets[\"eval\"].bin_values)\n",
    "    expected_values_all_trials.append(np.expand_dims(expected_values, axis=0))\n",
    "\n",
    "expected_values_all_trials = np.concat(expected_values_all_trials)\n",
    "\n",
    "def calc_mse():\n",
    "    dif = expected_values_all_trials - datasets[\"eval\"].bin_values\n",
    "    dif_sq = dif**2\n",
    "    out = dif_sq.mean()\n",
    "    return out\n",
    "\n",
    "def calc_mae():\n",
    "    dif = expected_values_all_trials - datasets[\"eval\"].bin_values\n",
    "    abs_dif = np.abs(dif)\n",
    "    out = abs_dif.mean()\n",
    "    return out\n",
    "\n",
    "mse = calc_mse()\n",
    "mae = calc_mae()\n",
    "print(\"mse:\", mse)\n",
    "print(\"mae:\", mae)\n",
    "\n",
    "\n",
    "expected_values_all_trials_means = np.mean(expected_values_all_trials, axis=0)\n",
    "expected_values_all_trials_stds = np.std(expected_values_all_trials, axis=0)\n",
    "\n",
    "setup_high_quality_mpl_params()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plot_prediction_linearity(\n",
    "    ax,\n",
    "    datasets[\"eval\"].bin_values,\n",
    "    expected_values_all_trials_means,\n",
    "    expected_values_all_trials_stds,\n",
    "    ref_line_buffer=0.05,\n",
    "    xlim=(-2.25, 1.35),\n",
    "    ylim=(-2.25, 1.35),\n",
    "    xlabel=r\"Actual $\\delta C_9$\", \n",
    "    ylabel=r\"Predicted $\\delta C_9$\"\n",
    ")\n",
    "\n",
    "make_plot_note(ax, f\"Event-by-event, Gen., {n_trials} boots., {n_events_per_trial} events/boots.\", fontsize=\"medium\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_events_per_set = 6_000\n",
    "n_sets = 2000\n",
    "\n",
    "target_bin_index = np.argwhere(datasets[\"eval\"].bin_values==-0.82).item()\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "pool_x = datasets[\"eval\"].features[datasets[\"eval\"].labels==target_bin_index]\n",
    "pool_y = datasets[\"eval\"].labels[datasets[\"eval\"].labels==target_bin_index]\n",
    "\n",
    "yhats = []\n",
    "for _ in range(n_sets):\n",
    "    selection_indices = rng.choice(len(pool_x), n_events_per_set)\n",
    "    boot_x = pool_x[selection_indices]\n",
    "    boot_y = pool_y[selection_indices]\n",
    "\n",
    "    log_probs = predict_log_probabilities(boot_x, model)\n",
    "    bin_values = datasets[\"eval\"].bin_values + 5\n",
    "    log_bin_values = np.log(bin_values)\n",
    "    lse = logsumexp(log_bin_values + log_probs.cpu().numpy())\n",
    "    yhat = np.exp(lse) - 5\n",
    "    yhats.append(yhat)\n",
    "\n",
    "yhats = np.array(yhats)\n",
    "\n",
    "mean= yhats.mean()\n",
    "std = yhats.std()\n",
    "\n",
    "setup_high_quality_mpl_params()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "xbounds = (-1.5, 0)\n",
    "ybounds = (0, 200)\n",
    "\n",
    "ax.hist(yhats, bins=50, range=xbounds)\n",
    "ax.vlines(-0.82, 0, ybounds[1], color=\"red\", label=\"Target (-0.82)\")\n",
    "ax.vlines(mean, 0, ybounds[1], color=\"red\", linestyles=\"--\", label=r\"$\\mu = $ \"+f\"{mean.round(3)}\")\n",
    "ax.hlines(20, mean, mean+std, color=\"orange\", linestyles=\"dashdot\", label=r\"$\\sigma = $ \"+f\"{std.round(3)}\")\n",
    "ax.set_xlabel(r\"Predicted $\\delta C_9$\")\n",
    "make_plot_note(ax, f\"Event-by-event, Gen., {n_sets} boots., {n_events_per_set} events/boots.\", fontsize=\"medium\")\n",
    "ax.set_xbound(*xbounds)\n",
    "ax.set_ybound(*ybounds)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detector Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shawn's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event-by-event Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detector Level with Backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shawn's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event-by-event Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maybe_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
